{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"BDOWm6hD66VD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620233432576,"user_tz":-420,"elapsed":26182,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"cfaf411b-5ed7-4cd5-badc-eb9a2f79927b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f9ujFbFy6kb4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620233501573,"user_tz":-420,"elapsed":6269,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"bc0b3131-0ac9-41f7-c517-0d91249dff1e"},"source":["%tensorflow_version 1.x\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n","from sklearn import metrics\n","import random\n","from random import randint\n","import time\n","import os\n","from sklearn.model_selection import train_test_split"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGzQvKiQ7xWI","executionInfo":{"status":"ok","timestamp":1620233504564,"user_tz":-420,"elapsed":2053,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"75dde5e5-61e3-4972-8c8d-d624850aa3a0"},"source":["%cd /content/drive/MyDrive/Train-Openpose-lstm/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Train-Openpose-lstm\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JWhzsMB26kb5"},"source":["## Preparing dataset:"]},{"cell_type":"code","metadata":{"id":"Cr0tgnZt6kb5","executionInfo":{"status":"ok","timestamp":1620233506167,"user_tz":-420,"elapsed":533,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}}},"source":["# Useful Constants\n","\n","# Output classes to learn how to classify\n","LABELS = [    \n","    \"Crouch\",\n","    \"Fall\",\n","    \"Lyin\",\n","    \"Punch\",\n","    \"Sit\",\n","    \"Stand\",\n","    \"Walk\"\n","\n","] \n","DATASET_PATH = \"data/HAR_pose_activities/database/\"\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vQYj6DR6kb6","executionInfo":{"status":"ok","timestamp":1620233517300,"user_tz":-420,"elapsed":6951,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"e9323e7d-7115-41b4-ec1d-3c51203f9870"},"source":["n_steps = 1 # 32 timesteps per series\n","# Load the networks inputs\n","def load_X_csv(X_path):\n","  X_= np.loadtxt(open(X_path, \"rb\"), delimiter=\",\", skiprows=1)\n","  print(X_.shape)\n","  blocks = int(len(X_) / n_steps)\n","  print(blocks)\n","  X_ = np.array(np.split(X_, blocks))\n","  print(X_.shape)\n","  return X_ \n","\n","def load_Y_csv(Y_path):\n","  Y_= np.loadtxt(open(Y_path, \"rb\"), delimiter=\",\", skiprows=1)\n","  Y_= Y_.astype(np.int32)\n","  Y_ = Y_.reshape((len(Y_),1))\n","  return Y_ -1 \n","\n","#X_train = load_X(X_train_path)\n","X_train = load_X_csv(DATASET_PATH + \"x.csv\")\n","#X_test = load_X(X_test_path)\n","#print (load_X(DATASET_PATH + \"X_train.txt\").shape)\n","#print (x_train.shape)\n","\n","#y_train = load_y(y_train_path)\n","y_train = load_Y_csv(DATASET_PATH + \"y.csv\")\n","#y_test = load_y(y_train_path)\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n","print (y_train.shape)\n","print (y_test.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(18152, 36)\n","18152\n","(18152, 1, 36)\n","(12161, 1)\n","(5991, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7jVNvccO6kb7"},"source":["## Set Parameters:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dc2CYaRD6kb7","executionInfo":{"status":"ok","timestamp":1620233519399,"user_tz":-420,"elapsed":722,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"c418c26c-a4e7-4588-8b26-57aed0abfa0e"},"source":["# Input Data \n","\n","training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n","test_data_count = len(X_test)  # 1197 test series\n","n_input = len(X_train[0][0])  # num input parameters per timestep\n","\n","n_hidden = 34 # Hidden layer num of features\n","n_classes = 7\n","\n","#updated for learning-rate decay\n","# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n","decaying_learning_rate = True\n","learning_rate = 0.0001 #used if decaying_learning_rate set to False\n","init_learning_rate = 0.005\n","decay_rate = 0.96 #the base of the exponential in the decay\n","decay_steps = 60000 #used in decay every 60000 steps with a base of 0.96\n","\n","global_step = tf.Variable(0, trainable=False)\n","lambda_loss_amount = 0.0015\n","\n","training_iters = training_data_count *1000  # Loop 300 times on the dataset, ie 300 epochs\n","batch_size = 128\n","display_iter = batch_size*8  # To show test set accuracy during training\n","\n","print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n","print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n","print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n","\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(X shape, y shape, every X's mean, every X's standard deviation)\n","(12161, 1, 36) (5991, 1) 0.3831073301363341 0.20073731600086114\n","\n","The dataset has not been preprocessed, is not normalised etc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jX5YcEXR6kb8"},"source":["## Utility functions for training:"]},{"cell_type":"code","metadata":{"id":"OY2h45Yi6kb8","executionInfo":{"status":"ok","timestamp":1620234005896,"user_tz":-420,"elapsed":983,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}}},"source":["def LSTM_RNN(_X, _weights, _biases):\n","    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n","\n","    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n","    _X = tf.reshape(_X, [-1, n_input])   \n","    # Rectifies Linear Unit activation function used\n","    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n","    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n","    _X = tf.split(_X, n_steps, 0) \n","\n","    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n","    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n","    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n","    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n","    \n","    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n","\n","    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n","    lstm_last_output = outputs[-1]\n","    \n","    # Linear activation\n","    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n","\n","\n","def extract_batch_size(_train, _labels, _unsampled, batch_size):\n","    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n","    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n","    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n","    \n","    shape = list(_train.shape)\n","    shape[0] = batch_size\n","    batch_s = np.empty(shape)\n","    batch_labels = np.empty((batch_size,1)) \n","\n","    for i in range(batch_size):\n","        # Loop index\n","        # index = random sample from _unsampled (indices)\n","        index = random.choice(_unsampled)\n","        batch_s[i] = _train[index] \n","        batch_labels[i] = _labels[index]\n","        _unsampled.remove(index)\n","\n","\n","    return batch_s, batch_labels, _unsampled\n","\n","\n","def one_hot(y_):\n","    # One hot encoding of the network outputs\n","    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n","    \n","    y_ = y_.reshape(len(y_))\n","    n_values = int(np.max(y_)) + 1\n","    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44_N1EZG6kb8"},"source":["## Build the network:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xnW2YMW6kb9","executionInfo":{"status":"ok","timestamp":1620234010881,"user_tz":-420,"elapsed":1535,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"8ffd34a4-e5c3-4660-f4b5-3a0282eb1e39"},"source":["\n","# Graph input/output\n","x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n","#x = tf.placeholder(tf.float32, [None, 32, n_input])\n","y = tf.placeholder(tf.float32, [None, n_classes])\n","\n","# Graph weights\n","weights = {\n","    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n","    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n","}\n","biases = {\n","    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n","    'out': tf.Variable(tf.random_normal([n_classes]))\n","}\n","\n","pred = LSTM_RNN(x, weights, biases)\n","\n","# Loss, optimizer and evaluation\n","l2 = lambda_loss_amount * sum(\n","    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",") # L2 loss prevents this overkill neural network to overfit the data\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n","if decaying_learning_rate:\n","    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n","\n","\n","#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n","\n","correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-11-a84a5ac6ee35>:16: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From <ipython-input-12-386bf58ae5d0>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pg5-8_uV6kb9"},"source":["## Train the network:"]},{"cell_type":"code","metadata":{"id":"dkucj-DW6kb-","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619792340348,"user_tz":-420,"elapsed":1292575,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"762f1e73-caf3-41c8-c779-a149e7f0fc91"},"source":["test_losses = []\n","test_accuracies = []\n","train_losses = []\n","train_accuracies = []\n","sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","\n","# Perform Training steps with \"batch_size\" amount of data at each loop. \n","# Elements of each batch are chosen randomly, without replacement, from X_train, \n","# restarting when remaining datapoints < batch_size\n","step = 1\n","time_start = time.time()\n","unsampled_indices = list(range(0, len(X_train)))\n","while step * batch_size <= training_iters:\n","    #print (sess.run(learning_rate)) #decaying learning rate\n","    #print (sess.run(global_step)) # global number of iterations\n","    if len(unsampled_indices) < batch_size:\n","        unsampled_indices = list(range(0, len(X_train)))\n","         \n","    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n","    batch_ys = one_hot(raw_labels)\n","    # check that encoded output is same length as num_classes, if not, pad it \n","    if len(batch_ys[0]) < n_classes:\n","        temp_ys = np.zeros((batch_size, n_classes))\n","        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n","        batch_ys = temp_ys\n","       \n","    \n","\n","    # Fit training using batch data\n","    _, loss, acc = sess.run(\n","        [optimizer, cost, accuracy],\n","        feed_dict={\n","            x: batch_xs, \n","            y: batch_ys\n","        }\n","    )\n","    train_losses.append(loss)\n","    train_accuracies.append(acc)\n","    \n","   ### Evaluate network only at some steps for faster training: \n","    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n","        \n","        # To not spam console, show training accuracy/loss in this \"if\"\n","        print(\"Iter #\" + str(step*batch_size) + \\\n","              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n","              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n","              \", Accuracy = {}\".format(acc))\n","        \n","        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n","        loss, acc = sess.run(\n","            [cost, accuracy], \n","            feed_dict={\n","                x: X_test,\n","                y: one_hot(y_test)\n","            }\n","        )\n","        test_losses.append(loss)\n","        test_accuracies.append(acc)\n","        print(\"PERFORMANCE ON TEST SET:             \" + \\\n","              \"Batch Loss = {}\".format(loss) + \\\n","              \", Accuracy = {}\".format(acc))\n","    \n","    step += 1\n","\n","print(\"Optimization Finished!\")\n","\n","# Accuracy for test data\n","\n","one_hot_predictions, accuracy, final_loss = sess.run(\n","    [pred, accuracy, cost],\n","    feed_dict={\n","        x: X_test,\n","        y: one_hot(y_test)\n","    }\n",")\n","\n","test_losses.append(final_loss)\n","test_accuracies.append(accuracy)\n","\n","print(\"FINAL RESULT: \" + \\\n","      \"Batch Loss = {}\".format(final_loss) + \\\n","      \", Accuracy = {}\".format(accuracy))\n","time_stop = time.time()\n","print(\"TOTAL TIME:  {}\".format(time_stop - time_start))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952175855636597, Accuracy = 0.9162076711654663\n","Iter #9603072:  Learning rate = 0.000007:   Batch Loss = 0.565345, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952340364456177, Accuracy = 0.9168753027915955\n","Iter #9604096:  Learning rate = 0.000007:   Batch Loss = 0.610895, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952965021133423, Accuracy = 0.9170422554016113\n","Iter #9605120:  Learning rate = 0.000007:   Batch Loss = 0.562380, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954872369766235, Accuracy = 0.9182106256484985\n","Iter #9606144:  Learning rate = 0.000007:   Batch Loss = 0.534662, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595556914806366, Accuracy = 0.9185444712638855\n","Iter #9607168:  Learning rate = 0.000007:   Batch Loss = 0.565476, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955197811126709, Accuracy = 0.9182106256484985\n","Iter #9608192:  Learning rate = 0.000007:   Batch Loss = 0.578161, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954694151878357, Accuracy = 0.9175429940223694\n","Iter #9609216:  Learning rate = 0.000007:   Batch Loss = 0.518845, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953893661499023, Accuracy = 0.9175429940223694\n","Iter #9610240:  Learning rate = 0.000007:   Batch Loss = 0.508765, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595362663269043, Accuracy = 0.9175429940223694\n","Iter #9611264:  Learning rate = 0.000007:   Batch Loss = 0.523149, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595295786857605, Accuracy = 0.9173760414123535\n","Iter #9612288:  Learning rate = 0.000007:   Batch Loss = 0.578442, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953257083892822, Accuracy = 0.9175429940223694\n","Iter #9613312:  Learning rate = 0.000007:   Batch Loss = 0.577068, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954411029815674, Accuracy = 0.9177098870277405\n","Iter #9614336:  Learning rate = 0.000007:   Batch Loss = 0.562198, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954416990280151, Accuracy = 0.9177098870277405\n","Iter #9615360:  Learning rate = 0.000007:   Batch Loss = 0.603889, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954010486602783, Accuracy = 0.9168753027915955\n","Iter #9616384:  Learning rate = 0.000007:   Batch Loss = 0.537309, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953664779663086, Accuracy = 0.9167084097862244\n","Iter #9617408:  Learning rate = 0.000007:   Batch Loss = 0.572692, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952826738357544, Accuracy = 0.9163745641708374\n","Iter #9618432:  Learning rate = 0.000007:   Batch Loss = 0.534801, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951873064041138, Accuracy = 0.9173760414123535\n","Iter #9619456:  Learning rate = 0.000007:   Batch Loss = 0.652976, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953066349029541, Accuracy = 0.9183775782585144\n","Iter #9620480:  Learning rate = 0.000007:   Batch Loss = 0.513534, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952401161193848, Accuracy = 0.9175429940223694\n","Iter #9621504:  Learning rate = 0.000007:   Batch Loss = 0.567615, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951604843139648, Accuracy = 0.9172091484069824\n","Iter #9622528:  Learning rate = 0.000007:   Batch Loss = 0.527199, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951396226882935, Accuracy = 0.9170422554016113\n","Iter #9623552:  Learning rate = 0.000007:   Batch Loss = 0.556399, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951572060585022, Accuracy = 0.9172091484069824\n","Iter #9624576:  Learning rate = 0.000007:   Batch Loss = 0.500329, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951331257820129, Accuracy = 0.9170422554016113\n","Iter #9625600:  Learning rate = 0.000007:   Batch Loss = 0.550393, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951246619224548, Accuracy = 0.9173760414123535\n","Iter #9626624:  Learning rate = 0.000007:   Batch Loss = 0.558253, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951628684997559, Accuracy = 0.9172091484069824\n","Iter #9627648:  Learning rate = 0.000007:   Batch Loss = 0.562391, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953035950660706, Accuracy = 0.9183775782585144\n","Iter #9628672:  Learning rate = 0.000007:   Batch Loss = 0.523463, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595451295375824, Accuracy = 0.9192121624946594\n","Iter #9629696:  Learning rate = 0.000007:   Batch Loss = 0.607564, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955820083618164, Accuracy = 0.9190452098846436\n","Iter #9630720:  Learning rate = 0.000007:   Batch Loss = 0.555966, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955697298049927, Accuracy = 0.9188783168792725\n","Iter #9631744:  Learning rate = 0.000007:   Batch Loss = 0.522520, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954949855804443, Accuracy = 0.9175429940223694\n","Iter #9632768:  Learning rate = 0.000007:   Batch Loss = 0.531842, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956515669822693, Accuracy = 0.9175429940223694\n","Iter #9633792:  Learning rate = 0.000007:   Batch Loss = 0.587017, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956700444221497, Accuracy = 0.9180437326431274\n","Iter #9634816:  Learning rate = 0.000007:   Batch Loss = 0.556398, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955933332443237, Accuracy = 0.9178768396377563\n","Iter #9635840:  Learning rate = 0.000007:   Batch Loss = 0.556776, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955378413200378, Accuracy = 0.9187114238739014\n","Iter #9636864:  Learning rate = 0.000007:   Batch Loss = 0.541481, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954141020774841, Accuracy = 0.9177098870277405\n","Iter #9637888:  Learning rate = 0.000007:   Batch Loss = 0.510970, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953582525253296, Accuracy = 0.9175429940223694\n","Iter #9638912:  Learning rate = 0.000007:   Batch Loss = 0.597364, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953197479248047, Accuracy = 0.9178768396377563\n","Iter #9639936:  Learning rate = 0.000007:   Batch Loss = 0.532600, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952460765838623, Accuracy = 0.9175429940223694\n","Iter #9640960:  Learning rate = 0.000007:   Batch Loss = 0.555952, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951859951019287, Accuracy = 0.9172091484069824\n","Iter #9641984:  Learning rate = 0.000007:   Batch Loss = 0.552950, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951483249664307, Accuracy = 0.9168753027915955\n","Iter #9643008:  Learning rate = 0.000007:   Batch Loss = 0.517816, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952025055885315, Accuracy = 0.9168753027915955\n","Iter #9644032:  Learning rate = 0.000007:   Batch Loss = 0.542745, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953169465065002, Accuracy = 0.9167084097862244\n","Iter #9645056:  Learning rate = 0.000007:   Batch Loss = 0.488735, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954018831253052, Accuracy = 0.9165414571762085\n","Iter #9646080:  Learning rate = 0.000007:   Batch Loss = 0.521169, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954297780990601, Accuracy = 0.9163745641708374\n","Iter #9647104:  Learning rate = 0.000007:   Batch Loss = 0.488898, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953361988067627, Accuracy = 0.9162076711654663\n","Iter #9648128:  Learning rate = 0.000007:   Batch Loss = 0.546799, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953198671340942, Accuracy = 0.9155399799346924\n","Iter #9649152:  Learning rate = 0.000007:   Batch Loss = 0.507442, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952737927436829, Accuracy = 0.9157068729400635\n","Iter #9650176:  Learning rate = 0.000007:   Batch Loss = 0.512468, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952664017677307, Accuracy = 0.9163745641708374\n","Iter #9651200:  Learning rate = 0.000007:   Batch Loss = 0.567645, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952584147453308, Accuracy = 0.9168753027915955\n","Iter #9652224:  Learning rate = 0.000007:   Batch Loss = 0.544729, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952287912368774, Accuracy = 0.9170422554016113\n","Iter #9653248:  Learning rate = 0.000007:   Batch Loss = 0.625473, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952205061912537, Accuracy = 0.9172091484069824\n","Iter #9654272:  Learning rate = 0.000007:   Batch Loss = 0.576410, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953418612480164, Accuracy = 0.9178768396377563\n","Iter #9655296:  Learning rate = 0.000007:   Batch Loss = 0.541492, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59548020362854, Accuracy = 0.9185444712638855\n","Iter #9656320:  Learning rate = 0.000007:   Batch Loss = 0.540212, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956010818481445, Accuracy = 0.9185444712638855\n","Iter #9657344:  Learning rate = 0.000007:   Batch Loss = 0.528892, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956006646156311, Accuracy = 0.9180437326431274\n","Iter #9658368:  Learning rate = 0.000007:   Batch Loss = 0.609432, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955199599266052, Accuracy = 0.9173760414123535\n","Iter #9659392:  Learning rate = 0.000007:   Batch Loss = 0.597383, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59537672996521, Accuracy = 0.9173760414123535\n","Iter #9660416:  Learning rate = 0.000007:   Batch Loss = 0.510653, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952767133712769, Accuracy = 0.9175429940223694\n","Iter #9661440:  Learning rate = 0.000007:   Batch Loss = 0.563423, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595301628112793, Accuracy = 0.9162076711654663\n","Iter #9662464:  Learning rate = 0.000007:   Batch Loss = 0.529122, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953691005706787, Accuracy = 0.9172091484069824\n","Iter #9663488:  Learning rate = 0.000007:   Batch Loss = 0.563719, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955017805099487, Accuracy = 0.9175429940223694\n","Iter #9664512:  Learning rate = 0.000007:   Batch Loss = 0.523412, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595570981502533, Accuracy = 0.9178768396377563\n","Iter #9665536:  Learning rate = 0.000007:   Batch Loss = 0.620387, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954577922821045, Accuracy = 0.9178768396377563\n","Iter #9666560:  Learning rate = 0.000007:   Batch Loss = 0.565653, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953354239463806, Accuracy = 0.9177098870277405\n","Iter #9667584:  Learning rate = 0.000007:   Batch Loss = 0.533502, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595238208770752, Accuracy = 0.9173760414123535\n","Iter #9668608:  Learning rate = 0.000007:   Batch Loss = 0.536205, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951544046401978, Accuracy = 0.9170422554016113\n","Iter #9669632:  Learning rate = 0.000007:   Batch Loss = 0.530816, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951095819473267, Accuracy = 0.9165414571762085\n","Iter #9670656:  Learning rate = 0.000007:   Batch Loss = 0.580223, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951399803161621, Accuracy = 0.9168753027915955\n","Iter #9671680:  Learning rate = 0.000007:   Batch Loss = 0.567010, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951508283615112, Accuracy = 0.9168753027915955\n","Iter #9672704:  Learning rate = 0.000007:   Batch Loss = 0.565240, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951485633850098, Accuracy = 0.9165414571762085\n","Iter #9673728:  Learning rate = 0.000007:   Batch Loss = 0.474385, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951884984970093, Accuracy = 0.9173760414123535\n","Iter #9674752:  Learning rate = 0.000007:   Batch Loss = 0.627411, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952655673027039, Accuracy = 0.9178768396377563\n","Iter #9675776:  Learning rate = 0.000007:   Batch Loss = 0.607195, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953476428985596, Accuracy = 0.9182106256484985\n","Iter #9676800:  Learning rate = 0.000007:   Batch Loss = 0.544331, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953965187072754, Accuracy = 0.9182106256484985\n","Iter #9677824:  Learning rate = 0.000007:   Batch Loss = 0.545742, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953752994537354, Accuracy = 0.9182106256484985\n","Iter #9678848:  Learning rate = 0.000007:   Batch Loss = 0.571201, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952978134155273, Accuracy = 0.9178768396377563\n","Iter #9679872:  Learning rate = 0.000007:   Batch Loss = 0.486057, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953092575073242, Accuracy = 0.9178768396377563\n","Iter #9680896:  Learning rate = 0.000007:   Batch Loss = 0.586516, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953621864318848, Accuracy = 0.9178768396377563\n","Iter #9681920:  Learning rate = 0.000007:   Batch Loss = 0.471447, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954859256744385, Accuracy = 0.9173760414123535\n","Iter #9682944:  Learning rate = 0.000007:   Batch Loss = 0.579364, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955828428268433, Accuracy = 0.9177098870277405\n","Iter #9683968:  Learning rate = 0.000007:   Batch Loss = 0.537111, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955917239189148, Accuracy = 0.9177098870277405\n","Iter #9684992:  Learning rate = 0.000007:   Batch Loss = 0.529007, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5958421230316162, Accuracy = 0.9188783168792725\n","Iter #9686016:  Learning rate = 0.000007:   Batch Loss = 0.531674, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5958247184753418, Accuracy = 0.9183775782585144\n","Iter #9687040:  Learning rate = 0.000007:   Batch Loss = 0.583758, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956447124481201, Accuracy = 0.9177098870277405\n","Iter #9688064:  Learning rate = 0.000007:   Batch Loss = 0.540761, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595372200012207, Accuracy = 0.9175429940223694\n","Iter #9689088:  Learning rate = 0.000007:   Batch Loss = 0.579080, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952448844909668, Accuracy = 0.9173760414123535\n","Iter #9690112:  Learning rate = 0.000007:   Batch Loss = 0.493960, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952466130256653, Accuracy = 0.9168753027915955\n","Iter #9691136:  Learning rate = 0.000007:   Batch Loss = 0.619537, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952434539794922, Accuracy = 0.9167084097862244\n","Iter #9692160:  Learning rate = 0.000007:   Batch Loss = 0.561040, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953092575073242, Accuracy = 0.9160407185554504\n","Iter #9693184:  Learning rate = 0.000007:   Batch Loss = 0.525728, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595287561416626, Accuracy = 0.9162076711654663\n","Iter #9694208:  Learning rate = 0.000007:   Batch Loss = 0.510414, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595278799533844, Accuracy = 0.9168753027915955\n","Iter #9695232:  Learning rate = 0.000007:   Batch Loss = 0.616003, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953261256217957, Accuracy = 0.9167084097862244\n","Iter #9696256:  Learning rate = 0.000007:   Batch Loss = 0.589521, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953695774078369, Accuracy = 0.9173760414123535\n","Iter #9697280:  Learning rate = 0.000007:   Batch Loss = 0.584735, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954099893569946, Accuracy = 0.9177098870277405\n","Iter #9698304:  Learning rate = 0.000007:   Batch Loss = 0.504538, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595538854598999, Accuracy = 0.9177098870277405\n","Iter #9699328:  Learning rate = 0.000007:   Batch Loss = 0.595404, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956274271011353, Accuracy = 0.9187114238739014\n","Iter #9700352:  Learning rate = 0.000007:   Batch Loss = 0.612822, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595562219619751, Accuracy = 0.9193790555000305\n","Iter #9701376:  Learning rate = 0.000007:   Batch Loss = 0.591306, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953826904296875, Accuracy = 0.9192121624946594\n","Iter #9702400:  Learning rate = 0.000007:   Batch Loss = 0.543302, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952348113059998, Accuracy = 0.9178768396377563\n","Iter #9703424:  Learning rate = 0.000007:   Batch Loss = 0.525842, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951884984970093, Accuracy = 0.9168753027915955\n","Iter #9704448:  Learning rate = 0.000007:   Batch Loss = 0.520299, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952126979827881, Accuracy = 0.9167084097862244\n","Iter #9705472:  Learning rate = 0.000007:   Batch Loss = 0.598481, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952203273773193, Accuracy = 0.9168753027915955\n","Iter #9706496:  Learning rate = 0.000007:   Batch Loss = 0.555513, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952552556991577, Accuracy = 0.9168753027915955\n","Iter #9707520:  Learning rate = 0.000007:   Batch Loss = 0.513247, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952228307723999, Accuracy = 0.9167084097862244\n","Iter #9708544:  Learning rate = 0.000007:   Batch Loss = 0.566812, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951611399650574, Accuracy = 0.9168753027915955\n","Iter #9709568:  Learning rate = 0.000007:   Batch Loss = 0.601519, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595198392868042, Accuracy = 0.9165414571762085\n","Iter #9710592:  Learning rate = 0.000007:   Batch Loss = 0.557895, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952128171920776, Accuracy = 0.9163745641708374\n","Iter #9711616:  Learning rate = 0.000007:   Batch Loss = 0.520824, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952649116516113, Accuracy = 0.9175429940223694\n","Iter #9712640:  Learning rate = 0.000007:   Batch Loss = 0.612734, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952492356300354, Accuracy = 0.9168753027915955\n","Iter #9713664:  Learning rate = 0.000007:   Batch Loss = 0.606338, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952004194259644, Accuracy = 0.9167084097862244\n","Iter #9714688:  Learning rate = 0.000007:   Batch Loss = 0.508188, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952543020248413, Accuracy = 0.9167084097862244\n","Iter #9715712:  Learning rate = 0.000007:   Batch Loss = 0.547182, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952862501144409, Accuracy = 0.9177098870277405\n","Iter #9716736:  Learning rate = 0.000007:   Batch Loss = 0.591491, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953630208969116, Accuracy = 0.9173760414123535\n","Iter #9717760:  Learning rate = 0.000007:   Batch Loss = 0.522254, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955708622932434, Accuracy = 0.9175429940223694\n","Iter #9718784:  Learning rate = 0.000007:   Batch Loss = 0.573229, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5957505106925964, Accuracy = 0.9175429940223694\n","Iter #9719808:  Learning rate = 0.000007:   Batch Loss = 0.580330, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5958135724067688, Accuracy = 0.9170422554016113\n","Iter #9720832:  Learning rate = 0.000007:   Batch Loss = 0.518499, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956720113754272, Accuracy = 0.9173760414123535\n","Iter #9721856:  Learning rate = 0.000007:   Batch Loss = 0.562850, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595488429069519, Accuracy = 0.9180437326431274\n","Iter #9722880:  Learning rate = 0.000007:   Batch Loss = 0.520253, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954625010490417, Accuracy = 0.9178768396377563\n","Iter #9723904:  Learning rate = 0.000007:   Batch Loss = 0.536888, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954634547233582, Accuracy = 0.9178768396377563\n","Iter #9724928:  Learning rate = 0.000007:   Batch Loss = 0.568314, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954163074493408, Accuracy = 0.9175429940223694\n","Iter #9725952:  Learning rate = 0.000007:   Batch Loss = 0.591915, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954160690307617, Accuracy = 0.9177098870277405\n","Iter #9726976:  Learning rate = 0.000007:   Batch Loss = 0.519807, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953388214111328, Accuracy = 0.9177098870277405\n","Iter #9728000:  Learning rate = 0.000007:   Batch Loss = 0.525544, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595201313495636, Accuracy = 0.9168753027915955\n","Iter #9729024:  Learning rate = 0.000007:   Batch Loss = 0.549939, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595139741897583, Accuracy = 0.9172091484069824\n","Iter #9730048:  Learning rate = 0.000007:   Batch Loss = 0.629185, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951440334320068, Accuracy = 0.9170422554016113\n","Iter #9731072:  Learning rate = 0.000007:   Batch Loss = 0.572468, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951117277145386, Accuracy = 0.9170422554016113\n","Iter #9732096:  Learning rate = 0.000007:   Batch Loss = 0.545056, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951125621795654, Accuracy = 0.9170422554016113\n","Iter #9733120:  Learning rate = 0.000007:   Batch Loss = 0.504292, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952025055885315, Accuracy = 0.9177098870277405\n","Iter #9734144:  Learning rate = 0.000007:   Batch Loss = 0.507568, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952801704406738, Accuracy = 0.9183775782585144\n","Iter #9735168:  Learning rate = 0.000007:   Batch Loss = 0.536055, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952049493789673, Accuracy = 0.9175429940223694\n","Iter #9736192:  Learning rate = 0.000007:   Batch Loss = 0.560472, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595164954662323, Accuracy = 0.9170422554016113\n","Iter #9737216:  Learning rate = 0.000007:   Batch Loss = 0.582089, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951752662658691, Accuracy = 0.9168753027915955\n","Iter #9738240:  Learning rate = 0.000007:   Batch Loss = 0.561617, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952490568161011, Accuracy = 0.9172091484069824\n","Iter #9739264:  Learning rate = 0.000007:   Batch Loss = 0.602977, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952832698822021, Accuracy = 0.9175429940223694\n","Iter #9740288:  Learning rate = 0.000007:   Batch Loss = 0.562067, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952919721603394, Accuracy = 0.9170422554016113\n","Iter #9741312:  Learning rate = 0.000007:   Batch Loss = 0.550568, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953497290611267, Accuracy = 0.9165414571762085\n","Iter #9742336:  Learning rate = 0.000007:   Batch Loss = 0.526357, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595382809638977, Accuracy = 0.9167084097862244\n","Iter #9743360:  Learning rate = 0.000007:   Batch Loss = 0.557626, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953571796417236, Accuracy = 0.9168753027915955\n","Iter #9744384:  Learning rate = 0.000007:   Batch Loss = 0.520523, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952802300453186, Accuracy = 0.9177098870277405\n","Iter #9745408:  Learning rate = 0.000007:   Batch Loss = 0.565920, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951855778694153, Accuracy = 0.9177098870277405\n","Iter #9746432:  Learning rate = 0.000007:   Batch Loss = 0.576270, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951475501060486, Accuracy = 0.9173760414123535\n","Iter #9747456:  Learning rate = 0.000007:   Batch Loss = 0.626763, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952070951461792, Accuracy = 0.9170422554016113\n","Iter #9748480:  Learning rate = 0.000007:   Batch Loss = 0.615957, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953028798103333, Accuracy = 0.9168753027915955\n","Iter #9749504:  Learning rate = 0.000007:   Batch Loss = 0.524572, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953412652015686, Accuracy = 0.9173760414123535\n","Iter #9750528:  Learning rate = 0.000007:   Batch Loss = 0.509206, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953860282897949, Accuracy = 0.9173760414123535\n","Iter #9751552:  Learning rate = 0.000007:   Batch Loss = 0.532099, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953667163848877, Accuracy = 0.9177098870277405\n","Iter #9752576:  Learning rate = 0.000007:   Batch Loss = 0.543263, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953451991081238, Accuracy = 0.9180437326431274\n","Iter #9753600:  Learning rate = 0.000007:   Batch Loss = 0.614613, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953421592712402, Accuracy = 0.9177098870277405\n","Iter #9754624:  Learning rate = 0.000007:   Batch Loss = 0.543658, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953969955444336, Accuracy = 0.9178768396377563\n","Iter #9755648:  Learning rate = 0.000007:   Batch Loss = 0.682247, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954537987709045, Accuracy = 0.9180437326431274\n","Iter #9756672:  Learning rate = 0.000007:   Batch Loss = 0.548358, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953292846679688, Accuracy = 0.9178768396377563\n","Iter #9757696:  Learning rate = 0.000007:   Batch Loss = 0.537277, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952513217926025, Accuracy = 0.9168753027915955\n","Iter #9758720:  Learning rate = 0.000007:   Batch Loss = 0.543255, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595342755317688, Accuracy = 0.9163745641708374\n","Iter #9759744:  Learning rate = 0.000007:   Batch Loss = 0.563522, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954753756523132, Accuracy = 0.9162076711654663\n","Iter #9760768:  Learning rate = 0.000007:   Batch Loss = 0.595816, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955489873886108, Accuracy = 0.9160407185554504\n","Iter #9761792:  Learning rate = 0.000007:   Batch Loss = 0.536504, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955417156219482, Accuracy = 0.9160407185554504\n","Iter #9762816:  Learning rate = 0.000007:   Batch Loss = 0.571747, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954227447509766, Accuracy = 0.9172091484069824\n","Iter #9763840:  Learning rate = 0.000007:   Batch Loss = 0.619335, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595359206199646, Accuracy = 0.9178768396377563\n","Iter #9764864:  Learning rate = 0.000007:   Batch Loss = 0.534537, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953783988952637, Accuracy = 0.9185444712638855\n","Iter #9765888:  Learning rate = 0.000007:   Batch Loss = 0.573463, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954885482788086, Accuracy = 0.9198797941207886\n","Iter #9766912:  Learning rate = 0.000007:   Batch Loss = 0.568630, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953881740570068, Accuracy = 0.9198797941207886\n","Iter #9767936:  Learning rate = 0.000007:   Batch Loss = 0.629258, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951833724975586, Accuracy = 0.9182106256484985\n","Iter #9768960:  Learning rate = 0.000007:   Batch Loss = 0.582020, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951269865036011, Accuracy = 0.9170422554016113\n","Iter #9769984:  Learning rate = 0.000007:   Batch Loss = 0.552864, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951070785522461, Accuracy = 0.9167084097862244\n","Iter #9771008:  Learning rate = 0.000007:   Batch Loss = 0.529203, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951489210128784, Accuracy = 0.9165414571762085\n","Iter #9772032:  Learning rate = 0.000007:   Batch Loss = 0.578755, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951670408248901, Accuracy = 0.9167084097862244\n","Iter #9773056:  Learning rate = 0.000007:   Batch Loss = 0.555911, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952328443527222, Accuracy = 0.9175429940223694\n","Iter #9774080:  Learning rate = 0.000007:   Batch Loss = 0.581639, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953209400177002, Accuracy = 0.9178768396377563\n","Iter #9775104:  Learning rate = 0.000007:   Batch Loss = 0.498196, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954626202583313, Accuracy = 0.9178768396377563\n","Iter #9776128:  Learning rate = 0.000007:   Batch Loss = 0.573829, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954476594924927, Accuracy = 0.9178768396377563\n","Iter #9777152:  Learning rate = 0.000007:   Batch Loss = 0.616689, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953402519226074, Accuracy = 0.9177098870277405\n","Iter #9778176:  Learning rate = 0.000007:   Batch Loss = 0.579302, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952630043029785, Accuracy = 0.9177098870277405\n","Iter #9779200:  Learning rate = 0.000007:   Batch Loss = 0.582224, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952065587043762, Accuracy = 0.9170422554016113\n","Iter #9780224:  Learning rate = 0.000006:   Batch Loss = 0.576470, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952067375183105, Accuracy = 0.9167084097862244\n","Iter #9781248:  Learning rate = 0.000006:   Batch Loss = 0.573532, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595180869102478, Accuracy = 0.9172091484069824\n","Iter #9782272:  Learning rate = 0.000006:   Batch Loss = 0.517597, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951857566833496, Accuracy = 0.9175429940223694\n","Iter #9783296:  Learning rate = 0.000006:   Batch Loss = 0.541095, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951781272888184, Accuracy = 0.9172091484069824\n","Iter #9784320:  Learning rate = 0.000006:   Batch Loss = 0.561323, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951722860336304, Accuracy = 0.9173760414123535\n","Iter #9785344:  Learning rate = 0.000006:   Batch Loss = 0.597399, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951876044273376, Accuracy = 0.9172091484069824\n","Iter #9786368:  Learning rate = 0.000006:   Batch Loss = 0.612659, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595237135887146, Accuracy = 0.9175429940223694\n","Iter #9787392:  Learning rate = 0.000006:   Batch Loss = 0.551878, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952515602111816, Accuracy = 0.9173760414123535\n","Iter #9788416:  Learning rate = 0.000006:   Batch Loss = 0.517779, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952622890472412, Accuracy = 0.9172091484069824\n","Iter #9789440:  Learning rate = 0.000006:   Batch Loss = 0.614275, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953645706176758, Accuracy = 0.9178768396377563\n","Iter #9790464:  Learning rate = 0.000006:   Batch Loss = 0.590760, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954174995422363, Accuracy = 0.9180437326431274\n","Iter #9791488:  Learning rate = 0.000006:   Batch Loss = 0.626056, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954593420028687, Accuracy = 0.9178768396377563\n","Iter #9792512:  Learning rate = 0.000006:   Batch Loss = 0.535102, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953667759895325, Accuracy = 0.9177098870277405\n","Iter #9793536:  Learning rate = 0.000006:   Batch Loss = 0.532734, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954227447509766, Accuracy = 0.9177098870277405\n","Iter #9794560:  Learning rate = 0.000006:   Batch Loss = 0.502157, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954128503799438, Accuracy = 0.9177098870277405\n","Iter #9795584:  Learning rate = 0.000006:   Batch Loss = 0.530049, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953470468521118, Accuracy = 0.9175429940223694\n","Iter #9796608:  Learning rate = 0.000006:   Batch Loss = 0.619561, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953316688537598, Accuracy = 0.9172091484069824\n","Iter #9797632:  Learning rate = 0.000006:   Batch Loss = 0.574650, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953894853591919, Accuracy = 0.9170422554016113\n","Iter #9798656:  Learning rate = 0.000006:   Batch Loss = 0.512649, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954421758651733, Accuracy = 0.9175429940223694\n","Iter #9799680:  Learning rate = 0.000006:   Batch Loss = 0.517095, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954256653785706, Accuracy = 0.9178768396377563\n","Iter #9800704:  Learning rate = 0.000006:   Batch Loss = 0.665175, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953824520111084, Accuracy = 0.9178768396377563\n","Iter #9801728:  Learning rate = 0.000006:   Batch Loss = 0.504271, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953325033187866, Accuracy = 0.9177098870277405\n","Iter #9802752:  Learning rate = 0.000006:   Batch Loss = 0.540013, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953770875930786, Accuracy = 0.9180437326431274\n","Iter #9803776:  Learning rate = 0.000006:   Batch Loss = 0.571818, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954627990722656, Accuracy = 0.9183775782585144\n","Iter #9804800:  Learning rate = 0.000006:   Batch Loss = 0.547269, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954006910324097, Accuracy = 0.9182106256484985\n","Iter #9805824:  Learning rate = 0.000006:   Batch Loss = 0.600657, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953176021575928, Accuracy = 0.9178768396377563\n","Iter #9806848:  Learning rate = 0.000006:   Batch Loss = 0.517797, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952607989311218, Accuracy = 0.9178768396377563\n","Iter #9807872:  Learning rate = 0.000006:   Batch Loss = 0.529340, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595297634601593, Accuracy = 0.9188783168792725\n","Iter #9808896:  Learning rate = 0.000006:   Batch Loss = 0.594895, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952513217926025, Accuracy = 0.9188783168792725\n","Iter #9809920:  Learning rate = 0.000006:   Batch Loss = 0.529474, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952352285385132, Accuracy = 0.9185444712638855\n","Iter #9810944:  Learning rate = 0.000006:   Batch Loss = 0.568050, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952008962631226, Accuracy = 0.9168753027915955\n","Iter #9811968:  Learning rate = 0.000006:   Batch Loss = 0.548593, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595235288143158, Accuracy = 0.9172091484069824\n","Iter #9812992:  Learning rate = 0.000006:   Batch Loss = 0.580435, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952563285827637, Accuracy = 0.9167084097862244\n","Iter #9814016:  Learning rate = 0.000006:   Batch Loss = 0.544653, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952931046485901, Accuracy = 0.9170422554016113\n","Iter #9815040:  Learning rate = 0.000006:   Batch Loss = 0.547271, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954104661941528, Accuracy = 0.9178768396377563\n","Iter #9816064:  Learning rate = 0.000006:   Batch Loss = 0.543317, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953994393348694, Accuracy = 0.9175429940223694\n","Iter #9817088:  Learning rate = 0.000006:   Batch Loss = 0.594081, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953874588012695, Accuracy = 0.9177098870277405\n","Iter #9818112:  Learning rate = 0.000006:   Batch Loss = 0.574997, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954197645187378, Accuracy = 0.9178768396377563\n","Iter #9819136:  Learning rate = 0.000006:   Batch Loss = 0.554164, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954233407974243, Accuracy = 0.9180437326431274\n","Iter #9820160:  Learning rate = 0.000006:   Batch Loss = 0.534717, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953220129013062, Accuracy = 0.9177098870277405\n","Iter #9821184:  Learning rate = 0.000006:   Batch Loss = 0.587501, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595283031463623, Accuracy = 0.9178768396377563\n","Iter #9822208:  Learning rate = 0.000006:   Batch Loss = 0.539690, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953301787376404, Accuracy = 0.9180437326431274\n","Iter #9823232:  Learning rate = 0.000006:   Batch Loss = 0.555641, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953914523124695, Accuracy = 0.9183775782585144\n","Iter #9824256:  Learning rate = 0.000006:   Batch Loss = 0.597280, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953031778335571, Accuracy = 0.9182106256484985\n","Iter #9825280:  Learning rate = 0.000006:   Batch Loss = 0.500502, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952178239822388, Accuracy = 0.9170422554016113\n","Iter #9826304:  Learning rate = 0.000006:   Batch Loss = 0.510574, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951886773109436, Accuracy = 0.9168753027915955\n","Iter #9827328:  Learning rate = 0.000006:   Batch Loss = 0.514683, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951562523841858, Accuracy = 0.9172091484069824\n","Iter #9828352:  Learning rate = 0.000006:   Batch Loss = 0.529334, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951340794563293, Accuracy = 0.9167084097862244\n","Iter #9829376:  Learning rate = 0.000006:   Batch Loss = 0.585925, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951892137527466, Accuracy = 0.9165414571762085\n","Iter #9830400:  Learning rate = 0.000006:   Batch Loss = 0.579810, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951827168464661, Accuracy = 0.9167084097862244\n","Iter #9831424:  Learning rate = 0.000006:   Batch Loss = 0.571819, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951958298683167, Accuracy = 0.9167084097862244\n","Iter #9832448:  Learning rate = 0.000006:   Batch Loss = 0.546780, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952404737472534, Accuracy = 0.9170422554016113\n","Iter #9833472:  Learning rate = 0.000006:   Batch Loss = 0.563728, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952435731887817, Accuracy = 0.9172091484069824\n","Iter #9834496:  Learning rate = 0.000006:   Batch Loss = 0.597929, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952638387680054, Accuracy = 0.9177098870277405\n","Iter #9835520:  Learning rate = 0.000006:   Batch Loss = 0.610168, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954779386520386, Accuracy = 0.9182106256484985\n","Iter #9836544:  Learning rate = 0.000006:   Batch Loss = 0.535790, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954744815826416, Accuracy = 0.9180437326431274\n","Iter #9837568:  Learning rate = 0.000006:   Batch Loss = 0.534679, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954157114028931, Accuracy = 0.9178768396377563\n","Iter #9838592:  Learning rate = 0.000006:   Batch Loss = 0.509716, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953726768493652, Accuracy = 0.9178768396377563\n","Iter #9839616:  Learning rate = 0.000006:   Batch Loss = 0.581592, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953305959701538, Accuracy = 0.9178768396377563\n","Iter #9840640:  Learning rate = 0.000006:   Batch Loss = 0.583121, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953621864318848, Accuracy = 0.9178768396377563\n","Iter #9841664:  Learning rate = 0.000006:   Batch Loss = 0.552373, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953869819641113, Accuracy = 0.9177098870277405\n","Iter #9842688:  Learning rate = 0.000006:   Batch Loss = 0.576457, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953174829483032, Accuracy = 0.9175429940223694\n","Iter #9843712:  Learning rate = 0.000006:   Batch Loss = 0.552040, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952434539794922, Accuracy = 0.9172091484069824\n","Iter #9844736:  Learning rate = 0.000006:   Batch Loss = 0.532113, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952179431915283, Accuracy = 0.9173760414123535\n","Iter #9845760:  Learning rate = 0.000006:   Batch Loss = 0.546249, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952105522155762, Accuracy = 0.9178768396377563\n","Iter #9846784:  Learning rate = 0.000006:   Batch Loss = 0.553051, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952267646789551, Accuracy = 0.9180437326431274\n","Iter #9847808:  Learning rate = 0.000006:   Batch Loss = 0.524577, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595166802406311, Accuracy = 0.9165414571762085\n","Iter #9848832:  Learning rate = 0.000006:   Batch Loss = 0.495069, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952008962631226, Accuracy = 0.9177098870277405\n","Iter #9849856:  Learning rate = 0.000006:   Batch Loss = 0.540106, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952293872833252, Accuracy = 0.9172091484069824\n","Iter #9850880:  Learning rate = 0.000006:   Batch Loss = 0.542073, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952665209770203, Accuracy = 0.9177098870277405\n","Iter #9851904:  Learning rate = 0.000006:   Batch Loss = 0.509112, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952780246734619, Accuracy = 0.9173760414123535\n","Iter #9852928:  Learning rate = 0.000006:   Batch Loss = 0.550929, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953049659729004, Accuracy = 0.9177098870277405\n","Iter #9853952:  Learning rate = 0.000006:   Batch Loss = 0.563528, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595302164554596, Accuracy = 0.9173760414123535\n","Iter #9854976:  Learning rate = 0.000006:   Batch Loss = 0.503639, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952436923980713, Accuracy = 0.9170422554016113\n","Iter #9856000:  Learning rate = 0.000006:   Batch Loss = 0.579408, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952153205871582, Accuracy = 0.9158738255500793\n","Iter #9857024:  Learning rate = 0.000006:   Batch Loss = 0.558281, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952905416488647, Accuracy = 0.9158738255500793\n","Iter #9858048:  Learning rate = 0.000006:   Batch Loss = 0.550520, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953572988510132, Accuracy = 0.9158738255500793\n","Iter #9859072:  Learning rate = 0.000006:   Batch Loss = 0.562771, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595355749130249, Accuracy = 0.9162076711654663\n","Iter #9860096:  Learning rate = 0.000006:   Batch Loss = 0.530549, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953514575958252, Accuracy = 0.9165414571762085\n","Iter #9861120:  Learning rate = 0.000006:   Batch Loss = 0.540932, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953638553619385, Accuracy = 0.9180437326431274\n","Iter #9862144:  Learning rate = 0.000006:   Batch Loss = 0.509795, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954332947731018, Accuracy = 0.9182106256484985\n","Iter #9863168:  Learning rate = 0.000006:   Batch Loss = 0.484052, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955653190612793, Accuracy = 0.9188783168792725\n","Iter #9864192:  Learning rate = 0.000006:   Batch Loss = 0.507018, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954611301422119, Accuracy = 0.9187114238739014\n","Iter #9865216:  Learning rate = 0.000006:   Batch Loss = 0.488979, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953456163406372, Accuracy = 0.9185444712638855\n","Iter #9866240:  Learning rate = 0.000006:   Batch Loss = 0.517177, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953317880630493, Accuracy = 0.9180437326431274\n","Iter #9867264:  Learning rate = 0.000006:   Batch Loss = 0.499781, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952699184417725, Accuracy = 0.9178768396377563\n","Iter #9868288:  Learning rate = 0.000006:   Batch Loss = 0.608039, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952227115631104, Accuracy = 0.9172091484069824\n","Iter #9869312:  Learning rate = 0.000006:   Batch Loss = 0.542299, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952204465866089, Accuracy = 0.9167084097862244\n","Iter #9870336:  Learning rate = 0.000006:   Batch Loss = 0.566290, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951766967773438, Accuracy = 0.9167084097862244\n","Iter #9871360:  Learning rate = 0.000006:   Batch Loss = 0.539106, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595185399055481, Accuracy = 0.9168753027915955\n","Iter #9872384:  Learning rate = 0.000006:   Batch Loss = 0.581836, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951945781707764, Accuracy = 0.9167084097862244\n","Iter #9873408:  Learning rate = 0.000006:   Batch Loss = 0.504982, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952222943305969, Accuracy = 0.9175429940223694\n","Iter #9874432:  Learning rate = 0.000006:   Batch Loss = 0.658866, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953223705291748, Accuracy = 0.9178768396377563\n","Iter #9875456:  Learning rate = 0.000006:   Batch Loss = 0.604898, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954461097717285, Accuracy = 0.9178768396377563\n","Iter #9876480:  Learning rate = 0.000006:   Batch Loss = 0.634832, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955512523651123, Accuracy = 0.9173760414123535\n","Iter #9877504:  Learning rate = 0.000006:   Batch Loss = 0.606170, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955023169517517, Accuracy = 0.9178768396377563\n","Iter #9878528:  Learning rate = 0.000006:   Batch Loss = 0.528351, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953666567802429, Accuracy = 0.9178768396377563\n","Iter #9879552:  Learning rate = 0.000006:   Batch Loss = 0.568263, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953269004821777, Accuracy = 0.9177098870277405\n","Iter #9880576:  Learning rate = 0.000006:   Batch Loss = 0.552180, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953531265258789, Accuracy = 0.9178768396377563\n","Iter #9881600:  Learning rate = 0.000006:   Batch Loss = 0.505142, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953640937805176, Accuracy = 0.9177098870277405\n","Iter #9882624:  Learning rate = 0.000006:   Batch Loss = 0.545846, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953168869018555, Accuracy = 0.9173760414123535\n","Iter #9883648:  Learning rate = 0.000006:   Batch Loss = 0.504969, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952789783477783, Accuracy = 0.9175429940223694\n","Iter #9884672:  Learning rate = 0.000006:   Batch Loss = 0.562597, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952807664871216, Accuracy = 0.9175429940223694\n","Iter #9885696:  Learning rate = 0.000006:   Batch Loss = 0.613114, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952266454696655, Accuracy = 0.9175429940223694\n","Iter #9886720:  Learning rate = 0.000006:   Batch Loss = 0.531260, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952036380767822, Accuracy = 0.9172091484069824\n","Iter #9887744:  Learning rate = 0.000006:   Batch Loss = 0.586422, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951852798461914, Accuracy = 0.9170422554016113\n","Iter #9888768:  Learning rate = 0.000006:   Batch Loss = 0.593302, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951880812644958, Accuracy = 0.9167084097862244\n","Iter #9889792:  Learning rate = 0.000006:   Batch Loss = 0.546646, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951566696166992, Accuracy = 0.9168753027915955\n","Iter #9890816:  Learning rate = 0.000006:   Batch Loss = 0.550071, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951458215713501, Accuracy = 0.9168753027915955\n","Iter #9891840:  Learning rate = 0.000006:   Batch Loss = 0.576662, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952281355857849, Accuracy = 0.9175429940223694\n","Iter #9892864:  Learning rate = 0.000006:   Batch Loss = 0.604805, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952813625335693, Accuracy = 0.9183775782585144\n","Iter #9893888:  Learning rate = 0.000006:   Batch Loss = 0.519772, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953150391578674, Accuracy = 0.9183775782585144\n","Iter #9894912:  Learning rate = 0.000006:   Batch Loss = 0.623794, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952606201171875, Accuracy = 0.9175429940223694\n","Iter #9895936:  Learning rate = 0.000006:   Batch Loss = 0.534822, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952973365783691, Accuracy = 0.9177098870277405\n","Iter #9896960:  Learning rate = 0.000006:   Batch Loss = 0.507772, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952976942062378, Accuracy = 0.9172091484069824\n","Iter #9897984:  Learning rate = 0.000006:   Batch Loss = 0.472486, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953149795532227, Accuracy = 0.9168753027915955\n","Iter #9899008:  Learning rate = 0.000006:   Batch Loss = 0.583617, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953298211097717, Accuracy = 0.9168753027915955\n","Iter #9900032:  Learning rate = 0.000006:   Batch Loss = 0.568954, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953605771064758, Accuracy = 0.9167084097862244\n","Iter #9901056:  Learning rate = 0.000006:   Batch Loss = 0.500241, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953647494316101, Accuracy = 0.9172091484069824\n","Iter #9902080:  Learning rate = 0.000006:   Batch Loss = 0.491416, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953836441040039, Accuracy = 0.9175429940223694\n","Iter #9903104:  Learning rate = 0.000006:   Batch Loss = 0.523784, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953817367553711, Accuracy = 0.9173760414123535\n","Iter #9904128:  Learning rate = 0.000006:   Batch Loss = 0.491527, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953776240348816, Accuracy = 0.9180437326431274\n","Iter #9905152:  Learning rate = 0.000006:   Batch Loss = 0.591474, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953583717346191, Accuracy = 0.9177098870277405\n","Iter #9906176:  Learning rate = 0.000006:   Batch Loss = 0.547440, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953356027603149, Accuracy = 0.9168753027915955\n","Iter #9907200:  Learning rate = 0.000006:   Batch Loss = 0.589135, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952820777893066, Accuracy = 0.9172091484069824\n","Iter #9908224:  Learning rate = 0.000006:   Batch Loss = 0.510635, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952577590942383, Accuracy = 0.9177098870277405\n","Iter #9909248:  Learning rate = 0.000006:   Batch Loss = 0.531212, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952072739601135, Accuracy = 0.9170422554016113\n","Iter #9910272:  Learning rate = 0.000006:   Batch Loss = 0.628386, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952097177505493, Accuracy = 0.9172091484069824\n","Iter #9911296:  Learning rate = 0.000006:   Batch Loss = 0.592734, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952093601226807, Accuracy = 0.9175429940223694\n","Iter #9912320:  Learning rate = 0.000006:   Batch Loss = 0.534604, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952278971672058, Accuracy = 0.9167084097862244\n","Iter #9913344:  Learning rate = 0.000006:   Batch Loss = 0.572050, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952977538108826, Accuracy = 0.9170422554016113\n","Iter #9914368:  Learning rate = 0.000006:   Batch Loss = 0.560005, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953258275985718, Accuracy = 0.9162076711654663\n","Iter #9915392:  Learning rate = 0.000006:   Batch Loss = 0.559664, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595266580581665, Accuracy = 0.9167084097862244\n","Iter #9916416:  Learning rate = 0.000006:   Batch Loss = 0.520220, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952491760253906, Accuracy = 0.9173760414123535\n","Iter #9917440:  Learning rate = 0.000006:   Batch Loss = 0.587227, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953049063682556, Accuracy = 0.9175429940223694\n","Iter #9918464:  Learning rate = 0.000006:   Batch Loss = 0.536922, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595341682434082, Accuracy = 0.9177098870277405\n","Iter #9919488:  Learning rate = 0.000006:   Batch Loss = 0.582164, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595317006111145, Accuracy = 0.9177098870277405\n","Iter #9920512:  Learning rate = 0.000006:   Batch Loss = 0.561353, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953651070594788, Accuracy = 0.9185444712638855\n","Iter #9921536:  Learning rate = 0.000006:   Batch Loss = 0.533670, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953396558761597, Accuracy = 0.9183775782585144\n","Iter #9922560:  Learning rate = 0.000006:   Batch Loss = 0.529484, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952682495117188, Accuracy = 0.9178768396377563\n","Iter #9923584:  Learning rate = 0.000006:   Batch Loss = 0.547964, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952505469322205, Accuracy = 0.9173760414123535\n","Iter #9924608:  Learning rate = 0.000006:   Batch Loss = 0.578988, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953110456466675, Accuracy = 0.9170422554016113\n","Iter #9925632:  Learning rate = 0.000006:   Batch Loss = 0.619398, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953750610351562, Accuracy = 0.9165414571762085\n","Iter #9926656:  Learning rate = 0.000006:   Batch Loss = 0.561464, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954083204269409, Accuracy = 0.9162076711654663\n","Iter #9927680:  Learning rate = 0.000006:   Batch Loss = 0.484575, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954228639602661, Accuracy = 0.9162076711654663\n","Iter #9928704:  Learning rate = 0.000006:   Batch Loss = 0.548931, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595420241355896, Accuracy = 0.9160407185554504\n","Iter #9929728:  Learning rate = 0.000006:   Batch Loss = 0.535540, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953590869903564, Accuracy = 0.9172091484069824\n","Iter #9930752:  Learning rate = 0.000006:   Batch Loss = 0.582049, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953372120857239, Accuracy = 0.9177098870277405\n","Iter #9931776:  Learning rate = 0.000006:   Batch Loss = 0.500563, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953171253204346, Accuracy = 0.9177098870277405\n","Iter #9932800:  Learning rate = 0.000006:   Batch Loss = 0.532643, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952610969543457, Accuracy = 0.9172091484069824\n","Iter #9933824:  Learning rate = 0.000006:   Batch Loss = 0.576081, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952631235122681, Accuracy = 0.9175429940223694\n","Iter #9934848:  Learning rate = 0.000006:   Batch Loss = 0.526052, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953432321548462, Accuracy = 0.9180437326431274\n","Iter #9935872:  Learning rate = 0.000006:   Batch Loss = 0.583053, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954118967056274, Accuracy = 0.9178768396377563\n","Iter #9936896:  Learning rate = 0.000006:   Batch Loss = 0.555490, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954219698905945, Accuracy = 0.9178768396377563\n","Iter #9937920:  Learning rate = 0.000006:   Batch Loss = 0.535678, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953844785690308, Accuracy = 0.9178768396377563\n","Iter #9938944:  Learning rate = 0.000006:   Batch Loss = 0.508617, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953951478004456, Accuracy = 0.9178768396377563\n","Iter #9939968:  Learning rate = 0.000006:   Batch Loss = 0.517110, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952669978141785, Accuracy = 0.9175429940223694\n","Iter #9940992:  Learning rate = 0.000006:   Batch Loss = 0.553180, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951887369155884, Accuracy = 0.9168753027915955\n","Iter #9942016:  Learning rate = 0.000006:   Batch Loss = 0.567029, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595190167427063, Accuracy = 0.9168753027915955\n","Iter #9943040:  Learning rate = 0.000006:   Batch Loss = 0.521382, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595203697681427, Accuracy = 0.9168753027915955\n","Iter #9944064:  Learning rate = 0.000006:   Batch Loss = 0.551531, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59522545337677, Accuracy = 0.9173760414123535\n","Iter #9945088:  Learning rate = 0.000006:   Batch Loss = 0.590555, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952430963516235, Accuracy = 0.9177098870277405\n","Iter #9946112:  Learning rate = 0.000006:   Batch Loss = 0.545761, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952293872833252, Accuracy = 0.9178768396377563\n","Iter #9947136:  Learning rate = 0.000006:   Batch Loss = 0.534214, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953136086463928, Accuracy = 0.9178768396377563\n","Iter #9948160:  Learning rate = 0.000006:   Batch Loss = 0.596568, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953474044799805, Accuracy = 0.9180437326431274\n","Iter #9949184:  Learning rate = 0.000006:   Batch Loss = 0.521501, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953181982040405, Accuracy = 0.9178768396377563\n","Iter #9950208:  Learning rate = 0.000006:   Batch Loss = 0.551278, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952948331832886, Accuracy = 0.9187114238739014\n","Iter #9951232:  Learning rate = 0.000006:   Batch Loss = 0.514608, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952529311180115, Accuracy = 0.9182106256484985\n","Iter #9952256:  Learning rate = 0.000006:   Batch Loss = 0.569613, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595227837562561, Accuracy = 0.9188783168792725\n","Iter #9953280:  Learning rate = 0.000006:   Batch Loss = 0.492957, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952137112617493, Accuracy = 0.9182106256484985\n","Iter #9954304:  Learning rate = 0.000006:   Batch Loss = 0.557034, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951098799705505, Accuracy = 0.9165414571762085\n","Iter #9955328:  Learning rate = 0.000006:   Batch Loss = 0.530836, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5950862169265747, Accuracy = 0.9170422554016113\n","Iter #9956352:  Learning rate = 0.000006:   Batch Loss = 0.606390, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951457023620605, Accuracy = 0.9168753027915955\n","Iter #9957376:  Learning rate = 0.000006:   Batch Loss = 0.601108, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951902866363525, Accuracy = 0.9170422554016113\n","Iter #9958400:  Learning rate = 0.000006:   Batch Loss = 0.532934, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952603220939636, Accuracy = 0.9173760414123535\n","Iter #9959424:  Learning rate = 0.000006:   Batch Loss = 0.660452, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954201221466064, Accuracy = 0.9177098870277405\n","Iter #9960448:  Learning rate = 0.000006:   Batch Loss = 0.493155, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955851674079895, Accuracy = 0.9175429940223694\n","Iter #9961472:  Learning rate = 0.000006:   Batch Loss = 0.537327, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956969857215881, Accuracy = 0.9173760414123535\n","Iter #9962496:  Learning rate = 0.000006:   Batch Loss = 0.515074, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956664681434631, Accuracy = 0.9177098870277405\n","Iter #9963520:  Learning rate = 0.000006:   Batch Loss = 0.527300, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954602360725403, Accuracy = 0.9180437326431274\n","Iter #9964544:  Learning rate = 0.000006:   Batch Loss = 0.532519, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953527688980103, Accuracy = 0.9183775782585144\n","Iter #9965568:  Learning rate = 0.000006:   Batch Loss = 0.523217, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953207015991211, Accuracy = 0.9183775782585144\n","Iter #9966592:  Learning rate = 0.000006:   Batch Loss = 0.614808, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953282117843628, Accuracy = 0.9182106256484985\n","Iter #9967616:  Learning rate = 0.000006:   Batch Loss = 0.509741, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953037738800049, Accuracy = 0.9180437326431274\n","Iter #9968640:  Learning rate = 0.000006:   Batch Loss = 0.514118, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953401327133179, Accuracy = 0.9180437326431274\n","Iter #9969664:  Learning rate = 0.000006:   Batch Loss = 0.577521, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953546762466431, Accuracy = 0.9178768396377563\n","Iter #9970688:  Learning rate = 0.000006:   Batch Loss = 0.528393, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953706502914429, Accuracy = 0.9178768396377563\n","Iter #9971712:  Learning rate = 0.000006:   Batch Loss = 0.576785, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953212976455688, Accuracy = 0.9178768396377563\n","Iter #9972736:  Learning rate = 0.000006:   Batch Loss = 0.532635, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953353643417358, Accuracy = 0.9175429940223694\n","Iter #9973760:  Learning rate = 0.000006:   Batch Loss = 0.493826, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954190492630005, Accuracy = 0.9177098870277405\n","Iter #9974784:  Learning rate = 0.000006:   Batch Loss = 0.533273, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954390168190002, Accuracy = 0.9175429940223694\n","Iter #9975808:  Learning rate = 0.000006:   Batch Loss = 0.511533, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954408645629883, Accuracy = 0.9178768396377563\n","Iter #9976832:  Learning rate = 0.000006:   Batch Loss = 0.518841, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954753160476685, Accuracy = 0.9177098870277405\n","Iter #9977856:  Learning rate = 0.000006:   Batch Loss = 0.615911, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955249667167664, Accuracy = 0.9180437326431274\n","Iter #9978880:  Learning rate = 0.000006:   Batch Loss = 0.560757, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955044031143188, Accuracy = 0.9180437326431274\n","Iter #9979904:  Learning rate = 0.000006:   Batch Loss = 0.581038, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954714417457581, Accuracy = 0.9180437326431274\n","Iter #9980928:  Learning rate = 0.000006:   Batch Loss = 0.516544, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953646898269653, Accuracy = 0.9180437326431274\n","Iter #9981952:  Learning rate = 0.000006:   Batch Loss = 0.542193, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952430963516235, Accuracy = 0.9177098870277405\n","Iter #9982976:  Learning rate = 0.000006:   Batch Loss = 0.496546, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595186173915863, Accuracy = 0.9168753027915955\n","Iter #9984000:  Learning rate = 0.000006:   Batch Loss = 0.528903, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951563119888306, Accuracy = 0.9168753027915955\n","Iter #9985024:  Learning rate = 0.000006:   Batch Loss = 0.584973, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951497554779053, Accuracy = 0.9168753027915955\n","Iter #9986048:  Learning rate = 0.000006:   Batch Loss = 0.615588, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951632857322693, Accuracy = 0.9168753027915955\n","Iter #9987072:  Learning rate = 0.000006:   Batch Loss = 0.554926, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951381921768188, Accuracy = 0.9165414571762085\n","Iter #9988096:  Learning rate = 0.000006:   Batch Loss = 0.507358, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951360464096069, Accuracy = 0.9162076711654663\n","Iter #9989120:  Learning rate = 0.000006:   Batch Loss = 0.570284, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951396226882935, Accuracy = 0.9160407185554504\n","Iter #9990144:  Learning rate = 0.000006:   Batch Loss = 0.601313, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951381325721741, Accuracy = 0.9172091484069824\n","Iter #9991168:  Learning rate = 0.000006:   Batch Loss = 0.572553, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951516032218933, Accuracy = 0.9168753027915955\n","Iter #9992192:  Learning rate = 0.000006:   Batch Loss = 0.542805, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952171087265015, Accuracy = 0.9173760414123535\n","Iter #9993216:  Learning rate = 0.000006:   Batch Loss = 0.568517, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952095985412598, Accuracy = 0.9173760414123535\n","Iter #9994240:  Learning rate = 0.000006:   Batch Loss = 0.647619, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951855778694153, Accuracy = 0.9172091484069824\n","Iter #9995264:  Learning rate = 0.000006:   Batch Loss = 0.502025, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952309370040894, Accuracy = 0.9177098870277405\n","Iter #9996288:  Learning rate = 0.000006:   Batch Loss = 0.495717, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953243970870972, Accuracy = 0.9177098870277405\n","Iter #9997312:  Learning rate = 0.000006:   Batch Loss = 0.631809, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953378081321716, Accuracy = 0.9178768396377563\n","Iter #9998336:  Learning rate = 0.000006:   Batch Loss = 0.578492, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595341682434082, Accuracy = 0.9177098870277405\n","Iter #9999360:  Learning rate = 0.000006:   Batch Loss = 0.621467, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953486561775208, Accuracy = 0.9178768396377563\n","Iter #10000384:  Learning rate = 0.000006:   Batch Loss = 0.538397, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595343291759491, Accuracy = 0.9177098870277405\n","Iter #10001408:  Learning rate = 0.000006:   Batch Loss = 0.548054, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953952074050903, Accuracy = 0.9178768396377563\n","Iter #10002432:  Learning rate = 0.000006:   Batch Loss = 0.501230, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954321622848511, Accuracy = 0.9173760414123535\n","Iter #10003456:  Learning rate = 0.000006:   Batch Loss = 0.565659, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953860282897949, Accuracy = 0.9177098870277405\n","Iter #10004480:  Learning rate = 0.000006:   Batch Loss = 0.603644, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953143835067749, Accuracy = 0.9173760414123535\n","Iter #10005504:  Learning rate = 0.000006:   Batch Loss = 0.584078, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952352285385132, Accuracy = 0.9172091484069824\n","Iter #10006528:  Learning rate = 0.000006:   Batch Loss = 0.587974, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951769351959229, Accuracy = 0.9172091484069824\n","Iter #10007552:  Learning rate = 0.000006:   Batch Loss = 0.570224, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952396988868713, Accuracy = 0.9177098870277405\n","Iter #10008576:  Learning rate = 0.000006:   Batch Loss = 0.620417, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952154397964478, Accuracy = 0.9173760414123535\n","Iter #10009600:  Learning rate = 0.000006:   Batch Loss = 0.510150, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952434539794922, Accuracy = 0.9175429940223694\n","Iter #10010624:  Learning rate = 0.000006:   Batch Loss = 0.573224, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952059030532837, Accuracy = 0.9172091484069824\n","Iter #10011648:  Learning rate = 0.000006:   Batch Loss = 0.520598, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951938033103943, Accuracy = 0.9172091484069824\n","Iter #10012672:  Learning rate = 0.000006:   Batch Loss = 0.527808, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951944589614868, Accuracy = 0.9175429940223694\n","Iter #10013696:  Learning rate = 0.000006:   Batch Loss = 0.528417, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951937437057495, Accuracy = 0.9167084097862244\n","Iter #10014720:  Learning rate = 0.000006:   Batch Loss = 0.611901, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951814651489258, Accuracy = 0.9167084097862244\n","Iter #10015744:  Learning rate = 0.000006:   Batch Loss = 0.536104, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952684879302979, Accuracy = 0.9172091484069824\n","Iter #10016768:  Learning rate = 0.000006:   Batch Loss = 0.504745, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952826738357544, Accuracy = 0.9178768396377563\n","Iter #10017792:  Learning rate = 0.000006:   Batch Loss = 0.566323, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952129364013672, Accuracy = 0.9178768396377563\n","Iter #10018816:  Learning rate = 0.000006:   Batch Loss = 0.627137, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952504277229309, Accuracy = 0.9177098870277405\n","Iter #10019840:  Learning rate = 0.000006:   Batch Loss = 0.558720, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953117609024048, Accuracy = 0.9177098870277405\n","Iter #10020864:  Learning rate = 0.000005:   Batch Loss = 0.559183, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953741669654846, Accuracy = 0.9173760414123535\n","Iter #10021888:  Learning rate = 0.000005:   Batch Loss = 0.501541, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954620838165283, Accuracy = 0.9172091484069824\n","Iter #10022912:  Learning rate = 0.000005:   Batch Loss = 0.564721, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954671502113342, Accuracy = 0.9168753027915955\n","Iter #10023936:  Learning rate = 0.000005:   Batch Loss = 0.505641, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954757928848267, Accuracy = 0.9170422554016113\n","Iter #10024960:  Learning rate = 0.000005:   Batch Loss = 0.631756, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953909158706665, Accuracy = 0.9175429940223694\n","Iter #10025984:  Learning rate = 0.000005:   Batch Loss = 0.590479, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59526127576828, Accuracy = 0.9178768396377563\n","Iter #10027008:  Learning rate = 0.000005:   Batch Loss = 0.613743, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595288872718811, Accuracy = 0.9177098870277405\n","Iter #10028032:  Learning rate = 0.000005:   Batch Loss = 0.514835, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953357219696045, Accuracy = 0.9178768396377563\n","Iter #10029056:  Learning rate = 0.000005:   Batch Loss = 0.545886, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954168438911438, Accuracy = 0.9180437326431274\n","Iter #10030080:  Learning rate = 0.000005:   Batch Loss = 0.587743, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953589677810669, Accuracy = 0.9175429940223694\n","Iter #10031104:  Learning rate = 0.000005:   Batch Loss = 0.561020, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952808856964111, Accuracy = 0.9177098870277405\n","Iter #10032128:  Learning rate = 0.000005:   Batch Loss = 0.625142, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951910018920898, Accuracy = 0.9170422554016113\n","Iter #10033152:  Learning rate = 0.000005:   Batch Loss = 0.639471, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951442718505859, Accuracy = 0.9170422554016113\n","Iter #10034176:  Learning rate = 0.000005:   Batch Loss = 0.539971, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951212644577026, Accuracy = 0.9167084097862244\n","Iter #10035200:  Learning rate = 0.000005:   Batch Loss = 0.563974, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951375961303711, Accuracy = 0.9167084097862244\n","Iter #10036224:  Learning rate = 0.000005:   Batch Loss = 0.514759, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951555967330933, Accuracy = 0.9170422554016113\n","Iter #10037248:  Learning rate = 0.000005:   Batch Loss = 0.633743, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951581001281738, Accuracy = 0.9170422554016113\n","Iter #10038272:  Learning rate = 0.000005:   Batch Loss = 0.540251, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595179557800293, Accuracy = 0.9173760414123535\n","Iter #10039296:  Learning rate = 0.000005:   Batch Loss = 0.542071, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595224142074585, Accuracy = 0.9173760414123535\n","Iter #10040320:  Learning rate = 0.000005:   Batch Loss = 0.489936, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952650308609009, Accuracy = 0.9178768396377563\n","Iter #10041344:  Learning rate = 0.000005:   Batch Loss = 0.607120, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952715873718262, Accuracy = 0.9178768396377563\n","Iter #10042368:  Learning rate = 0.000005:   Batch Loss = 0.561369, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953096151351929, Accuracy = 0.9178768396377563\n","Iter #10043392:  Learning rate = 0.000005:   Batch Loss = 0.550461, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953315496444702, Accuracy = 0.9177098870277405\n","Iter #10044416:  Learning rate = 0.000005:   Batch Loss = 0.554247, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953125953674316, Accuracy = 0.9175429940223694\n","Iter #10045440:  Learning rate = 0.000005:   Batch Loss = 0.542744, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952731370925903, Accuracy = 0.9168753027915955\n","Iter #10046464:  Learning rate = 0.000005:   Batch Loss = 0.538018, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595259964466095, Accuracy = 0.9165414571762085\n","Iter #10047488:  Learning rate = 0.000005:   Batch Loss = 0.613402, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952695608139038, Accuracy = 0.9170422554016113\n","Iter #10048512:  Learning rate = 0.000005:   Batch Loss = 0.562856, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953603386878967, Accuracy = 0.9175429940223694\n","Iter #10049536:  Learning rate = 0.000005:   Batch Loss = 0.601914, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953598022460938, Accuracy = 0.9180437326431274\n","Iter #10050560:  Learning rate = 0.000005:   Batch Loss = 0.512232, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953552722930908, Accuracy = 0.9180437326431274\n","Iter #10051584:  Learning rate = 0.000005:   Batch Loss = 0.557498, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953478217124939, Accuracy = 0.9178768396377563\n","Iter #10052608:  Learning rate = 0.000005:   Batch Loss = 0.556087, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953251123428345, Accuracy = 0.9177098870277405\n","Iter #10053632:  Learning rate = 0.000005:   Batch Loss = 0.521822, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952948331832886, Accuracy = 0.9183775782585144\n","Iter #10054656:  Learning rate = 0.000005:   Batch Loss = 0.587559, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953061580657959, Accuracy = 0.9180437326431274\n","Iter #10055680:  Learning rate = 0.000005:   Batch Loss = 0.583866, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953081250190735, Accuracy = 0.9180437326431274\n","Iter #10056704:  Learning rate = 0.000005:   Batch Loss = 0.589570, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952606201171875, Accuracy = 0.9178768396377563\n","Iter #10057728:  Learning rate = 0.000005:   Batch Loss = 0.486676, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952351093292236, Accuracy = 0.9173760414123535\n","Iter #10058752:  Learning rate = 0.000005:   Batch Loss = 0.490317, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953069925308228, Accuracy = 0.9178768396377563\n","Iter #10059776:  Learning rate = 0.000005:   Batch Loss = 0.575804, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954651832580566, Accuracy = 0.9183775782585144\n","Iter #10060800:  Learning rate = 0.000005:   Batch Loss = 0.559929, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955610275268555, Accuracy = 0.9183775782585144\n","Iter #10061824:  Learning rate = 0.000005:   Batch Loss = 0.601737, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955238938331604, Accuracy = 0.9182106256484985\n","Iter #10062848:  Learning rate = 0.000005:   Batch Loss = 0.483684, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955052971839905, Accuracy = 0.9182106256484985\n","Iter #10063872:  Learning rate = 0.000005:   Batch Loss = 0.526942, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953695774078369, Accuracy = 0.9178768396377563\n","Iter #10064896:  Learning rate = 0.000005:   Batch Loss = 0.585892, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952781438827515, Accuracy = 0.9175429940223694\n","Iter #10065920:  Learning rate = 0.000005:   Batch Loss = 0.531744, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952467918395996, Accuracy = 0.9172091484069824\n","Iter #10066944:  Learning rate = 0.000005:   Batch Loss = 0.538932, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952585935592651, Accuracy = 0.9168753027915955\n","Iter #10067968:  Learning rate = 0.000005:   Batch Loss = 0.503196, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952717661857605, Accuracy = 0.9167084097862244\n","Iter #10068992:  Learning rate = 0.000005:   Batch Loss = 0.533861, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952165126800537, Accuracy = 0.9167084097862244\n","Iter #10070016:  Learning rate = 0.000005:   Batch Loss = 0.518778, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952045917510986, Accuracy = 0.9168753027915955\n","Iter #10071040:  Learning rate = 0.000005:   Batch Loss = 0.578197, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952368974685669, Accuracy = 0.9167084097862244\n","Iter #10072064:  Learning rate = 0.000005:   Batch Loss = 0.560388, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59522545337677, Accuracy = 0.9167084097862244\n","Iter #10073088:  Learning rate = 0.000005:   Batch Loss = 0.497586, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952227115631104, Accuracy = 0.9158738255500793\n","Iter #10074112:  Learning rate = 0.000005:   Batch Loss = 0.537385, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952025055885315, Accuracy = 0.9160407185554504\n","Iter #10075136:  Learning rate = 0.000005:   Batch Loss = 0.626186, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952320098876953, Accuracy = 0.9167084097862244\n","Iter #10076160:  Learning rate = 0.000005:   Batch Loss = 0.497284, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952921509742737, Accuracy = 0.9167084097862244\n","Iter #10077184:  Learning rate = 0.000005:   Batch Loss = 0.599443, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952738523483276, Accuracy = 0.9168753027915955\n","Iter #10078208:  Learning rate = 0.000005:   Batch Loss = 0.535286, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952428579330444, Accuracy = 0.9173760414123535\n","Iter #10079232:  Learning rate = 0.000005:   Batch Loss = 0.672225, Accuracy = 0.8671875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952847003936768, Accuracy = 0.9177098870277405\n","Iter #10080256:  Learning rate = 0.000005:   Batch Loss = 0.623075, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953677892684937, Accuracy = 0.9178768396377563\n","Iter #10081280:  Learning rate = 0.000005:   Batch Loss = 0.560225, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953631401062012, Accuracy = 0.9180437326431274\n","Iter #10082304:  Learning rate = 0.000005:   Batch Loss = 0.555979, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953109264373779, Accuracy = 0.9180437326431274\n","Iter #10083328:  Learning rate = 0.000005:   Batch Loss = 0.546532, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952414274215698, Accuracy = 0.9180437326431274\n","Iter #10084352:  Learning rate = 0.000005:   Batch Loss = 0.478707, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952385663986206, Accuracy = 0.9178768396377563\n","Iter #10085376:  Learning rate = 0.000005:   Batch Loss = 0.510051, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952179431915283, Accuracy = 0.9177098870277405\n","Iter #10086400:  Learning rate = 0.000005:   Batch Loss = 0.672214, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952891111373901, Accuracy = 0.9177098870277405\n","Iter #10087424:  Learning rate = 0.000005:   Batch Loss = 0.508304, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953221321105957, Accuracy = 0.9175429940223694\n","Iter #10088448:  Learning rate = 0.000005:   Batch Loss = 0.583587, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953364372253418, Accuracy = 0.9177098870277405\n","Iter #10089472:  Learning rate = 0.000005:   Batch Loss = 0.523082, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953900814056396, Accuracy = 0.9175429940223694\n","Iter #10090496:  Learning rate = 0.000005:   Batch Loss = 0.535645, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953525304794312, Accuracy = 0.9175429940223694\n","Iter #10091520:  Learning rate = 0.000005:   Batch Loss = 0.533011, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952991247177124, Accuracy = 0.9177098870277405\n","Iter #10092544:  Learning rate = 0.000005:   Batch Loss = 0.531239, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952937602996826, Accuracy = 0.9177098870277405\n","Iter #10093568:  Learning rate = 0.000005:   Batch Loss = 0.523440, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952978134155273, Accuracy = 0.9177098870277405\n","Iter #10094592:  Learning rate = 0.000005:   Batch Loss = 0.566312, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953700542449951, Accuracy = 0.9168753027915955\n","Iter #10095616:  Learning rate = 0.000005:   Batch Loss = 0.473288, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953670740127563, Accuracy = 0.9173760414123535\n","Iter #10096640:  Learning rate = 0.000005:   Batch Loss = 0.575850, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954153537750244, Accuracy = 0.9173760414123535\n","Iter #10097664:  Learning rate = 0.000005:   Batch Loss = 0.574359, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595409631729126, Accuracy = 0.9182106256484985\n","Iter #10098688:  Learning rate = 0.000005:   Batch Loss = 0.582723, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953856110572815, Accuracy = 0.9180437326431274\n","Iter #10099712:  Learning rate = 0.000005:   Batch Loss = 0.593547, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953317284584045, Accuracy = 0.9177098870277405\n","Iter #10100736:  Learning rate = 0.000005:   Batch Loss = 0.544652, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952918529510498, Accuracy = 0.9178768396377563\n","Iter #10101760:  Learning rate = 0.000005:   Batch Loss = 0.627236, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952529907226562, Accuracy = 0.9170422554016113\n","Iter #10102784:  Learning rate = 0.000005:   Batch Loss = 0.571249, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952450037002563, Accuracy = 0.9180437326431274\n","Iter #10103808:  Learning rate = 0.000005:   Batch Loss = 0.547576, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952523946762085, Accuracy = 0.9177098870277405\n","Iter #10104832:  Learning rate = 0.000005:   Batch Loss = 0.581021, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952197313308716, Accuracy = 0.9175429940223694\n","Iter #10105856:  Learning rate = 0.000005:   Batch Loss = 0.498733, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952216386795044, Accuracy = 0.9175429940223694\n","Iter #10106880:  Learning rate = 0.000005:   Batch Loss = 0.574622, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952240228652954, Accuracy = 0.9175429940223694\n","Iter #10107904:  Learning rate = 0.000005:   Batch Loss = 0.513071, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595259428024292, Accuracy = 0.9177098870277405\n","Iter #10108928:  Learning rate = 0.000005:   Batch Loss = 0.561284, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953158140182495, Accuracy = 0.9175429940223694\n","Iter #10109952:  Learning rate = 0.000005:   Batch Loss = 0.569268, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953250527381897, Accuracy = 0.9175429940223694\n","Iter #10110976:  Learning rate = 0.000005:   Batch Loss = 0.516031, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953180193901062, Accuracy = 0.9177098870277405\n","Iter #10112000:  Learning rate = 0.000005:   Batch Loss = 0.584039, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953360199928284, Accuracy = 0.9180437326431274\n","Iter #10113024:  Learning rate = 0.000005:   Batch Loss = 0.633790, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953203439712524, Accuracy = 0.9180437326431274\n","Iter #10114048:  Learning rate = 0.000005:   Batch Loss = 0.539025, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595316469669342, Accuracy = 0.9175429940223694\n","Iter #10115072:  Learning rate = 0.000005:   Batch Loss = 0.537481, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952771902084351, Accuracy = 0.9173760414123535\n","Iter #10116096:  Learning rate = 0.000005:   Batch Loss = 0.551355, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595235288143158, Accuracy = 0.9175429940223694\n","Iter #10117120:  Learning rate = 0.000005:   Batch Loss = 0.540716, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595194935798645, Accuracy = 0.9172091484069824\n","Iter #10118144:  Learning rate = 0.000005:   Batch Loss = 0.618328, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951405763626099, Accuracy = 0.9170422554016113\n","Iter #10119168:  Learning rate = 0.000005:   Batch Loss = 0.551206, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951042175292969, Accuracy = 0.9165414571762085\n","Iter #10120192:  Learning rate = 0.000005:   Batch Loss = 0.549276, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59513258934021, Accuracy = 0.9163745641708374\n","Iter #10121216:  Learning rate = 0.000005:   Batch Loss = 0.613209, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595220685005188, Accuracy = 0.9170422554016113\n","Iter #10122240:  Learning rate = 0.000005:   Batch Loss = 0.506485, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952928066253662, Accuracy = 0.9182106256484985\n","Iter #10123264:  Learning rate = 0.000005:   Batch Loss = 0.543087, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953432321548462, Accuracy = 0.9178768396377563\n","Iter #10124288:  Learning rate = 0.000005:   Batch Loss = 0.628239, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953090190887451, Accuracy = 0.9175429940223694\n","Iter #10125312:  Learning rate = 0.000005:   Batch Loss = 0.554740, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952742099761963, Accuracy = 0.9173760414123535\n","Iter #10126336:  Learning rate = 0.000005:   Batch Loss = 0.596889, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952365398406982, Accuracy = 0.9168753027915955\n","Iter #10127360:  Learning rate = 0.000005:   Batch Loss = 0.570721, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048301696777, Accuracy = 0.9167084097862244\n","Iter #10128384:  Learning rate = 0.000005:   Batch Loss = 0.582564, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951511859893799, Accuracy = 0.9173760414123535\n","Iter #10129408:  Learning rate = 0.000005:   Batch Loss = 0.517602, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951735377311707, Accuracy = 0.9172091484069824\n","Iter #10130432:  Learning rate = 0.000005:   Batch Loss = 0.545814, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952190160751343, Accuracy = 0.9175429940223694\n","Iter #10131456:  Learning rate = 0.000005:   Batch Loss = 0.553296, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951437950134277, Accuracy = 0.9172091484069824\n","Iter #10132480:  Learning rate = 0.000005:   Batch Loss = 0.619066, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951098799705505, Accuracy = 0.9172091484069824\n","Iter #10133504:  Learning rate = 0.000005:   Batch Loss = 0.530357, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951237082481384, Accuracy = 0.9165414571762085\n","Iter #10134528:  Learning rate = 0.000005:   Batch Loss = 0.576206, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951250791549683, Accuracy = 0.9170422554016113\n","Iter #10135552:  Learning rate = 0.000005:   Batch Loss = 0.570649, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951300263404846, Accuracy = 0.9170422554016113\n","Iter #10136576:  Learning rate = 0.000005:   Batch Loss = 0.487155, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951314568519592, Accuracy = 0.9170422554016113\n","Iter #10137600:  Learning rate = 0.000005:   Batch Loss = 0.530455, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951642990112305, Accuracy = 0.9172091484069824\n","Iter #10138624:  Learning rate = 0.000005:   Batch Loss = 0.560713, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952571630477905, Accuracy = 0.9175429940223694\n","Iter #10139648:  Learning rate = 0.000005:   Batch Loss = 0.634055, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595359742641449, Accuracy = 0.9175429940223694\n","Iter #10140672:  Learning rate = 0.000005:   Batch Loss = 0.582352, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595531702041626, Accuracy = 0.9178768396377563\n","Iter #10141696:  Learning rate = 0.000005:   Batch Loss = 0.564052, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954715609550476, Accuracy = 0.9177098870277405\n","Iter #10142720:  Learning rate = 0.000005:   Batch Loss = 0.485202, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953642725944519, Accuracy = 0.9178768396377563\n","Iter #10143744:  Learning rate = 0.000005:   Batch Loss = 0.610671, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953125953674316, Accuracy = 0.9177098870277405\n","Iter #10144768:  Learning rate = 0.000005:   Batch Loss = 0.586092, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595311164855957, Accuracy = 0.9175429940223694\n","Iter #10145792:  Learning rate = 0.000005:   Batch Loss = 0.546407, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954214334487915, Accuracy = 0.9178768396377563\n","Iter #10146816:  Learning rate = 0.000005:   Batch Loss = 0.556323, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954331159591675, Accuracy = 0.9180437326431274\n","Iter #10147840:  Learning rate = 0.000005:   Batch Loss = 0.536376, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953954458236694, Accuracy = 0.9178768396377563\n","Iter #10148864:  Learning rate = 0.000005:   Batch Loss = 0.551187, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953917503356934, Accuracy = 0.9178768396377563\n","Iter #10149888:  Learning rate = 0.000005:   Batch Loss = 0.544080, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954498052597046, Accuracy = 0.9180437326431274\n","Iter #10150912:  Learning rate = 0.000005:   Batch Loss = 0.491198, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954901576042175, Accuracy = 0.9180437326431274\n","Iter #10151936:  Learning rate = 0.000005:   Batch Loss = 0.615045, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953959226608276, Accuracy = 0.9175429940223694\n","Iter #10152960:  Learning rate = 0.000005:   Batch Loss = 0.514962, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952492952346802, Accuracy = 0.9173760414123535\n","Iter #10153984:  Learning rate = 0.000005:   Batch Loss = 0.623340, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952286720275879, Accuracy = 0.9172091484069824\n","Iter #10155008:  Learning rate = 0.000005:   Batch Loss = 0.540821, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952206254005432, Accuracy = 0.9167084097862244\n","Iter #10156032:  Learning rate = 0.000005:   Batch Loss = 0.563657, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952714681625366, Accuracy = 0.9173760414123535\n","Iter #10157056:  Learning rate = 0.000005:   Batch Loss = 0.552838, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952960252761841, Accuracy = 0.9177098870277405\n","Iter #10158080:  Learning rate = 0.000005:   Batch Loss = 0.498493, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952730178833008, Accuracy = 0.9177098870277405\n","Iter #10159104:  Learning rate = 0.000005:   Batch Loss = 0.516396, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952370762825012, Accuracy = 0.9173760414123535\n","Iter #10160128:  Learning rate = 0.000005:   Batch Loss = 0.519716, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952097773551941, Accuracy = 0.9177098870277405\n","Iter #10161152:  Learning rate = 0.000005:   Batch Loss = 0.659800, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952922105789185, Accuracy = 0.9178768396377563\n","Iter #10162176:  Learning rate = 0.000005:   Batch Loss = 0.547346, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954204797744751, Accuracy = 0.9178768396377563\n","Iter #10163200:  Learning rate = 0.000005:   Batch Loss = 0.588381, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595402717590332, Accuracy = 0.9180437326431274\n","Iter #10164224:  Learning rate = 0.000005:   Batch Loss = 0.526535, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953390002250671, Accuracy = 0.9177098870277405\n","Iter #10165248:  Learning rate = 0.000005:   Batch Loss = 0.619981, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952622294425964, Accuracy = 0.9173760414123535\n","Iter #10166272:  Learning rate = 0.000005:   Batch Loss = 0.524165, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951880216598511, Accuracy = 0.9172091484069824\n","Iter #10167296:  Learning rate = 0.000005:   Batch Loss = 0.630255, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951449275016785, Accuracy = 0.9170422554016113\n","Iter #10168320:  Learning rate = 0.000005:   Batch Loss = 0.548070, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951519012451172, Accuracy = 0.9172091484069824\n","Iter #10169344:  Learning rate = 0.000005:   Batch Loss = 0.575393, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951920747756958, Accuracy = 0.9172091484069824\n","Iter #10170368:  Learning rate = 0.000005:   Batch Loss = 0.581134, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595196008682251, Accuracy = 0.9173760414123535\n","Iter #10171392:  Learning rate = 0.000005:   Batch Loss = 0.530623, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951576232910156, Accuracy = 0.9173760414123535\n","Iter #10172416:  Learning rate = 0.000005:   Batch Loss = 0.554328, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595142126083374, Accuracy = 0.9170422554016113\n","Iter #10173440:  Learning rate = 0.000005:   Batch Loss = 0.578169, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951546430587769, Accuracy = 0.9173760414123535\n","Iter #10174464:  Learning rate = 0.000005:   Batch Loss = 0.606565, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952053070068359, Accuracy = 0.9173760414123535\n","Iter #10175488:  Learning rate = 0.000005:   Batch Loss = 0.569904, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952220559120178, Accuracy = 0.9175429940223694\n","Iter #10176512:  Learning rate = 0.000005:   Batch Loss = 0.491784, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952495336532593, Accuracy = 0.9177098870277405\n","Iter #10177536:  Learning rate = 0.000005:   Batch Loss = 0.577399, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952515602111816, Accuracy = 0.9178768396377563\n","Iter #10178560:  Learning rate = 0.000005:   Batch Loss = 0.537957, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595301628112793, Accuracy = 0.9182106256484985\n","Iter #10179584:  Learning rate = 0.000005:   Batch Loss = 0.564230, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953761339187622, Accuracy = 0.9177098870277405\n","Iter #10180608:  Learning rate = 0.000005:   Batch Loss = 0.552820, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954369306564331, Accuracy = 0.9177098870277405\n","Iter #10181632:  Learning rate = 0.000005:   Batch Loss = 0.540329, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954551100730896, Accuracy = 0.9173760414123535\n","Iter #10182656:  Learning rate = 0.000005:   Batch Loss = 0.561953, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953755378723145, Accuracy = 0.9173760414123535\n","Iter #10183680:  Learning rate = 0.000005:   Batch Loss = 0.515996, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953280925750732, Accuracy = 0.9175429940223694\n","Iter #10184704:  Learning rate = 0.000005:   Batch Loss = 0.623053, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952881574630737, Accuracy = 0.9180437326431274\n","Iter #10185728:  Learning rate = 0.000005:   Batch Loss = 0.583235, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952552556991577, Accuracy = 0.9178768396377563\n","Iter #10186752:  Learning rate = 0.000005:   Batch Loss = 0.533222, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952197313308716, Accuracy = 0.9177098870277405\n","Iter #10187776:  Learning rate = 0.000005:   Batch Loss = 0.540758, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595154881477356, Accuracy = 0.9168753027915955\n","Iter #10188800:  Learning rate = 0.000005:   Batch Loss = 0.575989, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951343774795532, Accuracy = 0.9172091484069824\n","Iter #10189824:  Learning rate = 0.000005:   Batch Loss = 0.592916, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951803922653198, Accuracy = 0.9172091484069824\n","Iter #10190848:  Learning rate = 0.000005:   Batch Loss = 0.595029, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952337980270386, Accuracy = 0.9173760414123535\n","Iter #10191872:  Learning rate = 0.000005:   Batch Loss = 0.574623, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952852964401245, Accuracy = 0.9177098870277405\n","Iter #10192896:  Learning rate = 0.000005:   Batch Loss = 0.610829, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595311164855957, Accuracy = 0.9173760414123535\n","Iter #10193920:  Learning rate = 0.000005:   Batch Loss = 0.557575, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952740907669067, Accuracy = 0.9173760414123535\n","Iter #10194944:  Learning rate = 0.000005:   Batch Loss = 0.515235, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952434539794922, Accuracy = 0.9177098870277405\n","Iter #10195968:  Learning rate = 0.000005:   Batch Loss = 0.505569, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595228910446167, Accuracy = 0.9172091484069824\n","Iter #10196992:  Learning rate = 0.000005:   Batch Loss = 0.577536, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952547788619995, Accuracy = 0.9173760414123535\n","Iter #10198016:  Learning rate = 0.000005:   Batch Loss = 0.538721, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952663421630859, Accuracy = 0.9180437326431274\n","Iter #10199040:  Learning rate = 0.000005:   Batch Loss = 0.534633, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953325033187866, Accuracy = 0.9182106256484985\n","Iter #10200064:  Learning rate = 0.000005:   Batch Loss = 0.506514, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953282117843628, Accuracy = 0.9180437326431274\n","Iter #10201088:  Learning rate = 0.000005:   Batch Loss = 0.633367, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952492952346802, Accuracy = 0.9177098870277405\n","Iter #10202112:  Learning rate = 0.000005:   Batch Loss = 0.539254, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952140092849731, Accuracy = 0.9172091484069824\n","Iter #10203136:  Learning rate = 0.000005:   Batch Loss = 0.609517, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952663421630859, Accuracy = 0.9177098870277405\n","Iter #10204160:  Learning rate = 0.000005:   Batch Loss = 0.533222, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953227281570435, Accuracy = 0.9180437326431274\n","Iter #10205184:  Learning rate = 0.000005:   Batch Loss = 0.626967, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953301191329956, Accuracy = 0.9177098870277405\n","Iter #10206208:  Learning rate = 0.000005:   Batch Loss = 0.556967, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952841639518738, Accuracy = 0.9172091484069824\n","Iter #10207232:  Learning rate = 0.000005:   Batch Loss = 0.593779, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952826142311096, Accuracy = 0.9170422554016113\n","Iter #10208256:  Learning rate = 0.000005:   Batch Loss = 0.540043, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952968597412109, Accuracy = 0.9175429940223694\n","Iter #10209280:  Learning rate = 0.000005:   Batch Loss = 0.643134, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952541828155518, Accuracy = 0.9175429940223694\n","Iter #10210304:  Learning rate = 0.000005:   Batch Loss = 0.549143, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952771902084351, Accuracy = 0.9178768396377563\n","Iter #10211328:  Learning rate = 0.000005:   Batch Loss = 0.576458, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953177213668823, Accuracy = 0.9178768396377563\n","Iter #10212352:  Learning rate = 0.000005:   Batch Loss = 0.622966, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953682661056519, Accuracy = 0.9183775782585144\n","Iter #10213376:  Learning rate = 0.000005:   Batch Loss = 0.630487, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953430533409119, Accuracy = 0.9182106256484985\n","Iter #10214400:  Learning rate = 0.000005:   Batch Loss = 0.656422, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595237672328949, Accuracy = 0.9175429940223694\n","Iter #10215424:  Learning rate = 0.000005:   Batch Loss = 0.575012, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952423810958862, Accuracy = 0.9178768396377563\n","Iter #10216448:  Learning rate = 0.000005:   Batch Loss = 0.566988, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952634215354919, Accuracy = 0.9177098870277405\n","Iter #10217472:  Learning rate = 0.000005:   Batch Loss = 0.536146, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953066349029541, Accuracy = 0.9177098870277405\n","Iter #10218496:  Learning rate = 0.000005:   Batch Loss = 0.563550, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953409671783447, Accuracy = 0.9175429940223694\n","Iter #10219520:  Learning rate = 0.000005:   Batch Loss = 0.567786, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953255891799927, Accuracy = 0.9175429940223694\n","Iter #10220544:  Learning rate = 0.000005:   Batch Loss = 0.587533, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953236222267151, Accuracy = 0.9175429940223694\n","Iter #10221568:  Learning rate = 0.000005:   Batch Loss = 0.572037, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953156352043152, Accuracy = 0.9175429940223694\n","Iter #10222592:  Learning rate = 0.000005:   Batch Loss = 0.577863, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953109264373779, Accuracy = 0.9175429940223694\n","Iter #10223616:  Learning rate = 0.000005:   Batch Loss = 0.558191, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952787399291992, Accuracy = 0.9175429940223694\n","Iter #10224640:  Learning rate = 0.000005:   Batch Loss = 0.515256, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952450633049011, Accuracy = 0.9172091484069824\n","Iter #10225664:  Learning rate = 0.000005:   Batch Loss = 0.602736, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952106714248657, Accuracy = 0.9170422554016113\n","Iter #10226688:  Learning rate = 0.000005:   Batch Loss = 0.589097, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951828956604004, Accuracy = 0.9170422554016113\n","Iter #10227712:  Learning rate = 0.000005:   Batch Loss = 0.535568, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951234102249146, Accuracy = 0.9172091484069824\n","Iter #10228736:  Learning rate = 0.000005:   Batch Loss = 0.584241, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951080322265625, Accuracy = 0.9163745641708374\n","Iter #10229760:  Learning rate = 0.000005:   Batch Loss = 0.650046, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951093435287476, Accuracy = 0.9163745641708374\n","Iter #10230784:  Learning rate = 0.000005:   Batch Loss = 0.494798, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5950976610183716, Accuracy = 0.9163745641708374\n","Iter #10231808:  Learning rate = 0.000005:   Batch Loss = 0.528485, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595075786113739, Accuracy = 0.9165414571762085\n","Iter #10232832:  Learning rate = 0.000005:   Batch Loss = 0.559426, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5950936079025269, Accuracy = 0.9163745641708374\n","Iter #10233856:  Learning rate = 0.000005:   Batch Loss = 0.536336, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951524972915649, Accuracy = 0.9165414571762085\n","Iter #10234880:  Learning rate = 0.000005:   Batch Loss = 0.523833, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595205545425415, Accuracy = 0.9180437326431274\n","Iter #10235904:  Learning rate = 0.000005:   Batch Loss = 0.587720, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952759385108948, Accuracy = 0.9177098870277405\n","Iter #10236928:  Learning rate = 0.000005:   Batch Loss = 0.522668, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595361590385437, Accuracy = 0.9178768396377563\n","Iter #10237952:  Learning rate = 0.000005:   Batch Loss = 0.624569, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954225659370422, Accuracy = 0.9178768396377563\n","Iter #10238976:  Learning rate = 0.000005:   Batch Loss = 0.591913, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59540194272995, Accuracy = 0.9178768396377563\n","Iter #10240000:  Learning rate = 0.000005:   Batch Loss = 0.539001, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595323920249939, Accuracy = 0.9180437326431274\n","Iter #10241024:  Learning rate = 0.000005:   Batch Loss = 0.558479, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952233672142029, Accuracy = 0.9175429940223694\n","Iter #10242048:  Learning rate = 0.000005:   Batch Loss = 0.542215, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951477289199829, Accuracy = 0.9168753027915955\n","Iter #10243072:  Learning rate = 0.000005:   Batch Loss = 0.538959, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951497554779053, Accuracy = 0.9168753027915955\n","Iter #10244096:  Learning rate = 0.000005:   Batch Loss = 0.502592, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951629877090454, Accuracy = 0.9170422554016113\n","Iter #10245120:  Learning rate = 0.000005:   Batch Loss = 0.617972, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951670408248901, Accuracy = 0.9168753027915955\n","Iter #10246144:  Learning rate = 0.000005:   Batch Loss = 0.573969, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952085256576538, Accuracy = 0.9170422554016113\n","Iter #10247168:  Learning rate = 0.000005:   Batch Loss = 0.467177, Accuracy = 0.9921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952584743499756, Accuracy = 0.9168753027915955\n","Iter #10248192:  Learning rate = 0.000005:   Batch Loss = 0.564847, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595302939414978, Accuracy = 0.9170422554016113\n","Iter #10249216:  Learning rate = 0.000005:   Batch Loss = 0.678143, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952855348587036, Accuracy = 0.9178768396377563\n","Iter #10250240:  Learning rate = 0.000005:   Batch Loss = 0.562087, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953280329704285, Accuracy = 0.9178768396377563\n","Iter #10251264:  Learning rate = 0.000005:   Batch Loss = 0.655542, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595437228679657, Accuracy = 0.9182106256484985\n","Iter #10252288:  Learning rate = 0.000005:   Batch Loss = 0.619505, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954697132110596, Accuracy = 0.9182106256484985\n","Iter #10253312:  Learning rate = 0.000005:   Batch Loss = 0.545621, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953928232192993, Accuracy = 0.9178768396377563\n","Iter #10254336:  Learning rate = 0.000005:   Batch Loss = 0.598433, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953034162521362, Accuracy = 0.9177098870277405\n","Iter #10255360:  Learning rate = 0.000005:   Batch Loss = 0.490558, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952353477478027, Accuracy = 0.9175429940223694\n","Iter #10256384:  Learning rate = 0.000005:   Batch Loss = 0.561536, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952457189559937, Accuracy = 0.9175429940223694\n","Iter #10257408:  Learning rate = 0.000005:   Batch Loss = 0.554873, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952301025390625, Accuracy = 0.9165414571762085\n","Iter #10258432:  Learning rate = 0.000005:   Batch Loss = 0.532018, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952521562576294, Accuracy = 0.9168753027915955\n","Iter #10259456:  Learning rate = 0.000005:   Batch Loss = 0.622149, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952011346817017, Accuracy = 0.9172091484069824\n","Iter #10260480:  Learning rate = 0.000005:   Batch Loss = 0.536804, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951567888259888, Accuracy = 0.9168753027915955\n","Iter #10261504:  Learning rate = 0.000005:   Batch Loss = 0.526187, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951775312423706, Accuracy = 0.9173760414123535\n","Iter #10262528:  Learning rate = 0.000005:   Batch Loss = 0.518904, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952192544937134, Accuracy = 0.9172091484069824\n","Iter #10263552:  Learning rate = 0.000005:   Batch Loss = 0.552752, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953537225723267, Accuracy = 0.9180437326431274\n","Iter #10264576:  Learning rate = 0.000005:   Batch Loss = 0.515151, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954437255859375, Accuracy = 0.9177098870277405\n","Iter #10265600:  Learning rate = 0.000005:   Batch Loss = 0.527386, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954768061637878, Accuracy = 0.9182106256484985\n","Iter #10266624:  Learning rate = 0.000005:   Batch Loss = 0.514503, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954699516296387, Accuracy = 0.9180437326431274\n","Iter #10267648:  Learning rate = 0.000005:   Batch Loss = 0.564524, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595382809638977, Accuracy = 0.9178768396377563\n","Iter #10268672:  Learning rate = 0.000005:   Batch Loss = 0.555778, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953763723373413, Accuracy = 0.9178768396377563\n","Iter #10269696:  Learning rate = 0.000005:   Batch Loss = 0.574134, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953904390335083, Accuracy = 0.9177098870277405\n","Iter #10270720:  Learning rate = 0.000005:   Batch Loss = 0.560457, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954140424728394, Accuracy = 0.9178768396377563\n","Iter #10271744:  Learning rate = 0.000005:   Batch Loss = 0.596460, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953310132026672, Accuracy = 0.9178768396377563\n","Iter #10272768:  Learning rate = 0.000005:   Batch Loss = 0.606359, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953090190887451, Accuracy = 0.9180437326431274\n","Iter #10273792:  Learning rate = 0.000005:   Batch Loss = 0.522599, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59526526927948, Accuracy = 0.9175429940223694\n","Iter #10274816:  Learning rate = 0.000005:   Batch Loss = 0.622116, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952482223510742, Accuracy = 0.9173760414123535\n","Iter #10275840:  Learning rate = 0.000005:   Batch Loss = 0.535756, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952209830284119, Accuracy = 0.9173760414123535\n","Iter #10276864:  Learning rate = 0.000005:   Batch Loss = 0.561722, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951758623123169, Accuracy = 0.9172091484069824\n","Iter #10277888:  Learning rate = 0.000005:   Batch Loss = 0.594535, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951646566390991, Accuracy = 0.9168753027915955\n","Iter #10278912:  Learning rate = 0.000005:   Batch Loss = 0.489750, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952022075653076, Accuracy = 0.9170422554016113\n","Iter #10279936:  Learning rate = 0.000005:   Batch Loss = 0.555244, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952661037445068, Accuracy = 0.9168753027915955\n","Iter #10280960:  Learning rate = 0.000005:   Batch Loss = 0.536987, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952682495117188, Accuracy = 0.9172091484069824\n","Iter #10281984:  Learning rate = 0.000005:   Batch Loss = 0.560753, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952630639076233, Accuracy = 0.9172091484069824\n","Iter #10283008:  Learning rate = 0.000005:   Batch Loss = 0.584579, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952402353286743, Accuracy = 0.9173760414123535\n","Iter #10284032:  Learning rate = 0.000005:   Batch Loss = 0.540944, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952516794204712, Accuracy = 0.9175429940223694\n","Iter #10285056:  Learning rate = 0.000005:   Batch Loss = 0.562403, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952441692352295, Accuracy = 0.9177098870277405\n","Iter #10286080:  Learning rate = 0.000005:   Batch Loss = 0.559941, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952720046043396, Accuracy = 0.9178768396377563\n","Iter #10287104:  Learning rate = 0.000005:   Batch Loss = 0.483827, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952695608139038, Accuracy = 0.9177098870277405\n","Iter #10288128:  Learning rate = 0.000005:   Batch Loss = 0.573519, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953019857406616, Accuracy = 0.9177098870277405\n","Iter #10289152:  Learning rate = 0.000005:   Batch Loss = 0.524232, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953726768493652, Accuracy = 0.9178768396377563\n","Iter #10290176:  Learning rate = 0.000005:   Batch Loss = 0.559258, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953578948974609, Accuracy = 0.9180437326431274\n","Iter #10291200:  Learning rate = 0.000005:   Batch Loss = 0.542610, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953186750411987, Accuracy = 0.9180437326431274\n","Iter #10292224:  Learning rate = 0.000005:   Batch Loss = 0.567145, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595289945602417, Accuracy = 0.9180437326431274\n","Iter #10293248:  Learning rate = 0.000005:   Batch Loss = 0.536716, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952248573303223, Accuracy = 0.9180437326431274\n","Iter #10294272:  Learning rate = 0.000005:   Batch Loss = 0.542242, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951954126358032, Accuracy = 0.9177098870277405\n","Iter #10295296:  Learning rate = 0.000005:   Batch Loss = 0.538815, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952162146568298, Accuracy = 0.9173760414123535\n","Iter #10296320:  Learning rate = 0.000005:   Batch Loss = 0.524928, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952787399291992, Accuracy = 0.9172091484069824\n","Iter #10297344:  Learning rate = 0.000005:   Batch Loss = 0.543357, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953402519226074, Accuracy = 0.9175429940223694\n","Iter #10298368:  Learning rate = 0.000005:   Batch Loss = 0.490609, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59535151720047, Accuracy = 0.9178768396377563\n","Iter #10299392:  Learning rate = 0.000005:   Batch Loss = 0.585917, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952915549278259, Accuracy = 0.9173760414123535\n","Iter #10300416:  Learning rate = 0.000005:   Batch Loss = 0.572902, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952486395835876, Accuracy = 0.9177098870277405\n","Iter #10301440:  Learning rate = 0.000005:   Batch Loss = 0.462962, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952053070068359, Accuracy = 0.9172091484069824\n","Iter #10302464:  Learning rate = 0.000005:   Batch Loss = 0.495024, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951499938964844, Accuracy = 0.9170422554016113\n","Iter #10303488:  Learning rate = 0.000005:   Batch Loss = 0.597722, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951414108276367, Accuracy = 0.9172091484069824\n","Iter #10304512:  Learning rate = 0.000005:   Batch Loss = 0.581342, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951458215713501, Accuracy = 0.9168753027915955\n","Iter #10305536:  Learning rate = 0.000005:   Batch Loss = 0.473204, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951552391052246, Accuracy = 0.9170422554016113\n","Iter #10306560:  Learning rate = 0.000005:   Batch Loss = 0.509513, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952383875846863, Accuracy = 0.9178768396377563\n","Iter #10307584:  Learning rate = 0.000005:   Batch Loss = 0.579907, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952929258346558, Accuracy = 0.9178768396377563\n","Iter #10308608:  Learning rate = 0.000005:   Batch Loss = 0.477707, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953307747840881, Accuracy = 0.9178768396377563\n","Iter #10309632:  Learning rate = 0.000005:   Batch Loss = 0.589736, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953502058982849, Accuracy = 0.9177098870277405\n","Iter #10310656:  Learning rate = 0.000005:   Batch Loss = 0.621969, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953646898269653, Accuracy = 0.9175429940223694\n","Iter #10311680:  Learning rate = 0.000005:   Batch Loss = 0.505681, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953307747840881, Accuracy = 0.9178768396377563\n","Iter #10312704:  Learning rate = 0.000005:   Batch Loss = 0.549899, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953118801116943, Accuracy = 0.9180437326431274\n","Iter #10313728:  Learning rate = 0.000005:   Batch Loss = 0.578158, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953186750411987, Accuracy = 0.9180437326431274\n","Iter #10314752:  Learning rate = 0.000005:   Batch Loss = 0.526358, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952816009521484, Accuracy = 0.9178768396377563\n","Iter #10315776:  Learning rate = 0.000005:   Batch Loss = 0.559989, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952836871147156, Accuracy = 0.9172091484069824\n","Iter #10316800:  Learning rate = 0.000005:   Batch Loss = 0.564140, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952854156494141, Accuracy = 0.9168753027915955\n","Iter #10317824:  Learning rate = 0.000005:   Batch Loss = 0.540073, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952942371368408, Accuracy = 0.9170422554016113\n","Iter #10318848:  Learning rate = 0.000005:   Batch Loss = 0.532541, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952740907669067, Accuracy = 0.9172091484069824\n","Iter #10319872:  Learning rate = 0.000005:   Batch Loss = 0.561374, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595237135887146, Accuracy = 0.9173760414123535\n","Iter #10320896:  Learning rate = 0.000004:   Batch Loss = 0.551494, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951873064041138, Accuracy = 0.9168753027915955\n","Iter #10321920:  Learning rate = 0.000004:   Batch Loss = 0.531981, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951619148254395, Accuracy = 0.9170422554016113\n","Iter #10322944:  Learning rate = 0.000004:   Batch Loss = 0.664594, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952147245407104, Accuracy = 0.9177098870277405\n","Iter #10323968:  Learning rate = 0.000004:   Batch Loss = 0.587066, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595295786857605, Accuracy = 0.9178768396377563\n","Iter #10324992:  Learning rate = 0.000004:   Batch Loss = 0.550670, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954146981239319, Accuracy = 0.9187114238739014\n","Iter #10326016:  Learning rate = 0.000004:   Batch Loss = 0.562015, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954659581184387, Accuracy = 0.9187114238739014\n","Iter #10327040:  Learning rate = 0.000004:   Batch Loss = 0.520540, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954350233078003, Accuracy = 0.9185444712638855\n","Iter #10328064:  Learning rate = 0.000004:   Batch Loss = 0.578751, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595376193523407, Accuracy = 0.9180437326431274\n","Iter #10329088:  Learning rate = 0.000004:   Batch Loss = 0.535304, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595422625541687, Accuracy = 0.9180437326431274\n","Iter #10330112:  Learning rate = 0.000004:   Batch Loss = 0.540727, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954285860061646, Accuracy = 0.9180437326431274\n","Iter #10331136:  Learning rate = 0.000004:   Batch Loss = 0.611015, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954015851020813, Accuracy = 0.9180437326431274\n","Iter #10332160:  Learning rate = 0.000004:   Batch Loss = 0.522732, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953603982925415, Accuracy = 0.9175429940223694\n","Iter #10333184:  Learning rate = 0.000004:   Batch Loss = 0.500466, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952780246734619, Accuracy = 0.9173760414123535\n","Iter #10334208:  Learning rate = 0.000004:   Batch Loss = 0.567633, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952651500701904, Accuracy = 0.9168753027915955\n","Iter #10335232:  Learning rate = 0.000004:   Batch Loss = 0.541516, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595261812210083, Accuracy = 0.9165414571762085\n","Iter #10336256:  Learning rate = 0.000004:   Batch Loss = 0.512641, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595240592956543, Accuracy = 0.9170422554016113\n","Iter #10337280:  Learning rate = 0.000004:   Batch Loss = 0.551712, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952277779579163, Accuracy = 0.9168753027915955\n","Iter #10338304:  Learning rate = 0.000004:   Batch Loss = 0.565077, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952366590499878, Accuracy = 0.9168753027915955\n","Iter #10339328:  Learning rate = 0.000004:   Batch Loss = 0.517890, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952560901641846, Accuracy = 0.9168753027915955\n","Iter #10340352:  Learning rate = 0.000004:   Batch Loss = 0.570119, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952215790748596, Accuracy = 0.9168753027915955\n","Iter #10341376:  Learning rate = 0.000004:   Batch Loss = 0.496878, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952214598655701, Accuracy = 0.9168753027915955\n","Iter #10342400:  Learning rate = 0.000004:   Batch Loss = 0.548113, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952637195587158, Accuracy = 0.9168753027915955\n","Iter #10343424:  Learning rate = 0.000004:   Batch Loss = 0.565110, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953160524368286, Accuracy = 0.9170422554016113\n","Iter #10344448:  Learning rate = 0.000004:   Batch Loss = 0.613162, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953730344772339, Accuracy = 0.9173760414123535\n","Iter #10345472:  Learning rate = 0.000004:   Batch Loss = 0.528840, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953632593154907, Accuracy = 0.9177098870277405\n","Iter #10346496:  Learning rate = 0.000004:   Batch Loss = 0.512371, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953414440155029, Accuracy = 0.9178768396377563\n","Iter #10347520:  Learning rate = 0.000004:   Batch Loss = 0.540803, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595356822013855, Accuracy = 0.9177098870277405\n","Iter #10348544:  Learning rate = 0.000004:   Batch Loss = 0.556217, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595292329788208, Accuracy = 0.9180437326431274\n","Iter #10349568:  Learning rate = 0.000004:   Batch Loss = 0.562233, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952574014663696, Accuracy = 0.9178768396377563\n","Iter #10350592:  Learning rate = 0.000004:   Batch Loss = 0.527096, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952227711677551, Accuracy = 0.9178768396377563\n","Iter #10351616:  Learning rate = 0.000004:   Batch Loss = 0.589056, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595172643661499, Accuracy = 0.9172091484069824\n","Iter #10352640:  Learning rate = 0.000004:   Batch Loss = 0.536385, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952128171920776, Accuracy = 0.9173760414123535\n","Iter #10353664:  Learning rate = 0.000004:   Batch Loss = 0.640345, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952653884887695, Accuracy = 0.9182106256484985\n","Iter #10354688:  Learning rate = 0.000004:   Batch Loss = 0.568307, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952703356742859, Accuracy = 0.9180437326431274\n","Iter #10355712:  Learning rate = 0.000004:   Batch Loss = 0.576546, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952298641204834, Accuracy = 0.9180437326431274\n","Iter #10356736:  Learning rate = 0.000004:   Batch Loss = 0.525684, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951979160308838, Accuracy = 0.9177098870277405\n","Iter #10357760:  Learning rate = 0.000004:   Batch Loss = 0.555459, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952529311180115, Accuracy = 0.9183775782585144\n","Iter #10358784:  Learning rate = 0.000004:   Batch Loss = 0.522518, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953145027160645, Accuracy = 0.9180437326431274\n","Iter #10359808:  Learning rate = 0.000004:   Batch Loss = 0.601806, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953353643417358, Accuracy = 0.9180437326431274\n","Iter #10360832:  Learning rate = 0.000004:   Batch Loss = 0.494886, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952428579330444, Accuracy = 0.9178768396377563\n","Iter #10361856:  Learning rate = 0.000004:   Batch Loss = 0.621854, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952411890029907, Accuracy = 0.9173760414123535\n","Iter #10362880:  Learning rate = 0.000004:   Batch Loss = 0.530483, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952523946762085, Accuracy = 0.9177098870277405\n","Iter #10363904:  Learning rate = 0.000004:   Batch Loss = 0.584094, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952430963516235, Accuracy = 0.9170422554016113\n","Iter #10364928:  Learning rate = 0.000004:   Batch Loss = 0.594047, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951851606369019, Accuracy = 0.9168753027915955\n","Iter #10365952:  Learning rate = 0.000004:   Batch Loss = 0.542682, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951611995697021, Accuracy = 0.9175429940223694\n","Iter #10366976:  Learning rate = 0.000004:   Batch Loss = 0.558702, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951418876647949, Accuracy = 0.9172091484069824\n","Iter #10368000:  Learning rate = 0.000004:   Batch Loss = 0.514360, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951066017150879, Accuracy = 0.9172091484069824\n","Iter #10369024:  Learning rate = 0.000004:   Batch Loss = 0.571274, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951497554779053, Accuracy = 0.9173760414123535\n","Iter #10370048:  Learning rate = 0.000004:   Batch Loss = 0.620338, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951992273330688, Accuracy = 0.9175429940223694\n","Iter #10371072:  Learning rate = 0.000004:   Batch Loss = 0.558452, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952478051185608, Accuracy = 0.9177098870277405\n","Iter #10372096:  Learning rate = 0.000004:   Batch Loss = 0.555281, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952509045600891, Accuracy = 0.9177098870277405\n","Iter #10373120:  Learning rate = 0.000004:   Batch Loss = 0.526871, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952548980712891, Accuracy = 0.9175429940223694\n","Iter #10374144:  Learning rate = 0.000004:   Batch Loss = 0.591502, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952985286712646, Accuracy = 0.9177098870277405\n","Iter #10375168:  Learning rate = 0.000004:   Batch Loss = 0.615636, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953792929649353, Accuracy = 0.9178768396377563\n","Iter #10376192:  Learning rate = 0.000004:   Batch Loss = 0.541106, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955401062965393, Accuracy = 0.9182106256484985\n","Iter #10377216:  Learning rate = 0.000004:   Batch Loss = 0.580027, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956546068191528, Accuracy = 0.9185444712638855\n","Iter #10378240:  Learning rate = 0.000004:   Batch Loss = 0.586443, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5956153869628906, Accuracy = 0.9185444712638855\n","Iter #10379264:  Learning rate = 0.000004:   Batch Loss = 0.563908, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954347848892212, Accuracy = 0.9180437326431274\n","Iter #10380288:  Learning rate = 0.000004:   Batch Loss = 0.574366, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952775478363037, Accuracy = 0.9175429940223694\n","Iter #10381312:  Learning rate = 0.000004:   Batch Loss = 0.566983, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952520370483398, Accuracy = 0.9172091484069824\n","Iter #10382336:  Learning rate = 0.000004:   Batch Loss = 0.521508, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952074527740479, Accuracy = 0.9170422554016113\n","Iter #10383360:  Learning rate = 0.000004:   Batch Loss = 0.565164, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952131748199463, Accuracy = 0.9168753027915955\n","Iter #10384384:  Learning rate = 0.000004:   Batch Loss = 0.603920, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952327251434326, Accuracy = 0.9168753027915955\n","Iter #10385408:  Learning rate = 0.000004:   Batch Loss = 0.626032, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408313751221, Accuracy = 0.9168753027915955\n","Iter #10386432:  Learning rate = 0.000004:   Batch Loss = 0.548978, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595283567905426, Accuracy = 0.9172091484069824\n","Iter #10387456:  Learning rate = 0.000004:   Batch Loss = 0.607114, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952939987182617, Accuracy = 0.9177098870277405\n","Iter #10388480:  Learning rate = 0.000004:   Batch Loss = 0.522682, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952954292297363, Accuracy = 0.9175429940223694\n","Iter #10389504:  Learning rate = 0.000004:   Batch Loss = 0.552315, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953018665313721, Accuracy = 0.9177098870277405\n","Iter #10390528:  Learning rate = 0.000004:   Batch Loss = 0.597235, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595260739326477, Accuracy = 0.9173760414123535\n","Iter #10391552:  Learning rate = 0.000004:   Batch Loss = 0.647068, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952540636062622, Accuracy = 0.9175429940223694\n","Iter #10392576:  Learning rate = 0.000004:   Batch Loss = 0.528285, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953084230422974, Accuracy = 0.9180437326431274\n","Iter #10393600:  Learning rate = 0.000004:   Batch Loss = 0.658466, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953429937362671, Accuracy = 0.9180437326431274\n","Iter #10394624:  Learning rate = 0.000004:   Batch Loss = 0.612677, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953538417816162, Accuracy = 0.9178768396377563\n","Iter #10395648:  Learning rate = 0.000004:   Batch Loss = 0.512525, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952966213226318, Accuracy = 0.9177098870277405\n","Iter #10396672:  Learning rate = 0.000004:   Batch Loss = 0.521888, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952392816543579, Accuracy = 0.9173760414123535\n","Iter #10397696:  Learning rate = 0.000004:   Batch Loss = 0.523336, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952333211898804, Accuracy = 0.9168753027915955\n","Iter #10398720:  Learning rate = 0.000004:   Batch Loss = 0.528522, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952284336090088, Accuracy = 0.9168753027915955\n","Iter #10399744:  Learning rate = 0.000004:   Batch Loss = 0.526087, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951646566390991, Accuracy = 0.9168753027915955\n","Iter #10400768:  Learning rate = 0.000004:   Batch Loss = 0.538549, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951772928237915, Accuracy = 0.9168753027915955\n","Iter #10401792:  Learning rate = 0.000004:   Batch Loss = 0.606329, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951722860336304, Accuracy = 0.9168753027915955\n","Iter #10402816:  Learning rate = 0.000004:   Batch Loss = 0.547762, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951879024505615, Accuracy = 0.9162076711654663\n","Iter #10403840:  Learning rate = 0.000004:   Batch Loss = 0.589507, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952480435371399, Accuracy = 0.9172091484069824\n","Iter #10404864:  Learning rate = 0.000004:   Batch Loss = 0.639049, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952775478363037, Accuracy = 0.9175429940223694\n","Iter #10405888:  Learning rate = 0.000004:   Batch Loss = 0.586052, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595304012298584, Accuracy = 0.9178768396377563\n","Iter #10406912:  Learning rate = 0.000004:   Batch Loss = 0.607652, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952668190002441, Accuracy = 0.9180437326431274\n","Iter #10407936:  Learning rate = 0.000004:   Batch Loss = 0.575508, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952190160751343, Accuracy = 0.9180437326431274\n","Iter #10408960:  Learning rate = 0.000004:   Batch Loss = 0.610326, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952421426773071, Accuracy = 0.9177098870277405\n","Iter #10409984:  Learning rate = 0.000004:   Batch Loss = 0.551332, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595223069190979, Accuracy = 0.9173760414123535\n","Iter #10411008:  Learning rate = 0.000004:   Batch Loss = 0.577747, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952374935150146, Accuracy = 0.9175429940223694\n","Iter #10412032:  Learning rate = 0.000004:   Batch Loss = 0.547100, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952801704406738, Accuracy = 0.9172091484069824\n","Iter #10413056:  Learning rate = 0.000004:   Batch Loss = 0.584041, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953130722045898, Accuracy = 0.9172091484069824\n","Iter #10414080:  Learning rate = 0.000004:   Batch Loss = 0.536729, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952780246734619, Accuracy = 0.9175429940223694\n","Iter #10415104:  Learning rate = 0.000004:   Batch Loss = 0.549847, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952446460723877, Accuracy = 0.9170422554016113\n","Iter #10416128:  Learning rate = 0.000004:   Batch Loss = 0.548163, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952009558677673, Accuracy = 0.9170422554016113\n","Iter #10417152:  Learning rate = 0.000004:   Batch Loss = 0.529865, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951836109161377, Accuracy = 0.9177098870277405\n","Iter #10418176:  Learning rate = 0.000004:   Batch Loss = 0.505905, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952136516571045, Accuracy = 0.9178768396377563\n","Iter #10419200:  Learning rate = 0.000004:   Batch Loss = 0.520307, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952321290969849, Accuracy = 0.9182106256484985\n","Iter #10420224:  Learning rate = 0.000004:   Batch Loss = 0.543042, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952794551849365, Accuracy = 0.9183775782585144\n","Iter #10421248:  Learning rate = 0.000004:   Batch Loss = 0.565264, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952951908111572, Accuracy = 0.9180437326431274\n","Iter #10422272:  Learning rate = 0.000004:   Batch Loss = 0.513413, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953956842422485, Accuracy = 0.9182106256484985\n","Iter #10423296:  Learning rate = 0.000004:   Batch Loss = 0.537569, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954892635345459, Accuracy = 0.9182106256484985\n","Iter #10424320:  Learning rate = 0.000004:   Batch Loss = 0.594612, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5955104827880859, Accuracy = 0.9180437326431274\n","Iter #10425344:  Learning rate = 0.000004:   Batch Loss = 0.579280, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954385995864868, Accuracy = 0.9177098870277405\n","Iter #10426368:  Learning rate = 0.000004:   Batch Loss = 0.575451, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953841209411621, Accuracy = 0.9178768396377563\n","Iter #10427392:  Learning rate = 0.000004:   Batch Loss = 0.591787, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952805280685425, Accuracy = 0.9177098870277405\n","Iter #10428416:  Learning rate = 0.000004:   Batch Loss = 0.545272, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952097177505493, Accuracy = 0.9168753027915955\n","Iter #10429440:  Learning rate = 0.000004:   Batch Loss = 0.578168, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952127575874329, Accuracy = 0.9168753027915955\n","Iter #10430464:  Learning rate = 0.000004:   Batch Loss = 0.599382, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952118635177612, Accuracy = 0.9167084097862244\n","Iter #10431488:  Learning rate = 0.000004:   Batch Loss = 0.582155, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951919555664062, Accuracy = 0.9172091484069824\n","Iter #10432512:  Learning rate = 0.000004:   Batch Loss = 0.593984, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029228210449, Accuracy = 0.9173760414123535\n","Iter #10433536:  Learning rate = 0.000004:   Batch Loss = 0.588223, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952289700508118, Accuracy = 0.9175429940223694\n","Iter #10434560:  Learning rate = 0.000004:   Batch Loss = 0.490443, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952707529067993, Accuracy = 0.9180437326431274\n","Iter #10435584:  Learning rate = 0.000004:   Batch Loss = 0.578184, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595273494720459, Accuracy = 0.9177098870277405\n","Iter #10436608:  Learning rate = 0.000004:   Batch Loss = 0.591728, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952993631362915, Accuracy = 0.9178768396377563\n","Iter #10437632:  Learning rate = 0.000004:   Batch Loss = 0.591483, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953601598739624, Accuracy = 0.9177098870277405\n","Iter #10438656:  Learning rate = 0.000004:   Batch Loss = 0.497946, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954058170318604, Accuracy = 0.9177098870277405\n","Iter #10439680:  Learning rate = 0.000004:   Batch Loss = 0.652956, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954424142837524, Accuracy = 0.9178768396377563\n","Iter #10440704:  Learning rate = 0.000004:   Batch Loss = 0.575469, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954042673110962, Accuracy = 0.9178768396377563\n","Iter #10441728:  Learning rate = 0.000004:   Batch Loss = 0.491582, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953291654586792, Accuracy = 0.9178768396377563\n","Iter #10442752:  Learning rate = 0.000004:   Batch Loss = 0.516165, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952851176261902, Accuracy = 0.9180437326431274\n","Iter #10443776:  Learning rate = 0.000004:   Batch Loss = 0.548098, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952006578445435, Accuracy = 0.9178768396377563\n","Iter #10444800:  Learning rate = 0.000004:   Batch Loss = 0.513778, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951364040374756, Accuracy = 0.9172091484069824\n","Iter #10445824:  Learning rate = 0.000004:   Batch Loss = 0.516032, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951502323150635, Accuracy = 0.9173760414123535\n","Iter #10446848:  Learning rate = 0.000004:   Batch Loss = 0.490840, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951346158981323, Accuracy = 0.9168753027915955\n","Iter #10447872:  Learning rate = 0.000004:   Batch Loss = 0.558097, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951070785522461, Accuracy = 0.9170422554016113\n","Iter #10448896:  Learning rate = 0.000004:   Batch Loss = 0.569607, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5950884819030762, Accuracy = 0.9170422554016113\n","Iter #10449920:  Learning rate = 0.000004:   Batch Loss = 0.555433, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5950865745544434, Accuracy = 0.9172091484069824\n","Iter #10450944:  Learning rate = 0.000004:   Batch Loss = 0.561113, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951049327850342, Accuracy = 0.9173760414123535\n","Iter #10451968:  Learning rate = 0.000004:   Batch Loss = 0.568126, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951176881790161, Accuracy = 0.9170422554016113\n","Iter #10452992:  Learning rate = 0.000004:   Batch Loss = 0.631717, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951648354530334, Accuracy = 0.9172091484069824\n","Iter #10454016:  Learning rate = 0.000004:   Batch Loss = 0.507645, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952186584472656, Accuracy = 0.9177098870277405\n","Iter #10455040:  Learning rate = 0.000004:   Batch Loss = 0.578582, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952312350273132, Accuracy = 0.9177098870277405\n","Iter #10456064:  Learning rate = 0.000004:   Batch Loss = 0.546112, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952310562133789, Accuracy = 0.9173760414123535\n","Iter #10457088:  Learning rate = 0.000004:   Batch Loss = 0.538956, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952593088150024, Accuracy = 0.9175429940223694\n","Iter #10458112:  Learning rate = 0.000004:   Batch Loss = 0.539667, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952734351158142, Accuracy = 0.9178768396377563\n","Iter #10459136:  Learning rate = 0.000004:   Batch Loss = 0.520095, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952714085578918, Accuracy = 0.9182106256484985\n","Iter #10460160:  Learning rate = 0.000004:   Batch Loss = 0.552321, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953164100646973, Accuracy = 0.9182106256484985\n","Iter #10461184:  Learning rate = 0.000004:   Batch Loss = 0.500584, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953766703605652, Accuracy = 0.9180437326431274\n","Iter #10462208:  Learning rate = 0.000004:   Batch Loss = 0.535900, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954194068908691, Accuracy = 0.9180437326431274\n","Iter #10463232:  Learning rate = 0.000004:   Batch Loss = 0.535721, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954123735427856, Accuracy = 0.9182106256484985\n","Iter #10464256:  Learning rate = 0.000004:   Batch Loss = 0.588485, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595426619052887, Accuracy = 0.9180437326431274\n","Iter #10465280:  Learning rate = 0.000004:   Batch Loss = 0.580429, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953400731086731, Accuracy = 0.9178768396377563\n","Iter #10466304:  Learning rate = 0.000004:   Batch Loss = 0.494952, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952694416046143, Accuracy = 0.9177098870277405\n","Iter #10467328:  Learning rate = 0.000004:   Batch Loss = 0.589354, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952773094177246, Accuracy = 0.9175429940223694\n","Iter #10468352:  Learning rate = 0.000004:   Batch Loss = 0.595759, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952708721160889, Accuracy = 0.9178768396377563\n","Iter #10469376:  Learning rate = 0.000004:   Batch Loss = 0.595722, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952566862106323, Accuracy = 0.9180437326431274\n","Iter #10470400:  Learning rate = 0.000004:   Batch Loss = 0.527137, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156186103821, Accuracy = 0.9173760414123535\n","Iter #10471424:  Learning rate = 0.000004:   Batch Loss = 0.521126, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951938033103943, Accuracy = 0.9175429940223694\n","Iter #10472448:  Learning rate = 0.000004:   Batch Loss = 0.514426, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952134132385254, Accuracy = 0.9168753027915955\n","Iter #10473472:  Learning rate = 0.000004:   Batch Loss = 0.554914, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952074527740479, Accuracy = 0.9168753027915955\n","Iter #10474496:  Learning rate = 0.000004:   Batch Loss = 0.504840, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952277779579163, Accuracy = 0.9168753027915955\n","Iter #10475520:  Learning rate = 0.000004:   Batch Loss = 0.563941, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952321290969849, Accuracy = 0.9168753027915955\n","Iter #10476544:  Learning rate = 0.000004:   Batch Loss = 0.560227, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952391624450684, Accuracy = 0.9170422554016113\n","Iter #10477568:  Learning rate = 0.000004:   Batch Loss = 0.531234, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952262878417969, Accuracy = 0.9170422554016113\n","Iter #10478592:  Learning rate = 0.000004:   Batch Loss = 0.593158, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988697052002, Accuracy = 0.9168753027915955\n","Iter #10479616:  Learning rate = 0.000004:   Batch Loss = 0.545868, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951867699623108, Accuracy = 0.9168753027915955\n","Iter #10480640:  Learning rate = 0.000004:   Batch Loss = 0.573727, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951914191246033, Accuracy = 0.9173760414123535\n","Iter #10481664:  Learning rate = 0.000004:   Batch Loss = 0.641996, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952364206314087, Accuracy = 0.9177098870277405\n","Iter #10482688:  Learning rate = 0.000004:   Batch Loss = 0.529414, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952690243721008, Accuracy = 0.9178768396377563\n","Iter #10483712:  Learning rate = 0.000004:   Batch Loss = 0.521291, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952398777008057, Accuracy = 0.9180437326431274\n","Iter #10484736:  Learning rate = 0.000004:   Batch Loss = 0.572238, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595188319683075, Accuracy = 0.9167084097862244\n","Iter #10485760:  Learning rate = 0.000004:   Batch Loss = 0.509119, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951722860336304, Accuracy = 0.9165414571762085\n","Iter #10486784:  Learning rate = 0.000004:   Batch Loss = 0.631816, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951603651046753, Accuracy = 0.9165414571762085\n","Iter #10487808:  Learning rate = 0.000004:   Batch Loss = 0.566511, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951448678970337, Accuracy = 0.9163745641708374\n","Iter #10488832:  Learning rate = 0.000004:   Batch Loss = 0.470782, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595199704170227, Accuracy = 0.9173760414123535\n","Iter #10489856:  Learning rate = 0.000004:   Batch Loss = 0.562159, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952432155609131, Accuracy = 0.9175429940223694\n","Iter #10490880:  Learning rate = 0.000004:   Batch Loss = 0.662126, Accuracy = 0.8671875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952638387680054, Accuracy = 0.9178768396377563\n","Iter #10491904:  Learning rate = 0.000004:   Batch Loss = 0.517264, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595329761505127, Accuracy = 0.9178768396377563\n","Iter #10492928:  Learning rate = 0.000004:   Batch Loss = 0.505609, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595394492149353, Accuracy = 0.9177098870277405\n","Iter #10493952:  Learning rate = 0.000004:   Batch Loss = 0.550666, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953964591026306, Accuracy = 0.9177098870277405\n","Iter #10494976:  Learning rate = 0.000004:   Batch Loss = 0.552443, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953186750411987, Accuracy = 0.9177098870277405\n","Iter #10496000:  Learning rate = 0.000004:   Batch Loss = 0.548487, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952601432800293, Accuracy = 0.9175429940223694\n","Iter #10497024:  Learning rate = 0.000004:   Batch Loss = 0.496015, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952869057655334, Accuracy = 0.9177098870277405\n","Iter #10498048:  Learning rate = 0.000004:   Batch Loss = 0.516648, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952783226966858, Accuracy = 0.9175429940223694\n","Iter #10499072:  Learning rate = 0.000004:   Batch Loss = 0.590370, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595252513885498, Accuracy = 0.9173760414123535\n","Iter #10500096:  Learning rate = 0.000004:   Batch Loss = 0.543068, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952647924423218, Accuracy = 0.9177098870277405\n","Iter #10501120:  Learning rate = 0.000004:   Batch Loss = 0.537447, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953232049942017, Accuracy = 0.9177098870277405\n","Iter #10502144:  Learning rate = 0.000004:   Batch Loss = 0.520880, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954080820083618, Accuracy = 0.9178768396377563\n","Iter #10503168:  Learning rate = 0.000004:   Batch Loss = 0.578812, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954012870788574, Accuracy = 0.9175429940223694\n","Iter #10504192:  Learning rate = 0.000004:   Batch Loss = 0.579675, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953337550163269, Accuracy = 0.9180437326431274\n","Iter #10505216:  Learning rate = 0.000004:   Batch Loss = 0.571949, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953043699264526, Accuracy = 0.9180437326431274\n","Iter #10506240:  Learning rate = 0.000004:   Batch Loss = 0.505936, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952332019805908, Accuracy = 0.9175429940223694\n","Iter #10507264:  Learning rate = 0.000004:   Batch Loss = 0.546784, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952417254447937, Accuracy = 0.9177098870277405\n","Iter #10508288:  Learning rate = 0.000004:   Batch Loss = 0.589571, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952609777450562, Accuracy = 0.9175429940223694\n","Iter #10509312:  Learning rate = 0.000004:   Batch Loss = 0.626635, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953149199485779, Accuracy = 0.9180437326431274\n","Iter #10510336:  Learning rate = 0.000004:   Batch Loss = 0.543272, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953575372695923, Accuracy = 0.9180437326431274\n","Iter #10511360:  Learning rate = 0.000004:   Batch Loss = 0.548792, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953954458236694, Accuracy = 0.9180437326431274\n","Iter #10512384:  Learning rate = 0.000004:   Batch Loss = 0.544624, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954477787017822, Accuracy = 0.9178768396377563\n","Iter #10513408:  Learning rate = 0.000004:   Batch Loss = 0.544354, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953305959701538, Accuracy = 0.9180437326431274\n","Iter #10514432:  Learning rate = 0.000004:   Batch Loss = 0.550611, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952703952789307, Accuracy = 0.9177098870277405\n","Iter #10515456:  Learning rate = 0.000004:   Batch Loss = 0.588526, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952293276786804, Accuracy = 0.9175429940223694\n","Iter #10516480:  Learning rate = 0.000004:   Batch Loss = 0.571112, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952101945877075, Accuracy = 0.9172091484069824\n","Iter #10517504:  Learning rate = 0.000004:   Batch Loss = 0.568076, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952006578445435, Accuracy = 0.9170422554016113\n","Iter #10518528:  Learning rate = 0.000004:   Batch Loss = 0.571519, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951830148696899, Accuracy = 0.9172091484069824\n","Iter #10519552:  Learning rate = 0.000004:   Batch Loss = 0.564513, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951733589172363, Accuracy = 0.9172091484069824\n","Iter #10520576:  Learning rate = 0.000004:   Batch Loss = 0.597403, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951234698295593, Accuracy = 0.9170422554016113\n","Iter #10521600:  Learning rate = 0.000004:   Batch Loss = 0.545531, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951114296913147, Accuracy = 0.9170422554016113\n","Iter #10522624:  Learning rate = 0.000004:   Batch Loss = 0.573441, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951226949691772, Accuracy = 0.9173760414123535\n","Iter #10523648:  Learning rate = 0.000004:   Batch Loss = 0.516874, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951793193817139, Accuracy = 0.9175429940223694\n","Iter #10524672:  Learning rate = 0.000004:   Batch Loss = 0.589744, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595189094543457, Accuracy = 0.9175429940223694\n","Iter #10525696:  Learning rate = 0.000004:   Batch Loss = 0.505008, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951519012451172, Accuracy = 0.9172091484069824\n","Iter #10526720:  Learning rate = 0.000004:   Batch Loss = 0.539931, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595115602016449, Accuracy = 0.9172091484069824\n","Iter #10527744:  Learning rate = 0.000004:   Batch Loss = 0.532675, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5950928926467896, Accuracy = 0.9170422554016113\n","Iter #10528768:  Learning rate = 0.000004:   Batch Loss = 0.566751, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951287150382996, Accuracy = 0.9172091484069824\n","Iter #10529792:  Learning rate = 0.000004:   Batch Loss = 0.593114, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952142477035522, Accuracy = 0.9177098870277405\n","Iter #10530816:  Learning rate = 0.000004:   Batch Loss = 0.606316, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952414274215698, Accuracy = 0.9177098870277405\n","Iter #10531840:  Learning rate = 0.000004:   Batch Loss = 0.654003, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952565670013428, Accuracy = 0.9177098870277405\n","Iter #10532864:  Learning rate = 0.000004:   Batch Loss = 0.571010, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952370166778564, Accuracy = 0.9178768396377563\n","Iter #10533888:  Learning rate = 0.000004:   Batch Loss = 0.558909, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951985120773315, Accuracy = 0.9173760414123535\n","Iter #10534912:  Learning rate = 0.000004:   Batch Loss = 0.549862, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951788425445557, Accuracy = 0.9173760414123535\n","Iter #10535936:  Learning rate = 0.000004:   Batch Loss = 0.581048, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595191240310669, Accuracy = 0.9177098870277405\n","Iter #10536960:  Learning rate = 0.000004:   Batch Loss = 0.576621, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951835513114929, Accuracy = 0.9175429940223694\n","Iter #10537984:  Learning rate = 0.000004:   Batch Loss = 0.606429, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951954126358032, Accuracy = 0.9175429940223694\n","Iter #10539008:  Learning rate = 0.000004:   Batch Loss = 0.640694, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952108502388, Accuracy = 0.9177098870277405\n","Iter #10540032:  Learning rate = 0.000004:   Batch Loss = 0.602890, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952492952346802, Accuracy = 0.9177098870277405\n","Iter #10541056:  Learning rate = 0.000004:   Batch Loss = 0.495237, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952448844909668, Accuracy = 0.9178768396377563\n","Iter #10542080:  Learning rate = 0.000004:   Batch Loss = 0.545660, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952402353286743, Accuracy = 0.9178768396377563\n","Iter #10543104:  Learning rate = 0.000004:   Batch Loss = 0.575258, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952714681625366, Accuracy = 0.9177098870277405\n","Iter #10544128:  Learning rate = 0.000004:   Batch Loss = 0.639820, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953207015991211, Accuracy = 0.9178768396377563\n","Iter #10545152:  Learning rate = 0.000004:   Batch Loss = 0.548311, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595329761505127, Accuracy = 0.9178768396377563\n","Iter #10546176:  Learning rate = 0.000004:   Batch Loss = 0.608922, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953242778778076, Accuracy = 0.9182106256484985\n","Iter #10547200:  Learning rate = 0.000004:   Batch Loss = 0.544715, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953126549720764, Accuracy = 0.9178768396377563\n","Iter #10548224:  Learning rate = 0.000004:   Batch Loss = 0.545125, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952826738357544, Accuracy = 0.9178768396377563\n","Iter #10549248:  Learning rate = 0.000004:   Batch Loss = 0.545820, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952649116516113, Accuracy = 0.9175429940223694\n","Iter #10550272:  Learning rate = 0.000004:   Batch Loss = 0.457627, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952723026275635, Accuracy = 0.9178768396377563\n","Iter #10551296:  Learning rate = 0.000004:   Batch Loss = 0.602895, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953115820884705, Accuracy = 0.9178768396377563\n","Iter #10552320:  Learning rate = 0.000004:   Batch Loss = 0.595392, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953261852264404, Accuracy = 0.9178768396377563\n","Iter #10553344:  Learning rate = 0.000004:   Batch Loss = 0.560254, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953293442726135, Accuracy = 0.9177098870277405\n","Iter #10554368:  Learning rate = 0.000004:   Batch Loss = 0.538817, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595319390296936, Accuracy = 0.9177098870277405\n","Iter #10555392:  Learning rate = 0.000004:   Batch Loss = 0.496762, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952669382095337, Accuracy = 0.9178768396377563\n","Iter #10556416:  Learning rate = 0.000004:   Batch Loss = 0.550547, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952542424201965, Accuracy = 0.9178768396377563\n","Iter #10557440:  Learning rate = 0.000004:   Batch Loss = 0.567446, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952273607254028, Accuracy = 0.9170422554016113\n","Iter #10558464:  Learning rate = 0.000004:   Batch Loss = 0.538780, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952454805374146, Accuracy = 0.9168753027915955\n","Iter #10559488:  Learning rate = 0.000004:   Batch Loss = 0.557703, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952621102333069, Accuracy = 0.9172091484069824\n","Iter #10560512:  Learning rate = 0.000004:   Batch Loss = 0.542633, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595332682132721, Accuracy = 0.9175429940223694\n","Iter #10561536:  Learning rate = 0.000004:   Batch Loss = 0.502133, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953736305236816, Accuracy = 0.9175429940223694\n","Iter #10562560:  Learning rate = 0.000004:   Batch Loss = 0.625970, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953584909439087, Accuracy = 0.9177098870277405\n","Iter #10563584:  Learning rate = 0.000004:   Batch Loss = 0.579637, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595319926738739, Accuracy = 0.9180437326431274\n","Iter #10564608:  Learning rate = 0.000004:   Batch Loss = 0.561171, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952707529067993, Accuracy = 0.9182106256484985\n","Iter #10565632:  Learning rate = 0.000004:   Batch Loss = 0.601795, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952693223953247, Accuracy = 0.9177098870277405\n","Iter #10566656:  Learning rate = 0.000004:   Batch Loss = 0.540411, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952262878417969, Accuracy = 0.9178768396377563\n","Iter #10567680:  Learning rate = 0.000004:   Batch Loss = 0.524779, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951947569847107, Accuracy = 0.9173760414123535\n","Iter #10568704:  Learning rate = 0.000004:   Batch Loss = 0.575117, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952149629592896, Accuracy = 0.9178768396377563\n","Iter #10569728:  Learning rate = 0.000004:   Batch Loss = 0.485394, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952320694923401, Accuracy = 0.9182106256484985\n","Iter #10570752:  Learning rate = 0.000004:   Batch Loss = 0.510429, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952675342559814, Accuracy = 0.9180437326431274\n","Iter #10571776:  Learning rate = 0.000004:   Batch Loss = 0.582146, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952363014221191, Accuracy = 0.9177098870277405\n","Iter #10572800:  Learning rate = 0.000004:   Batch Loss = 0.546322, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952494740486145, Accuracy = 0.9172091484069824\n","Iter #10573824:  Learning rate = 0.000004:   Batch Loss = 0.686636, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952510833740234, Accuracy = 0.9170422554016113\n","Iter #10574848:  Learning rate = 0.000004:   Batch Loss = 0.521165, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952094793319702, Accuracy = 0.9170422554016113\n","Iter #10575872:  Learning rate = 0.000004:   Batch Loss = 0.594990, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952236652374268, Accuracy = 0.9172091484069824\n","Iter #10576896:  Learning rate = 0.000004:   Batch Loss = 0.560262, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59521484375, Accuracy = 0.9170422554016113\n","Iter #10577920:  Learning rate = 0.000004:   Batch Loss = 0.649765, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951911211013794, Accuracy = 0.9170422554016113\n","Iter #10578944:  Learning rate = 0.000004:   Batch Loss = 0.517910, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951887369155884, Accuracy = 0.9173760414123535\n","Iter #10579968:  Learning rate = 0.000004:   Batch Loss = 0.542076, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951834917068481, Accuracy = 0.9172091484069824\n","Iter #10580992:  Learning rate = 0.000004:   Batch Loss = 0.561502, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951878428459167, Accuracy = 0.9178768396377563\n","Iter #10582016:  Learning rate = 0.000004:   Batch Loss = 0.526632, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951709747314453, Accuracy = 0.9173760414123535\n","Iter #10583040:  Learning rate = 0.000004:   Batch Loss = 0.597929, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951513051986694, Accuracy = 0.9172091484069824\n","Iter #10584064:  Learning rate = 0.000004:   Batch Loss = 0.549611, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951772928237915, Accuracy = 0.9172091484069824\n","Iter #10585088:  Learning rate = 0.000004:   Batch Loss = 0.519064, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951848030090332, Accuracy = 0.9173760414123535\n","Iter #10586112:  Learning rate = 0.000004:   Batch Loss = 0.594003, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952169299125671, Accuracy = 0.9173760414123535\n","Iter #10587136:  Learning rate = 0.000004:   Batch Loss = 0.551374, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952309370040894, Accuracy = 0.9172091484069824\n","Iter #10588160:  Learning rate = 0.000004:   Batch Loss = 0.502729, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952290296554565, Accuracy = 0.9173760414123535\n","Iter #10589184:  Learning rate = 0.000004:   Batch Loss = 0.612730, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595239520072937, Accuracy = 0.9177098870277405\n","Iter #10590208:  Learning rate = 0.000004:   Batch Loss = 0.561462, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952943563461304, Accuracy = 0.9178768396377563\n","Iter #10591232:  Learning rate = 0.000004:   Batch Loss = 0.670706, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952844023704529, Accuracy = 0.9177098870277405\n","Iter #10592256:  Learning rate = 0.000004:   Batch Loss = 0.592919, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952717065811157, Accuracy = 0.9178768396377563\n","Iter #10593280:  Learning rate = 0.000004:   Batch Loss = 0.529993, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953186750411987, Accuracy = 0.9178768396377563\n","Iter #10594304:  Learning rate = 0.000004:   Batch Loss = 0.585772, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953699946403503, Accuracy = 0.9183775782585144\n","Iter #10595328:  Learning rate = 0.000004:   Batch Loss = 0.523690, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954644083976746, Accuracy = 0.9190452098846436\n","Iter #10596352:  Learning rate = 0.000004:   Batch Loss = 0.556029, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954865217208862, Accuracy = 0.9190452098846436\n","Iter #10597376:  Learning rate = 0.000004:   Batch Loss = 0.626483, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954183340072632, Accuracy = 0.9187114238739014\n","Iter #10598400:  Learning rate = 0.000004:   Batch Loss = 0.523122, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952990651130676, Accuracy = 0.9180437326431274\n","Iter #10599424:  Learning rate = 0.000004:   Batch Loss = 0.617009, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952495336532593, Accuracy = 0.9178768396377563\n","Iter #10600448:  Learning rate = 0.000004:   Batch Loss = 0.545780, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952702164649963, Accuracy = 0.9182106256484985\n","Iter #10601472:  Learning rate = 0.000004:   Batch Loss = 0.593263, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952243804931641, Accuracy = 0.9180437326431274\n","Iter #10602496:  Learning rate = 0.000004:   Batch Loss = 0.552630, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952158570289612, Accuracy = 0.9177098870277405\n","Iter #10603520:  Learning rate = 0.000004:   Batch Loss = 0.566603, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595252275466919, Accuracy = 0.9167084097862244\n","Iter #10604544:  Learning rate = 0.000004:   Batch Loss = 0.553844, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952735543251038, Accuracy = 0.9160407185554504\n","Iter #10605568:  Learning rate = 0.000004:   Batch Loss = 0.527335, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953270196914673, Accuracy = 0.9158738255500793\n","Iter #10606592:  Learning rate = 0.000004:   Batch Loss = 0.617664, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953528881072998, Accuracy = 0.9162076711654663\n","Iter #10607616:  Learning rate = 0.000004:   Batch Loss = 0.534030, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595364511013031, Accuracy = 0.9158738255500793\n","Iter #10608640:  Learning rate = 0.000004:   Batch Loss = 0.587263, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952755212783813, Accuracy = 0.9165414571762085\n","Iter #10609664:  Learning rate = 0.000004:   Batch Loss = 0.588072, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952117443084717, Accuracy = 0.9168753027915955\n","Iter #10610688:  Learning rate = 0.000004:   Batch Loss = 0.538089, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952429175376892, Accuracy = 0.9178768396377563\n","Iter #10611712:  Learning rate = 0.000004:   Batch Loss = 0.556126, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595279335975647, Accuracy = 0.9178768396377563\n","Iter #10612736:  Learning rate = 0.000004:   Batch Loss = 0.556561, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952509641647339, Accuracy = 0.9180437326431274\n","Iter #10613760:  Learning rate = 0.000004:   Batch Loss = 0.570277, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952736139297485, Accuracy = 0.9180437326431274\n","Iter #10614784:  Learning rate = 0.000004:   Batch Loss = 0.586311, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953094363212585, Accuracy = 0.9180437326431274\n","Iter #10615808:  Learning rate = 0.000004:   Batch Loss = 0.589543, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595291256904602, Accuracy = 0.9178768396377563\n","Iter #10616832:  Learning rate = 0.000004:   Batch Loss = 0.651350, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952938795089722, Accuracy = 0.9178768396377563\n","Iter #10617856:  Learning rate = 0.000004:   Batch Loss = 0.575949, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953138470649719, Accuracy = 0.9178768396377563\n","Iter #10618880:  Learning rate = 0.000004:   Batch Loss = 0.528469, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952963829040527, Accuracy = 0.9177098870277405\n","Iter #10619904:  Learning rate = 0.000004:   Batch Loss = 0.548356, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953212976455688, Accuracy = 0.9178768396377563\n","Iter #10620928:  Learning rate = 0.000004:   Batch Loss = 0.544874, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953948497772217, Accuracy = 0.9177098870277405\n","Iter #10621952:  Learning rate = 0.000004:   Batch Loss = 0.561731, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954585671424866, Accuracy = 0.9177098870277405\n","Iter #10622976:  Learning rate = 0.000004:   Batch Loss = 0.518945, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954424738883972, Accuracy = 0.9180437326431274\n","Iter #10624000:  Learning rate = 0.000004:   Batch Loss = 0.610899, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953933596611023, Accuracy = 0.9177098870277405\n","Iter #10625024:  Learning rate = 0.000004:   Batch Loss = 0.542658, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953381061553955, Accuracy = 0.9178768396377563\n","Iter #10626048:  Learning rate = 0.000004:   Batch Loss = 0.560447, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595227062702179, Accuracy = 0.9177098870277405\n","Iter #10627072:  Learning rate = 0.000004:   Batch Loss = 0.572949, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595177948474884, Accuracy = 0.9172091484069824\n","Iter #10628096:  Learning rate = 0.000004:   Batch Loss = 0.492489, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951653122901917, Accuracy = 0.9172091484069824\n","Iter #10629120:  Learning rate = 0.000004:   Batch Loss = 0.606776, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951722860336304, Accuracy = 0.9168753027915955\n","Iter #10630144:  Learning rate = 0.000004:   Batch Loss = 0.607060, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951908826828003, Accuracy = 0.9170422554016113\n","Iter #10631168:  Learning rate = 0.000004:   Batch Loss = 0.505103, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952103137969971, Accuracy = 0.9173760414123535\n","Iter #10632192:  Learning rate = 0.000004:   Batch Loss = 0.553433, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952494740486145, Accuracy = 0.9177098870277405\n","Iter #10633216:  Learning rate = 0.000004:   Batch Loss = 0.508636, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952506065368652, Accuracy = 0.9178768396377563\n","Iter #10634240:  Learning rate = 0.000004:   Batch Loss = 0.536630, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952431559562683, Accuracy = 0.9180437326431274\n","Iter #10635264:  Learning rate = 0.000004:   Batch Loss = 0.550993, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952359437942505, Accuracy = 0.9180437326431274\n","Iter #10636288:  Learning rate = 0.000004:   Batch Loss = 0.572924, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952352285385132, Accuracy = 0.9180437326431274\n","Iter #10637312:  Learning rate = 0.000004:   Batch Loss = 0.592892, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595218300819397, Accuracy = 0.9177098870277405\n","Iter #10638336:  Learning rate = 0.000004:   Batch Loss = 0.599284, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952098369598389, Accuracy = 0.9178768396377563\n","Iter #10639360:  Learning rate = 0.000004:   Batch Loss = 0.536791, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952003002166748, Accuracy = 0.9172091484069824\n","Iter #10640384:  Learning rate = 0.000004:   Batch Loss = 0.585059, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951962471008301, Accuracy = 0.9172091484069824\n","Iter #10641408:  Learning rate = 0.000004:   Batch Loss = 0.573767, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952311158180237, Accuracy = 0.9177098870277405\n","Iter #10642432:  Learning rate = 0.000004:   Batch Loss = 0.670189, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408313751221, Accuracy = 0.9177098870277405\n","Iter #10643456:  Learning rate = 0.000004:   Batch Loss = 0.571658, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952765941619873, Accuracy = 0.9178768396377563\n","Iter #10644480:  Learning rate = 0.000004:   Batch Loss = 0.575464, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953176021575928, Accuracy = 0.9182106256484985\n","Iter #10645504:  Learning rate = 0.000004:   Batch Loss = 0.534904, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953018665313721, Accuracy = 0.9180437326431274\n","Iter #10646528:  Learning rate = 0.000004:   Batch Loss = 0.595046, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952937602996826, Accuracy = 0.9178768396377563\n","Iter #10647552:  Learning rate = 0.000004:   Batch Loss = 0.585732, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952703952789307, Accuracy = 0.9177098870277405\n","Iter #10648576:  Learning rate = 0.000004:   Batch Loss = 0.536489, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953172445297241, Accuracy = 0.9178768396377563\n","Iter #10649600:  Learning rate = 0.000004:   Batch Loss = 0.524839, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953013896942139, Accuracy = 0.9178768396377563\n","Iter #10650624:  Learning rate = 0.000004:   Batch Loss = 0.574370, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952557325363159, Accuracy = 0.9177098870277405\n","Iter #10651648:  Learning rate = 0.000004:   Batch Loss = 0.597103, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952204465866089, Accuracy = 0.9172091484069824\n","Iter #10652672:  Learning rate = 0.000004:   Batch Loss = 0.530796, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952019691467285, Accuracy = 0.9167084097862244\n","Iter #10653696:  Learning rate = 0.000004:   Batch Loss = 0.568794, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951966047286987, Accuracy = 0.9168753027915955\n","Iter #10654720:  Learning rate = 0.000004:   Batch Loss = 0.611513, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951566696166992, Accuracy = 0.9168753027915955\n","Iter #10655744:  Learning rate = 0.000004:   Batch Loss = 0.654379, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951864719390869, Accuracy = 0.9167084097862244\n","Iter #10656768:  Learning rate = 0.000004:   Batch Loss = 0.515254, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952472686767578, Accuracy = 0.9172091484069824\n","Iter #10657792:  Learning rate = 0.000004:   Batch Loss = 0.515741, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952684283256531, Accuracy = 0.9178768396377563\n","Iter #10658816:  Learning rate = 0.000004:   Batch Loss = 0.587245, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953006744384766, Accuracy = 0.9178768396377563\n","Iter #10659840:  Learning rate = 0.000004:   Batch Loss = 0.594904, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953018069267273, Accuracy = 0.9180437326431274\n","Iter #10660864:  Learning rate = 0.000004:   Batch Loss = 0.572051, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953100919723511, Accuracy = 0.9177098870277405\n","Iter #10661888:  Learning rate = 0.000004:   Batch Loss = 0.537574, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953266620635986, Accuracy = 0.9177098870277405\n","Iter #10662912:  Learning rate = 0.000004:   Batch Loss = 0.549875, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953010320663452, Accuracy = 0.9178768396377563\n","Iter #10663936:  Learning rate = 0.000004:   Batch Loss = 0.546605, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952494740486145, Accuracy = 0.9175429940223694\n","Iter #10664960:  Learning rate = 0.000004:   Batch Loss = 0.581385, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951741933822632, Accuracy = 0.9173760414123535\n","Iter #10665984:  Learning rate = 0.000004:   Batch Loss = 0.524275, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951204299926758, Accuracy = 0.9170422554016113\n","Iter #10667008:  Learning rate = 0.000004:   Batch Loss = 0.556707, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951032042503357, Accuracy = 0.9172091484069824\n","Iter #10668032:  Learning rate = 0.000004:   Batch Loss = 0.624098, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951114892959595, Accuracy = 0.9173760414123535\n","Iter #10669056:  Learning rate = 0.000004:   Batch Loss = 0.578532, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951479077339172, Accuracy = 0.9175429940223694\n","Iter #10670080:  Learning rate = 0.000004:   Batch Loss = 0.550717, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595176100730896, Accuracy = 0.9177098870277405\n","Iter #10671104:  Learning rate = 0.000004:   Batch Loss = 0.540339, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595186710357666, Accuracy = 0.9177098870277405\n","Iter #10672128:  Learning rate = 0.000004:   Batch Loss = 0.500052, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952401757240295, Accuracy = 0.9180437326431274\n","Iter #10673152:  Learning rate = 0.000004:   Batch Loss = 0.578054, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952425599098206, Accuracy = 0.9180437326431274\n","Iter #10674176:  Learning rate = 0.000004:   Batch Loss = 0.592393, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951541662216187, Accuracy = 0.9173760414123535\n","Iter #10675200:  Learning rate = 0.000004:   Batch Loss = 0.589911, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951423048973083, Accuracy = 0.9168753027915955\n","Iter #10676224:  Learning rate = 0.000004:   Batch Loss = 0.538352, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951898097991943, Accuracy = 0.9172091484069824\n","Iter #10677248:  Learning rate = 0.000004:   Batch Loss = 0.576627, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951959490776062, Accuracy = 0.9168753027915955\n","Iter #10678272:  Learning rate = 0.000004:   Batch Loss = 0.517947, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952004194259644, Accuracy = 0.9173760414123535\n","Iter #10679296:  Learning rate = 0.000004:   Batch Loss = 0.515258, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952125787734985, Accuracy = 0.9173760414123535\n","Iter #10680320:  Learning rate = 0.000003:   Batch Loss = 0.561968, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156782150269, Accuracy = 0.9175429940223694\n","Iter #10681344:  Learning rate = 0.000003:   Batch Loss = 0.578231, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952497720718384, Accuracy = 0.9180437326431274\n","Iter #10682368:  Learning rate = 0.000003:   Batch Loss = 0.575249, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952982306480408, Accuracy = 0.9178768396377563\n","Iter #10683392:  Learning rate = 0.000003:   Batch Loss = 0.521811, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953211784362793, Accuracy = 0.9178768396377563\n","Iter #10684416:  Learning rate = 0.000003:   Batch Loss = 0.517294, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952966213226318, Accuracy = 0.9178768396377563\n","Iter #10685440:  Learning rate = 0.000003:   Batch Loss = 0.616678, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952900648117065, Accuracy = 0.9178768396377563\n","Iter #10686464:  Learning rate = 0.000003:   Batch Loss = 0.536024, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952795743942261, Accuracy = 0.9178768396377563\n","Iter #10687488:  Learning rate = 0.000003:   Batch Loss = 0.527569, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952440500259399, Accuracy = 0.9178768396377563\n","Iter #10688512:  Learning rate = 0.000003:   Batch Loss = 0.604098, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952239036560059, Accuracy = 0.9175429940223694\n","Iter #10689536:  Learning rate = 0.000003:   Batch Loss = 0.637533, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952062606811523, Accuracy = 0.9172091484069824\n","Iter #10690560:  Learning rate = 0.000003:   Batch Loss = 0.600379, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952236652374268, Accuracy = 0.9170422554016113\n","Iter #10691584:  Learning rate = 0.000003:   Batch Loss = 0.478966, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952465534210205, Accuracy = 0.9170422554016113\n","Iter #10692608:  Learning rate = 0.000003:   Batch Loss = 0.616478, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952117443084717, Accuracy = 0.9173760414123535\n","Iter #10693632:  Learning rate = 0.000003:   Batch Loss = 0.648696, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595203161239624, Accuracy = 0.9173760414123535\n","Iter #10694656:  Learning rate = 0.000003:   Batch Loss = 0.534546, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951765775680542, Accuracy = 0.9170422554016113\n","Iter #10695680:  Learning rate = 0.000003:   Batch Loss = 0.563953, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951622724533081, Accuracy = 0.9173760414123535\n","Iter #10696704:  Learning rate = 0.000003:   Batch Loss = 0.621271, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951383113861084, Accuracy = 0.9172091484069824\n","Iter #10697728:  Learning rate = 0.000003:   Batch Loss = 0.523731, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951727628707886, Accuracy = 0.9175429940223694\n","Iter #10698752:  Learning rate = 0.000003:   Batch Loss = 0.496205, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595206081867218, Accuracy = 0.9175429940223694\n","Iter #10699776:  Learning rate = 0.000003:   Batch Loss = 0.488042, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952144861221313, Accuracy = 0.9175429940223694\n","Iter #10700800:  Learning rate = 0.000003:   Batch Loss = 0.511994, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952416658401489, Accuracy = 0.9180437326431274\n","Iter #10701824:  Learning rate = 0.000003:   Batch Loss = 0.517482, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952332615852356, Accuracy = 0.9177098870277405\n","Iter #10702848:  Learning rate = 0.000003:   Batch Loss = 0.538448, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952631235122681, Accuracy = 0.9177098870277405\n","Iter #10703872:  Learning rate = 0.000003:   Batch Loss = 0.544460, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952740907669067, Accuracy = 0.9173760414123535\n","Iter #10704896:  Learning rate = 0.000003:   Batch Loss = 0.566789, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952674746513367, Accuracy = 0.9170422554016113\n","Iter #10705920:  Learning rate = 0.000003:   Batch Loss = 0.523038, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952706933021545, Accuracy = 0.9170422554016113\n","Iter #10706944:  Learning rate = 0.000003:   Batch Loss = 0.550457, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952937602996826, Accuracy = 0.9173760414123535\n","Iter #10707968:  Learning rate = 0.000003:   Batch Loss = 0.493225, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952975749969482, Accuracy = 0.9175429940223694\n","Iter #10708992:  Learning rate = 0.000003:   Batch Loss = 0.538021, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952776074409485, Accuracy = 0.9173760414123535\n","Iter #10710016:  Learning rate = 0.000003:   Batch Loss = 0.567591, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952879786491394, Accuracy = 0.9177098870277405\n","Iter #10711040:  Learning rate = 0.000003:   Batch Loss = 0.544210, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952837467193604, Accuracy = 0.9175429940223694\n","Iter #10712064:  Learning rate = 0.000003:   Batch Loss = 0.605838, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952708721160889, Accuracy = 0.9175429940223694\n","Iter #10713088:  Learning rate = 0.000003:   Batch Loss = 0.559268, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952708721160889, Accuracy = 0.9177098870277405\n","Iter #10714112:  Learning rate = 0.000003:   Batch Loss = 0.523886, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952582359313965, Accuracy = 0.9177098870277405\n","Iter #10715136:  Learning rate = 0.000003:   Batch Loss = 0.560254, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952891111373901, Accuracy = 0.9178768396377563\n","Iter #10716160:  Learning rate = 0.000003:   Batch Loss = 0.521334, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952629446983337, Accuracy = 0.9178768396377563\n","Iter #10717184:  Learning rate = 0.000003:   Batch Loss = 0.544056, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952881574630737, Accuracy = 0.9180437326431274\n","Iter #10718208:  Learning rate = 0.000003:   Batch Loss = 0.561714, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952903032302856, Accuracy = 0.9180437326431274\n","Iter #10719232:  Learning rate = 0.000003:   Batch Loss = 0.588485, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952509641647339, Accuracy = 0.9178768396377563\n","Iter #10720256:  Learning rate = 0.000003:   Batch Loss = 0.579972, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952162742614746, Accuracy = 0.9178768396377563\n","Iter #10721280:  Learning rate = 0.000003:   Batch Loss = 0.589810, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952118635177612, Accuracy = 0.9175429940223694\n","Iter #10722304:  Learning rate = 0.000003:   Batch Loss = 0.589512, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595191478729248, Accuracy = 0.9173760414123535\n","Iter #10723328:  Learning rate = 0.000003:   Batch Loss = 0.524094, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952077507972717, Accuracy = 0.9172091484069824\n","Iter #10724352:  Learning rate = 0.000003:   Batch Loss = 0.577780, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952269434928894, Accuracy = 0.9177098870277405\n","Iter #10725376:  Learning rate = 0.000003:   Batch Loss = 0.521475, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952321290969849, Accuracy = 0.9177098870277405\n","Iter #10726400:  Learning rate = 0.000003:   Batch Loss = 0.573562, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952537655830383, Accuracy = 0.9178768396377563\n","Iter #10727424:  Learning rate = 0.000003:   Batch Loss = 0.537414, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952786207199097, Accuracy = 0.9180437326431274\n","Iter #10728448:  Learning rate = 0.000003:   Batch Loss = 0.550154, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952551364898682, Accuracy = 0.9178768396377563\n","Iter #10729472:  Learning rate = 0.000003:   Batch Loss = 0.510123, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952563285827637, Accuracy = 0.9178768396377563\n","Iter #10730496:  Learning rate = 0.000003:   Batch Loss = 0.554588, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952672958374023, Accuracy = 0.9180437326431274\n","Iter #10731520:  Learning rate = 0.000003:   Batch Loss = 0.499283, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952960252761841, Accuracy = 0.9178768396377563\n","Iter #10732544:  Learning rate = 0.000003:   Batch Loss = 0.512566, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953994989395142, Accuracy = 0.9178768396377563\n","Iter #10733568:  Learning rate = 0.000003:   Batch Loss = 0.562955, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954513549804688, Accuracy = 0.9183775782585144\n","Iter #10734592:  Learning rate = 0.000003:   Batch Loss = 0.569523, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5954164862632751, Accuracy = 0.9183775782585144\n","Iter #10735616:  Learning rate = 0.000003:   Batch Loss = 0.619026, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953661203384399, Accuracy = 0.9178768396377563\n","Iter #10736640:  Learning rate = 0.000003:   Batch Loss = 0.584341, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952689051628113, Accuracy = 0.9178768396377563\n","Iter #10737664:  Learning rate = 0.000003:   Batch Loss = 0.565894, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595196008682251, Accuracy = 0.9172091484069824\n","Iter #10738688:  Learning rate = 0.000003:   Batch Loss = 0.499177, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951370000839233, Accuracy = 0.9172091484069824\n","Iter #10739712:  Learning rate = 0.000003:   Batch Loss = 0.549650, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951171517372131, Accuracy = 0.9172091484069824\n","Iter #10740736:  Learning rate = 0.000003:   Batch Loss = 0.579930, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951051115989685, Accuracy = 0.9170422554016113\n","Iter #10741760:  Learning rate = 0.000003:   Batch Loss = 0.504027, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951120853424072, Accuracy = 0.9170422554016113\n","Iter #10742784:  Learning rate = 0.000003:   Batch Loss = 0.578066, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951054096221924, Accuracy = 0.9170422554016113\n","Iter #10743808:  Learning rate = 0.000003:   Batch Loss = 0.559744, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951297283172607, Accuracy = 0.9170422554016113\n","Iter #10744832:  Learning rate = 0.000003:   Batch Loss = 0.592489, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951723456382751, Accuracy = 0.9173760414123535\n","Iter #10745856:  Learning rate = 0.000003:   Batch Loss = 0.574945, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951655507087708, Accuracy = 0.9173760414123535\n","Iter #10746880:  Learning rate = 0.000003:   Batch Loss = 0.579425, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951344966888428, Accuracy = 0.9172091484069824\n","Iter #10747904:  Learning rate = 0.000003:   Batch Loss = 0.566638, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951398611068726, Accuracy = 0.9172091484069824\n","Iter #10748928:  Learning rate = 0.000003:   Batch Loss = 0.542181, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951899886131287, Accuracy = 0.9172091484069824\n","Iter #10749952:  Learning rate = 0.000003:   Batch Loss = 0.560600, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952275991439819, Accuracy = 0.9170422554016113\n","Iter #10750976:  Learning rate = 0.000003:   Batch Loss = 0.505523, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952413082122803, Accuracy = 0.9175429940223694\n","Iter #10752000:  Learning rate = 0.000003:   Batch Loss = 0.606907, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952155590057373, Accuracy = 0.9177098870277405\n","Iter #10753024:  Learning rate = 0.000003:   Batch Loss = 0.591520, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952489376068115, Accuracy = 0.9178768396377563\n","Iter #10754048:  Learning rate = 0.000003:   Batch Loss = 0.606706, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952612161636353, Accuracy = 0.9178768396377563\n","Iter #10755072:  Learning rate = 0.000003:   Batch Loss = 0.615338, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952801704406738, Accuracy = 0.9177098870277405\n","Iter #10756096:  Learning rate = 0.000003:   Batch Loss = 0.559154, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952275395393372, Accuracy = 0.9177098870277405\n","Iter #10757120:  Learning rate = 0.000003:   Batch Loss = 0.586783, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951858758926392, Accuracy = 0.9172091484069824\n","Iter #10758144:  Learning rate = 0.000003:   Batch Loss = 0.507102, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952063798904419, Accuracy = 0.9172091484069824\n","Iter #10759168:  Learning rate = 0.000003:   Batch Loss = 0.513483, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952064990997314, Accuracy = 0.9172091484069824\n","Iter #10760192:  Learning rate = 0.000003:   Batch Loss = 0.581418, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952274203300476, Accuracy = 0.9170422554016113\n","Iter #10761216:  Learning rate = 0.000003:   Batch Loss = 0.540570, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952471494674683, Accuracy = 0.9170422554016113\n","Iter #10762240:  Learning rate = 0.000003:   Batch Loss = 0.638582, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595281720161438, Accuracy = 0.9175429940223694\n","Iter #10763264:  Learning rate = 0.000003:   Batch Loss = 0.635217, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952993631362915, Accuracy = 0.9177098870277405\n","Iter #10764288:  Learning rate = 0.000003:   Batch Loss = 0.553704, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952906608581543, Accuracy = 0.9180437326431274\n","Iter #10765312:  Learning rate = 0.000003:   Batch Loss = 0.619199, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952578783035278, Accuracy = 0.9175429940223694\n","Iter #10766336:  Learning rate = 0.000003:   Batch Loss = 0.561387, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952274203300476, Accuracy = 0.9175429940223694\n","Iter #10767360:  Learning rate = 0.000003:   Batch Loss = 0.525166, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951970815658569, Accuracy = 0.9178768396377563\n","Iter #10768384:  Learning rate = 0.000003:   Batch Loss = 0.545693, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951913595199585, Accuracy = 0.9175429940223694\n","Iter #10769408:  Learning rate = 0.000003:   Batch Loss = 0.695654, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122807502747, Accuracy = 0.9177098870277405\n","Iter #10770432:  Learning rate = 0.000003:   Batch Loss = 0.542174, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952451229095459, Accuracy = 0.9177098870277405\n","Iter #10771456:  Learning rate = 0.000003:   Batch Loss = 0.545961, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952527523040771, Accuracy = 0.9177098870277405\n","Iter #10772480:  Learning rate = 0.000003:   Batch Loss = 0.538950, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952215194702148, Accuracy = 0.9178768396377563\n","Iter #10773504:  Learning rate = 0.000003:   Batch Loss = 0.543652, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952432155609131, Accuracy = 0.9172091484069824\n","Iter #10774528:  Learning rate = 0.000003:   Batch Loss = 0.607140, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952543616294861, Accuracy = 0.9168753027915955\n","Iter #10775552:  Learning rate = 0.000003:   Batch Loss = 0.541144, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952662229537964, Accuracy = 0.9170422554016113\n","Iter #10776576:  Learning rate = 0.000003:   Batch Loss = 0.597471, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952805876731873, Accuracy = 0.9170422554016113\n","Iter #10777600:  Learning rate = 0.000003:   Batch Loss = 0.540418, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595319390296936, Accuracy = 0.9173760414123535\n","Iter #10778624:  Learning rate = 0.000003:   Batch Loss = 0.538263, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953521728515625, Accuracy = 0.9177098870277405\n","Iter #10779648:  Learning rate = 0.000003:   Batch Loss = 0.544437, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595350444316864, Accuracy = 0.9178768396377563\n","Iter #10780672:  Learning rate = 0.000003:   Batch Loss = 0.560567, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953599810600281, Accuracy = 0.9177098870277405\n","Iter #10781696:  Learning rate = 0.000003:   Batch Loss = 0.537076, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953022241592407, Accuracy = 0.9178768396377563\n","Iter #10782720:  Learning rate = 0.000003:   Batch Loss = 0.606307, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952472686767578, Accuracy = 0.9177098870277405\n","Iter #10783744:  Learning rate = 0.000003:   Batch Loss = 0.571423, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952519178390503, Accuracy = 0.9178768396377563\n","Iter #10784768:  Learning rate = 0.000003:   Batch Loss = 0.579434, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952481627464294, Accuracy = 0.9173760414123535\n","Iter #10785792:  Learning rate = 0.000003:   Batch Loss = 0.548155, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952218770980835, Accuracy = 0.9173760414123535\n","Iter #10786816:  Learning rate = 0.000003:   Batch Loss = 0.534910, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952432155609131, Accuracy = 0.9177098870277405\n","Iter #10787840:  Learning rate = 0.000003:   Batch Loss = 0.558135, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952495336532593, Accuracy = 0.9175429940223694\n","Iter #10788864:  Learning rate = 0.000003:   Batch Loss = 0.593625, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952403545379639, Accuracy = 0.9177098870277405\n","Iter #10789888:  Learning rate = 0.000003:   Batch Loss = 0.603408, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952627658843994, Accuracy = 0.9177098870277405\n","Iter #10790912:  Learning rate = 0.000003:   Batch Loss = 0.533146, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952677726745605, Accuracy = 0.9175429940223694\n","Iter #10791936:  Learning rate = 0.000003:   Batch Loss = 0.561363, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952355265617371, Accuracy = 0.9180437326431274\n","Iter #10792960:  Learning rate = 0.000003:   Batch Loss = 0.563140, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952141284942627, Accuracy = 0.9178768396377563\n","Iter #10793984:  Learning rate = 0.000003:   Batch Loss = 0.550551, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59517902135849, Accuracy = 0.9170422554016113\n","Iter #10795008:  Learning rate = 0.000003:   Batch Loss = 0.596218, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951398611068726, Accuracy = 0.9167084097862244\n","Iter #10796032:  Learning rate = 0.000003:   Batch Loss = 0.603836, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595130205154419, Accuracy = 0.9167084097862244\n","Iter #10797056:  Learning rate = 0.000003:   Batch Loss = 0.573418, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951676368713379, Accuracy = 0.9172091484069824\n","Iter #10798080:  Learning rate = 0.000003:   Batch Loss = 0.558017, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952242612838745, Accuracy = 0.9178768396377563\n","Iter #10799104:  Learning rate = 0.000003:   Batch Loss = 0.539276, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952356457710266, Accuracy = 0.9178768396377563\n","Iter #10800128:  Learning rate = 0.000003:   Batch Loss = 0.549285, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952067375183105, Accuracy = 0.9178768396377563\n","Iter #10801152:  Learning rate = 0.000003:   Batch Loss = 0.550973, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595169723033905, Accuracy = 0.9175429940223694\n","Iter #10802176:  Learning rate = 0.000003:   Batch Loss = 0.546726, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951468348503113, Accuracy = 0.9170422554016113\n","Iter #10803200:  Learning rate = 0.000003:   Batch Loss = 0.568059, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951428413391113, Accuracy = 0.9173760414123535\n","Iter #10804224:  Learning rate = 0.000003:   Batch Loss = 0.540384, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951899290084839, Accuracy = 0.9170422554016113\n","Iter #10805248:  Learning rate = 0.000003:   Batch Loss = 0.524257, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952443480491638, Accuracy = 0.9177098870277405\n","Iter #10806272:  Learning rate = 0.000003:   Batch Loss = 0.550298, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952904224395752, Accuracy = 0.9175429940223694\n","Iter #10807296:  Learning rate = 0.000003:   Batch Loss = 0.580885, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953087210655212, Accuracy = 0.9170422554016113\n","Iter #10808320:  Learning rate = 0.000003:   Batch Loss = 0.505717, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953064560890198, Accuracy = 0.9172091484069824\n","Iter #10809344:  Learning rate = 0.000003:   Batch Loss = 0.574556, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952870845794678, Accuracy = 0.9178768396377563\n","Iter #10810368:  Learning rate = 0.000003:   Batch Loss = 0.581916, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952943563461304, Accuracy = 0.9178768396377563\n","Iter #10811392:  Learning rate = 0.000003:   Batch Loss = 0.581469, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953002572059631, Accuracy = 0.9178768396377563\n","Iter #10812416:  Learning rate = 0.000003:   Batch Loss = 0.551199, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953512787818909, Accuracy = 0.9175429940223694\n","Iter #10813440:  Learning rate = 0.000003:   Batch Loss = 0.529861, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953832864761353, Accuracy = 0.9175429940223694\n","Iter #10814464:  Learning rate = 0.000003:   Batch Loss = 0.538987, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595386266708374, Accuracy = 0.9175429940223694\n","Iter #10815488:  Learning rate = 0.000003:   Batch Loss = 0.539357, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953667163848877, Accuracy = 0.9177098870277405\n","Iter #10816512:  Learning rate = 0.000003:   Batch Loss = 0.599622, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953385233879089, Accuracy = 0.9178768396377563\n","Iter #10817536:  Learning rate = 0.000003:   Batch Loss = 0.533841, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953317880630493, Accuracy = 0.9180437326431274\n","Iter #10818560:  Learning rate = 0.000003:   Batch Loss = 0.579296, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953137874603271, Accuracy = 0.9178768396377563\n","Iter #10819584:  Learning rate = 0.000003:   Batch Loss = 0.554726, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952510833740234, Accuracy = 0.9175429940223694\n","Iter #10820608:  Learning rate = 0.000003:   Batch Loss = 0.569747, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952439308166504, Accuracy = 0.9173760414123535\n","Iter #10821632:  Learning rate = 0.000003:   Batch Loss = 0.565878, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952300429344177, Accuracy = 0.9177098870277405\n","Iter #10822656:  Learning rate = 0.000003:   Batch Loss = 0.564231, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952132940292358, Accuracy = 0.9175429940223694\n","Iter #10823680:  Learning rate = 0.000003:   Batch Loss = 0.525884, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952146053314209, Accuracy = 0.9178768396377563\n","Iter #10824704:  Learning rate = 0.000003:   Batch Loss = 0.571286, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952327251434326, Accuracy = 0.9180437326431274\n","Iter #10825728:  Learning rate = 0.000003:   Batch Loss = 0.573777, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952476263046265, Accuracy = 0.9180437326431274\n","Iter #10826752:  Learning rate = 0.000003:   Batch Loss = 0.558383, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952308177947998, Accuracy = 0.9177098870277405\n","Iter #10827776:  Learning rate = 0.000003:   Batch Loss = 0.557911, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952774286270142, Accuracy = 0.9180437326431274\n","Iter #10828800:  Learning rate = 0.000003:   Batch Loss = 0.558505, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953066349029541, Accuracy = 0.9180437326431274\n","Iter #10829824:  Learning rate = 0.000003:   Batch Loss = 0.547713, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953177213668823, Accuracy = 0.9180437326431274\n","Iter #10830848:  Learning rate = 0.000003:   Batch Loss = 0.571684, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952631235122681, Accuracy = 0.9180437326431274\n","Iter #10831872:  Learning rate = 0.000003:   Batch Loss = 0.616183, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595238447189331, Accuracy = 0.9180437326431274\n","Iter #10832896:  Learning rate = 0.000003:   Batch Loss = 0.480299, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952033400535583, Accuracy = 0.9175429940223694\n","Iter #10833920:  Learning rate = 0.000003:   Batch Loss = 0.524799, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595169186592102, Accuracy = 0.9172091484069824\n","Iter #10834944:  Learning rate = 0.000003:   Batch Loss = 0.525657, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595162034034729, Accuracy = 0.9170422554016113\n","Iter #10835968:  Learning rate = 0.000003:   Batch Loss = 0.554973, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951676368713379, Accuracy = 0.9170422554016113\n","Iter #10836992:  Learning rate = 0.000003:   Batch Loss = 0.465586, Accuracy = 0.9921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951530933380127, Accuracy = 0.9172091484069824\n","Iter #10838016:  Learning rate = 0.000003:   Batch Loss = 0.525602, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951610803604126, Accuracy = 0.9170422554016113\n","Iter #10839040:  Learning rate = 0.000003:   Batch Loss = 0.578530, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951828956604004, Accuracy = 0.9168753027915955\n","Iter #10840064:  Learning rate = 0.000003:   Batch Loss = 0.563515, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951942205429077, Accuracy = 0.9168753027915955\n","Iter #10841088:  Learning rate = 0.000003:   Batch Loss = 0.578828, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951809883117676, Accuracy = 0.9167084097862244\n","Iter #10842112:  Learning rate = 0.000003:   Batch Loss = 0.551445, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951855182647705, Accuracy = 0.9175429940223694\n","Iter #10843136:  Learning rate = 0.000003:   Batch Loss = 0.565378, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952256321907043, Accuracy = 0.9177098870277405\n","Iter #10844160:  Learning rate = 0.000003:   Batch Loss = 0.551093, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952597856521606, Accuracy = 0.9182106256484985\n","Iter #10845184:  Learning rate = 0.000003:   Batch Loss = 0.598833, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952602624893188, Accuracy = 0.9182106256484985\n","Iter #10846208:  Learning rate = 0.000003:   Batch Loss = 0.489399, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952619314193726, Accuracy = 0.9180437326431274\n","Iter #10847232:  Learning rate = 0.000003:   Batch Loss = 0.558930, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595248818397522, Accuracy = 0.9177098870277405\n","Iter #10848256:  Learning rate = 0.000003:   Batch Loss = 0.608547, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952295660972595, Accuracy = 0.9177098870277405\n","Iter #10849280:  Learning rate = 0.000003:   Batch Loss = 0.546097, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952574014663696, Accuracy = 0.9177098870277405\n","Iter #10850304:  Learning rate = 0.000003:   Batch Loss = 0.592021, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952847003936768, Accuracy = 0.9175429940223694\n","Iter #10851328:  Learning rate = 0.000003:   Batch Loss = 0.586126, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952737331390381, Accuracy = 0.9175429940223694\n","Iter #10852352:  Learning rate = 0.000003:   Batch Loss = 0.557161, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952602624893188, Accuracy = 0.9175429940223694\n","Iter #10853376:  Learning rate = 0.000003:   Batch Loss = 0.581534, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953208208084106, Accuracy = 0.9178768396377563\n","Iter #10854400:  Learning rate = 0.000003:   Batch Loss = 0.559208, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953428745269775, Accuracy = 0.9178768396377563\n","Iter #10855424:  Learning rate = 0.000003:   Batch Loss = 0.604280, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953433513641357, Accuracy = 0.9180437326431274\n","Iter #10856448:  Learning rate = 0.000003:   Batch Loss = 0.513329, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953469276428223, Accuracy = 0.9180437326431274\n","Iter #10857472:  Learning rate = 0.000003:   Batch Loss = 0.504167, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953238010406494, Accuracy = 0.9180437326431274\n","Iter #10858496:  Learning rate = 0.000003:   Batch Loss = 0.575913, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952854156494141, Accuracy = 0.9177098870277405\n","Iter #10859520:  Learning rate = 0.000003:   Batch Loss = 0.585539, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952180624008179, Accuracy = 0.9175429940223694\n","Iter #10860544:  Learning rate = 0.000003:   Batch Loss = 0.530165, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951793193817139, Accuracy = 0.9175429940223694\n","Iter #10861568:  Learning rate = 0.000003:   Batch Loss = 0.595714, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951991081237793, Accuracy = 0.9177098870277405\n","Iter #10862592:  Learning rate = 0.000003:   Batch Loss = 0.539087, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952264070510864, Accuracy = 0.9178768396377563\n","Iter #10863616:  Learning rate = 0.000003:   Batch Loss = 0.517460, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595217227935791, Accuracy = 0.9178768396377563\n","Iter #10864640:  Learning rate = 0.000003:   Batch Loss = 0.552614, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951989889144897, Accuracy = 0.9178768396377563\n","Iter #10865664:  Learning rate = 0.000003:   Batch Loss = 0.625289, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951989889144897, Accuracy = 0.9178768396377563\n","Iter #10866688:  Learning rate = 0.000003:   Batch Loss = 0.555045, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951768159866333, Accuracy = 0.9177098870277405\n","Iter #10867712:  Learning rate = 0.000003:   Batch Loss = 0.553721, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951342582702637, Accuracy = 0.9173760414123535\n","Iter #10868736:  Learning rate = 0.000003:   Batch Loss = 0.485651, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951133966445923, Accuracy = 0.9172091484069824\n","Iter #10869760:  Learning rate = 0.000003:   Batch Loss = 0.587708, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951352715492249, Accuracy = 0.9172091484069824\n","Iter #10870784:  Learning rate = 0.000003:   Batch Loss = 0.600921, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951722860336304, Accuracy = 0.9175429940223694\n","Iter #10871808:  Learning rate = 0.000003:   Batch Loss = 0.602438, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951893329620361, Accuracy = 0.9175429940223694\n","Iter #10872832:  Learning rate = 0.000003:   Batch Loss = 0.524431, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951997637748718, Accuracy = 0.9178768396377563\n","Iter #10873856:  Learning rate = 0.000003:   Batch Loss = 0.530472, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952476859092712, Accuracy = 0.9178768396377563\n","Iter #10874880:  Learning rate = 0.000003:   Batch Loss = 0.545864, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952860116958618, Accuracy = 0.9182106256484985\n","Iter #10875904:  Learning rate = 0.000003:   Batch Loss = 0.533736, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952953100204468, Accuracy = 0.9182106256484985\n","Iter #10876928:  Learning rate = 0.000003:   Batch Loss = 0.655275, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952433347702026, Accuracy = 0.9177098870277405\n","Iter #10877952:  Learning rate = 0.000003:   Batch Loss = 0.545964, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951790809631348, Accuracy = 0.9172091484069824\n","Iter #10878976:  Learning rate = 0.000003:   Batch Loss = 0.563432, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951459407806396, Accuracy = 0.9173760414123535\n","Iter #10880000:  Learning rate = 0.000003:   Batch Loss = 0.530405, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951367616653442, Accuracy = 0.9172091484069824\n","Iter #10881024:  Learning rate = 0.000003:   Batch Loss = 0.515112, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951457023620605, Accuracy = 0.9170422554016113\n","Iter #10882048:  Learning rate = 0.000003:   Batch Loss = 0.538874, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951704978942871, Accuracy = 0.9172091484069824\n","Iter #10883072:  Learning rate = 0.000003:   Batch Loss = 0.576302, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952075123786926, Accuracy = 0.9173760414123535\n","Iter #10884096:  Learning rate = 0.000003:   Batch Loss = 0.536347, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59520423412323, Accuracy = 0.9175429940223694\n","Iter #10885120:  Learning rate = 0.000003:   Batch Loss = 0.541647, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952075719833374, Accuracy = 0.9175429940223694\n","Iter #10886144:  Learning rate = 0.000003:   Batch Loss = 0.585749, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951807498931885, Accuracy = 0.9172091484069824\n","Iter #10887168:  Learning rate = 0.000003:   Batch Loss = 0.555535, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951690673828125, Accuracy = 0.9172091484069824\n","Iter #10888192:  Learning rate = 0.000003:   Batch Loss = 0.575146, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951321125030518, Accuracy = 0.9168753027915955\n","Iter #10889216:  Learning rate = 0.000003:   Batch Loss = 0.535081, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951408743858337, Accuracy = 0.9170422554016113\n","Iter #10890240:  Learning rate = 0.000003:   Batch Loss = 0.584703, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951371192932129, Accuracy = 0.9170422554016113\n","Iter #10891264:  Learning rate = 0.000003:   Batch Loss = 0.594431, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951709747314453, Accuracy = 0.9172091484069824\n","Iter #10892288:  Learning rate = 0.000003:   Batch Loss = 0.606388, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952131748199463, Accuracy = 0.9168753027915955\n","Iter #10893312:  Learning rate = 0.000003:   Batch Loss = 0.533526, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952375531196594, Accuracy = 0.9172091484069824\n","Iter #10894336:  Learning rate = 0.000003:   Batch Loss = 0.540594, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952270030975342, Accuracy = 0.9172091484069824\n","Iter #10895360:  Learning rate = 0.000003:   Batch Loss = 0.551789, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952627658843994, Accuracy = 0.9178768396377563\n","Iter #10896384:  Learning rate = 0.000003:   Batch Loss = 0.613732, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953036546707153, Accuracy = 0.9180437326431274\n","Iter #10897408:  Learning rate = 0.000003:   Batch Loss = 0.509172, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953246355056763, Accuracy = 0.9178768396377563\n","Iter #10898432:  Learning rate = 0.000003:   Batch Loss = 0.547661, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595353364944458, Accuracy = 0.9177098870277405\n","Iter #10899456:  Learning rate = 0.000003:   Batch Loss = 0.561400, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953502655029297, Accuracy = 0.9177098870277405\n","Iter #10900480:  Learning rate = 0.000003:   Batch Loss = 0.540223, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595339834690094, Accuracy = 0.9177098870277405\n","Iter #10901504:  Learning rate = 0.000003:   Batch Loss = 0.580124, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952879190444946, Accuracy = 0.9177098870277405\n","Iter #10902528:  Learning rate = 0.000003:   Batch Loss = 0.584157, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595285952091217, Accuracy = 0.9173760414123535\n","Iter #10903552:  Learning rate = 0.000003:   Batch Loss = 0.577481, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595281720161438, Accuracy = 0.9173760414123535\n","Iter #10904576:  Learning rate = 0.000003:   Batch Loss = 0.553078, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952630043029785, Accuracy = 0.9178768396377563\n","Iter #10905600:  Learning rate = 0.000003:   Batch Loss = 0.492126, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952558517456055, Accuracy = 0.9178768396377563\n","Iter #10906624:  Learning rate = 0.000003:   Batch Loss = 0.551670, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952267646789551, Accuracy = 0.9177098870277405\n","Iter #10907648:  Learning rate = 0.000003:   Batch Loss = 0.579485, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595223605632782, Accuracy = 0.9177098870277405\n","Iter #10908672:  Learning rate = 0.000003:   Batch Loss = 0.570208, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951995253562927, Accuracy = 0.9175429940223694\n","Iter #10909696:  Learning rate = 0.000003:   Batch Loss = 0.608769, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951927900314331, Accuracy = 0.9173760414123535\n","Iter #10910720:  Learning rate = 0.000003:   Batch Loss = 0.536161, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952621698379517, Accuracy = 0.9180437326431274\n","Iter #10911744:  Learning rate = 0.000003:   Batch Loss = 0.538434, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952795743942261, Accuracy = 0.9178768396377563\n","Iter #10912768:  Learning rate = 0.000003:   Batch Loss = 0.519232, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952299237251282, Accuracy = 0.9180437326431274\n","Iter #10913792:  Learning rate = 0.000003:   Batch Loss = 0.587795, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952082872390747, Accuracy = 0.9178768396377563\n","Iter #10914816:  Learning rate = 0.000003:   Batch Loss = 0.537578, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952385663986206, Accuracy = 0.9180437326431274\n","Iter #10915840:  Learning rate = 0.000003:   Batch Loss = 0.512286, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952343940734863, Accuracy = 0.9180437326431274\n","Iter #10916864:  Learning rate = 0.000003:   Batch Loss = 0.522665, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952414274215698, Accuracy = 0.9177098870277405\n","Iter #10917888:  Learning rate = 0.000003:   Batch Loss = 0.592406, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952197313308716, Accuracy = 0.9177098870277405\n","Iter #10918912:  Learning rate = 0.000003:   Batch Loss = 0.589797, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952328443527222, Accuracy = 0.9178768396377563\n","Iter #10919936:  Learning rate = 0.000003:   Batch Loss = 0.574116, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952362418174744, Accuracy = 0.9178768396377563\n","Iter #10920960:  Learning rate = 0.000003:   Batch Loss = 0.565466, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952526330947876, Accuracy = 0.9178768396377563\n","Iter #10921984:  Learning rate = 0.000003:   Batch Loss = 0.587564, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952669978141785, Accuracy = 0.9178768396377563\n","Iter #10923008:  Learning rate = 0.000003:   Batch Loss = 0.567830, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953306555747986, Accuracy = 0.9182106256484985\n","Iter #10924032:  Learning rate = 0.000003:   Batch Loss = 0.485825, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595374345779419, Accuracy = 0.9180437326431274\n","Iter #10925056:  Learning rate = 0.000003:   Batch Loss = 0.490209, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953891277313232, Accuracy = 0.9178768396377563\n","Iter #10926080:  Learning rate = 0.000003:   Batch Loss = 0.521619, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953806638717651, Accuracy = 0.9180437326431274\n","Iter #10927104:  Learning rate = 0.000003:   Batch Loss = 0.568460, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59532630443573, Accuracy = 0.9178768396377563\n","Iter #10928128:  Learning rate = 0.000003:   Batch Loss = 0.500325, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952673554420471, Accuracy = 0.9177098870277405\n","Iter #10929152:  Learning rate = 0.000003:   Batch Loss = 0.542608, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952171087265015, Accuracy = 0.9173760414123535\n","Iter #10930176:  Learning rate = 0.000003:   Batch Loss = 0.582207, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952013731002808, Accuracy = 0.9175429940223694\n","Iter #10931200:  Learning rate = 0.000003:   Batch Loss = 0.542234, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122807502747, Accuracy = 0.9177098870277405\n","Iter #10932224:  Learning rate = 0.000003:   Batch Loss = 0.567910, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951992273330688, Accuracy = 0.9173760414123535\n","Iter #10933248:  Learning rate = 0.000003:   Batch Loss = 0.548972, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951695442199707, Accuracy = 0.9170422554016113\n","Iter #10934272:  Learning rate = 0.000003:   Batch Loss = 0.675837, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951589345932007, Accuracy = 0.9172091484069824\n","Iter #10935296:  Learning rate = 0.000003:   Batch Loss = 0.538275, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951693058013916, Accuracy = 0.9170422554016113\n","Iter #10936320:  Learning rate = 0.000003:   Batch Loss = 0.591632, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951858758926392, Accuracy = 0.9172091484069824\n","Iter #10937344:  Learning rate = 0.000003:   Batch Loss = 0.520372, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951958894729614, Accuracy = 0.9170422554016113\n","Iter #10938368:  Learning rate = 0.000003:   Batch Loss = 0.624499, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951998233795166, Accuracy = 0.9172091484069824\n","Iter #10939392:  Learning rate = 0.000003:   Batch Loss = 0.544544, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595234215259552, Accuracy = 0.9173760414123535\n","Iter #10940416:  Learning rate = 0.000003:   Batch Loss = 0.548971, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952568650245667, Accuracy = 0.9177098870277405\n","Iter #10941440:  Learning rate = 0.000003:   Batch Loss = 0.580506, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952595472335815, Accuracy = 0.9177098870277405\n","Iter #10942464:  Learning rate = 0.000003:   Batch Loss = 0.493679, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952575206756592, Accuracy = 0.9177098870277405\n","Iter #10943488:  Learning rate = 0.000003:   Batch Loss = 0.574374, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952500700950623, Accuracy = 0.9177098870277405\n","Iter #10944512:  Learning rate = 0.000003:   Batch Loss = 0.579003, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952333211898804, Accuracy = 0.9178768396377563\n","Iter #10945536:  Learning rate = 0.000003:   Batch Loss = 0.571814, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952167510986328, Accuracy = 0.9178768396377563\n","Iter #10946560:  Learning rate = 0.000003:   Batch Loss = 0.605806, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952108502388, Accuracy = 0.9178768396377563\n","Iter #10947584:  Learning rate = 0.000003:   Batch Loss = 0.571173, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952131748199463, Accuracy = 0.9177098870277405\n","Iter #10948608:  Learning rate = 0.000003:   Batch Loss = 0.550302, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951862931251526, Accuracy = 0.9175429940223694\n","Iter #10949632:  Learning rate = 0.000003:   Batch Loss = 0.574226, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951800346374512, Accuracy = 0.9175429940223694\n","Iter #10950656:  Learning rate = 0.000003:   Batch Loss = 0.571591, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951460599899292, Accuracy = 0.9172091484069824\n","Iter #10951680:  Learning rate = 0.000003:   Batch Loss = 0.534927, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951657295227051, Accuracy = 0.9175429940223694\n","Iter #10952704:  Learning rate = 0.000003:   Batch Loss = 0.570349, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951797962188721, Accuracy = 0.9175429940223694\n","Iter #10953728:  Learning rate = 0.000003:   Batch Loss = 0.612496, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951961278915405, Accuracy = 0.9175429940223694\n","Iter #10954752:  Learning rate = 0.000003:   Batch Loss = 0.524111, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595204770565033, Accuracy = 0.9178768396377563\n","Iter #10955776:  Learning rate = 0.000003:   Batch Loss = 0.540954, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952174663543701, Accuracy = 0.9178768396377563\n","Iter #10956800:  Learning rate = 0.000003:   Batch Loss = 0.599093, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952193737030029, Accuracy = 0.9178768396377563\n","Iter #10957824:  Learning rate = 0.000003:   Batch Loss = 0.497940, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951955318450928, Accuracy = 0.9175429940223694\n","Iter #10958848:  Learning rate = 0.000003:   Batch Loss = 0.559538, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951730012893677, Accuracy = 0.9168753027915955\n","Iter #10959872:  Learning rate = 0.000003:   Batch Loss = 0.594325, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951886177062988, Accuracy = 0.9168753027915955\n","Iter #10960896:  Learning rate = 0.000003:   Batch Loss = 0.527533, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951955318450928, Accuracy = 0.9168753027915955\n","Iter #10961920:  Learning rate = 0.000003:   Batch Loss = 0.470687, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952128171920776, Accuracy = 0.9168753027915955\n","Iter #10962944:  Learning rate = 0.000003:   Batch Loss = 0.633920, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048897743225, Accuracy = 0.9168753027915955\n","Iter #10963968:  Learning rate = 0.000003:   Batch Loss = 0.565461, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952279567718506, Accuracy = 0.9168753027915955\n","Iter #10964992:  Learning rate = 0.000003:   Batch Loss = 0.559675, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952575206756592, Accuracy = 0.9180437326431274\n","Iter #10966016:  Learning rate = 0.000003:   Batch Loss = 0.569756, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953017473220825, Accuracy = 0.9180437326431274\n","Iter #10967040:  Learning rate = 0.000003:   Batch Loss = 0.573993, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953171253204346, Accuracy = 0.9180437326431274\n","Iter #10968064:  Learning rate = 0.000003:   Batch Loss = 0.533283, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953131318092346, Accuracy = 0.9178768396377563\n","Iter #10969088:  Learning rate = 0.000003:   Batch Loss = 0.554531, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953118801116943, Accuracy = 0.9180437326431274\n","Iter #10970112:  Learning rate = 0.000003:   Batch Loss = 0.565580, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953340530395508, Accuracy = 0.9175429940223694\n","Iter #10971136:  Learning rate = 0.000003:   Batch Loss = 0.537571, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595341145992279, Accuracy = 0.9177098870277405\n","Iter #10972160:  Learning rate = 0.000003:   Batch Loss = 0.599920, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953550338745117, Accuracy = 0.9177098870277405\n","Iter #10973184:  Learning rate = 0.000003:   Batch Loss = 0.569594, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953359007835388, Accuracy = 0.9180437326431274\n","Iter #10974208:  Learning rate = 0.000003:   Batch Loss = 0.561440, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952954292297363, Accuracy = 0.9178768396377563\n","Iter #10975232:  Learning rate = 0.000003:   Batch Loss = 0.555862, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595241129398346, Accuracy = 0.9177098870277405\n","Iter #10976256:  Learning rate = 0.000003:   Batch Loss = 0.553399, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951997637748718, Accuracy = 0.9168753027915955\n","Iter #10977280:  Learning rate = 0.000003:   Batch Loss = 0.594033, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951737761497498, Accuracy = 0.9173760414123535\n","Iter #10978304:  Learning rate = 0.000003:   Batch Loss = 0.541996, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951947569847107, Accuracy = 0.9175429940223694\n","Iter #10979328:  Learning rate = 0.000003:   Batch Loss = 0.565651, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951778888702393, Accuracy = 0.9172091484069824\n","Iter #10980352:  Learning rate = 0.000003:   Batch Loss = 0.665688, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951827168464661, Accuracy = 0.9172091484069824\n","Iter #10981376:  Learning rate = 0.000003:   Batch Loss = 0.630203, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952025651931763, Accuracy = 0.9178768396377563\n","Iter #10982400:  Learning rate = 0.000003:   Batch Loss = 0.571726, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952141284942627, Accuracy = 0.9178768396377563\n","Iter #10983424:  Learning rate = 0.000003:   Batch Loss = 0.513473, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952275991439819, Accuracy = 0.9177098870277405\n","Iter #10984448:  Learning rate = 0.000003:   Batch Loss = 0.539453, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952236652374268, Accuracy = 0.9170422554016113\n","Iter #10985472:  Learning rate = 0.000003:   Batch Loss = 0.498273, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122211456299, Accuracy = 0.9170422554016113\n","Iter #10986496:  Learning rate = 0.000003:   Batch Loss = 0.591691, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952098369598389, Accuracy = 0.9170422554016113\n","Iter #10987520:  Learning rate = 0.000003:   Batch Loss = 0.585563, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952147245407104, Accuracy = 0.9173760414123535\n","Iter #10988544:  Learning rate = 0.000003:   Batch Loss = 0.612514, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952157974243164, Accuracy = 0.9173760414123535\n","Iter #10989568:  Learning rate = 0.000003:   Batch Loss = 0.514525, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952227711677551, Accuracy = 0.9177098870277405\n","Iter #10990592:  Learning rate = 0.000003:   Batch Loss = 0.569028, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952005386352539, Accuracy = 0.9175429940223694\n","Iter #10991616:  Learning rate = 0.000003:   Batch Loss = 0.625334, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951775312423706, Accuracy = 0.9172091484069824\n","Iter #10992640:  Learning rate = 0.000003:   Batch Loss = 0.522194, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595214307308197, Accuracy = 0.9177098870277405\n","Iter #10993664:  Learning rate = 0.000003:   Batch Loss = 0.581349, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952310562133789, Accuracy = 0.9177098870277405\n","Iter #10994688:  Learning rate = 0.000003:   Batch Loss = 0.528176, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952321290969849, Accuracy = 0.9178768396377563\n","Iter #10995712:  Learning rate = 0.000003:   Batch Loss = 0.516255, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952520966529846, Accuracy = 0.9177098870277405\n","Iter #10996736:  Learning rate = 0.000003:   Batch Loss = 0.546260, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952770709991455, Accuracy = 0.9175429940223694\n","Iter #10997760:  Learning rate = 0.000003:   Batch Loss = 0.565727, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952965617179871, Accuracy = 0.9177098870277405\n","Iter #10998784:  Learning rate = 0.000003:   Batch Loss = 0.571665, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595300555229187, Accuracy = 0.9177098870277405\n","Iter #10999808:  Learning rate = 0.000003:   Batch Loss = 0.503913, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595272421836853, Accuracy = 0.9177098870277405\n","Iter #11000832:  Learning rate = 0.000003:   Batch Loss = 0.526064, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952643752098083, Accuracy = 0.9177098870277405\n","Iter #11001856:  Learning rate = 0.000003:   Batch Loss = 0.553176, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595255970954895, Accuracy = 0.9178768396377563\n","Iter #11002880:  Learning rate = 0.000003:   Batch Loss = 0.573637, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952231884002686, Accuracy = 0.9180437326431274\n","Iter #11003904:  Learning rate = 0.000003:   Batch Loss = 0.572169, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951937437057495, Accuracy = 0.9175429940223694\n","Iter #11004928:  Learning rate = 0.000003:   Batch Loss = 0.562960, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952025651931763, Accuracy = 0.9177098870277405\n","Iter #11005952:  Learning rate = 0.000003:   Batch Loss = 0.560547, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952110290527344, Accuracy = 0.9177098870277405\n","Iter #11006976:  Learning rate = 0.000003:   Batch Loss = 0.570964, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951822996139526, Accuracy = 0.9173760414123535\n","Iter #11008000:  Learning rate = 0.000003:   Batch Loss = 0.548470, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951828956604004, Accuracy = 0.9173760414123535\n","Iter #11009024:  Learning rate = 0.000003:   Batch Loss = 0.550315, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951581001281738, Accuracy = 0.9177098870277405\n","Iter #11010048:  Learning rate = 0.000003:   Batch Loss = 0.549317, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595149040222168, Accuracy = 0.9173760414123535\n","Iter #11011072:  Learning rate = 0.000003:   Batch Loss = 0.551574, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951611399650574, Accuracy = 0.9175429940223694\n","Iter #11012096:  Learning rate = 0.000003:   Batch Loss = 0.581716, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951700210571289, Accuracy = 0.9175429940223694\n","Iter #11013120:  Learning rate = 0.000003:   Batch Loss = 0.645190, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951764583587646, Accuracy = 0.9173760414123535\n","Iter #11014144:  Learning rate = 0.000003:   Batch Loss = 0.520333, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595191478729248, Accuracy = 0.9173760414123535\n","Iter #11015168:  Learning rate = 0.000003:   Batch Loss = 0.572030, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952293872833252, Accuracy = 0.9180437326431274\n","Iter #11016192:  Learning rate = 0.000003:   Batch Loss = 0.546630, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952532887458801, Accuracy = 0.9180437326431274\n","Iter #11017216:  Learning rate = 0.000003:   Batch Loss = 0.494041, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952454805374146, Accuracy = 0.9178768396377563\n","Iter #11018240:  Learning rate = 0.000003:   Batch Loss = 0.531203, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408313751221, Accuracy = 0.9180437326431274\n","Iter #11019264:  Learning rate = 0.000003:   Batch Loss = 0.556759, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952153205871582, Accuracy = 0.9178768396377563\n","Iter #11020288:  Learning rate = 0.000003:   Batch Loss = 0.633581, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952187776565552, Accuracy = 0.9178768396377563\n","Iter #11021312:  Learning rate = 0.000003:   Batch Loss = 0.532592, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595248281955719, Accuracy = 0.9178768396377563\n","Iter #11022336:  Learning rate = 0.000003:   Batch Loss = 0.543417, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952643156051636, Accuracy = 0.9177098870277405\n","Iter #11023360:  Learning rate = 0.000003:   Batch Loss = 0.594420, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595268964767456, Accuracy = 0.9178768396377563\n","Iter #11024384:  Learning rate = 0.000003:   Batch Loss = 0.478537, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952506065368652, Accuracy = 0.9177098870277405\n","Iter #11025408:  Learning rate = 0.000003:   Batch Loss = 0.554860, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952438116073608, Accuracy = 0.9177098870277405\n","Iter #11026432:  Learning rate = 0.000003:   Batch Loss = 0.537451, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952319502830505, Accuracy = 0.9180437326431274\n","Iter #11027456:  Learning rate = 0.000003:   Batch Loss = 0.566533, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952463746070862, Accuracy = 0.9180437326431274\n","Iter #11028480:  Learning rate = 0.000003:   Batch Loss = 0.612233, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952649116516113, Accuracy = 0.9180437326431274\n","Iter #11029504:  Learning rate = 0.000003:   Batch Loss = 0.672349, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952451229095459, Accuracy = 0.9178768396377563\n","Iter #11030528:  Learning rate = 0.000003:   Batch Loss = 0.562900, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951998233795166, Accuracy = 0.9175429940223694\n","Iter #11031552:  Learning rate = 0.000003:   Batch Loss = 0.545378, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951858758926392, Accuracy = 0.9172091484069824\n","Iter #11032576:  Learning rate = 0.000003:   Batch Loss = 0.558272, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952034592628479, Accuracy = 0.9175429940223694\n","Iter #11033600:  Learning rate = 0.000003:   Batch Loss = 0.582550, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952039957046509, Accuracy = 0.9175429940223694\n","Iter #11034624:  Learning rate = 0.000003:   Batch Loss = 0.511399, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952112674713135, Accuracy = 0.9177098870277405\n","Iter #11035648:  Learning rate = 0.000003:   Batch Loss = 0.595580, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595206618309021, Accuracy = 0.9177098870277405\n","Iter #11036672:  Learning rate = 0.000003:   Batch Loss = 0.517272, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951921343803406, Accuracy = 0.9175429940223694\n","Iter #11037696:  Learning rate = 0.000003:   Batch Loss = 0.555529, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951927900314331, Accuracy = 0.9177098870277405\n","Iter #11038720:  Learning rate = 0.000003:   Batch Loss = 0.551360, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595220685005188, Accuracy = 0.9177098870277405\n","Iter #11039744:  Learning rate = 0.000003:   Batch Loss = 0.527771, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595240592956543, Accuracy = 0.9178768396377563\n","Iter #11040768:  Learning rate = 0.000003:   Batch Loss = 0.525979, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952507853507996, Accuracy = 0.9180437326431274\n","Iter #11041792:  Learning rate = 0.000003:   Batch Loss = 0.564386, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952274799346924, Accuracy = 0.9177098870277405\n","Iter #11042816:  Learning rate = 0.000003:   Batch Loss = 0.641280, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951843857765198, Accuracy = 0.9173760414123535\n","Iter #11043840:  Learning rate = 0.000003:   Batch Loss = 0.573460, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951834321022034, Accuracy = 0.9172091484069824\n","Iter #11044864:  Learning rate = 0.000003:   Batch Loss = 0.610010, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952134728431702, Accuracy = 0.9173760414123535\n","Iter #11045888:  Learning rate = 0.000003:   Batch Loss = 0.496177, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595226526260376, Accuracy = 0.9173760414123535\n","Iter #11046912:  Learning rate = 0.000003:   Batch Loss = 0.571878, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952206254005432, Accuracy = 0.9168753027915955\n","Iter #11047936:  Learning rate = 0.000003:   Batch Loss = 0.528201, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952028632164001, Accuracy = 0.9168753027915955\n","Iter #11048960:  Learning rate = 0.000003:   Batch Loss = 0.505205, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951778888702393, Accuracy = 0.9165414571762085\n","Iter #11049984:  Learning rate = 0.000003:   Batch Loss = 0.574051, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951753854751587, Accuracy = 0.9172091484069824\n","Iter #11051008:  Learning rate = 0.000003:   Batch Loss = 0.586810, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595178484916687, Accuracy = 0.9172091484069824\n","Iter #11052032:  Learning rate = 0.000003:   Batch Loss = 0.525220, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952177047729492, Accuracy = 0.9178768396377563\n","Iter #11053056:  Learning rate = 0.000003:   Batch Loss = 0.536644, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952315330505371, Accuracy = 0.9175429940223694\n","Iter #11054080:  Learning rate = 0.000003:   Batch Loss = 0.664262, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952425003051758, Accuracy = 0.9177098870277405\n","Iter #11055104:  Learning rate = 0.000003:   Batch Loss = 0.544203, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952540636062622, Accuracy = 0.9178768396377563\n","Iter #11056128:  Learning rate = 0.000003:   Batch Loss = 0.567111, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952744483947754, Accuracy = 0.9180437326431274\n","Iter #11057152:  Learning rate = 0.000003:   Batch Loss = 0.532503, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952922105789185, Accuracy = 0.9182106256484985\n","Iter #11058176:  Learning rate = 0.000003:   Batch Loss = 0.549699, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953166484832764, Accuracy = 0.9182106256484985\n","Iter #11059200:  Learning rate = 0.000003:   Batch Loss = 0.517295, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953333377838135, Accuracy = 0.9180437326431274\n","Iter #11060224:  Learning rate = 0.000003:   Batch Loss = 0.655583, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953271389007568, Accuracy = 0.9182106256484985\n","Iter #11061248:  Learning rate = 0.000003:   Batch Loss = 0.621142, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595321774482727, Accuracy = 0.9180437326431274\n","Iter #11062272:  Learning rate = 0.000003:   Batch Loss = 0.568456, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952832102775574, Accuracy = 0.9178768396377563\n","Iter #11063296:  Learning rate = 0.000003:   Batch Loss = 0.497397, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952503681182861, Accuracy = 0.9175429940223694\n","Iter #11064320:  Learning rate = 0.000003:   Batch Loss = 0.570787, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952361822128296, Accuracy = 0.9178768396377563\n","Iter #11065344:  Learning rate = 0.000003:   Batch Loss = 0.529796, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595231294631958, Accuracy = 0.9177098870277405\n","Iter #11066368:  Learning rate = 0.000003:   Batch Loss = 0.552669, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952051281929016, Accuracy = 0.9178768396377563\n","Iter #11067392:  Learning rate = 0.000003:   Batch Loss = 0.590262, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595169186592102, Accuracy = 0.9172091484069824\n","Iter #11068416:  Learning rate = 0.000003:   Batch Loss = 0.544960, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951735377311707, Accuracy = 0.9177098870277405\n","Iter #11069440:  Learning rate = 0.000003:   Batch Loss = 0.576823, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951716899871826, Accuracy = 0.9177098870277405\n","Iter #11070464:  Learning rate = 0.000003:   Batch Loss = 0.586087, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951910018920898, Accuracy = 0.9178768396377563\n","Iter #11071488:  Learning rate = 0.000003:   Batch Loss = 0.510871, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952231884002686, Accuracy = 0.9178768396377563\n","Iter #11072512:  Learning rate = 0.000003:   Batch Loss = 0.544038, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952805876731873, Accuracy = 0.9180437326431274\n","Iter #11073536:  Learning rate = 0.000003:   Batch Loss = 0.636038, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952756404876709, Accuracy = 0.9178768396377563\n","Iter #11074560:  Learning rate = 0.000003:   Batch Loss = 0.578322, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952867269515991, Accuracy = 0.9178768396377563\n","Iter #11075584:  Learning rate = 0.000003:   Batch Loss = 0.503589, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952746868133545, Accuracy = 0.9178768396377563\n","Iter #11076608:  Learning rate = 0.000003:   Batch Loss = 0.612706, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952235460281372, Accuracy = 0.9178768396377563\n","Iter #11077632:  Learning rate = 0.000003:   Batch Loss = 0.596672, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595201849937439, Accuracy = 0.9175429940223694\n","Iter #11078656:  Learning rate = 0.000003:   Batch Loss = 0.475852, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951975584030151, Accuracy = 0.9175429940223694\n","Iter #11079680:  Learning rate = 0.000003:   Batch Loss = 0.557945, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952005386352539, Accuracy = 0.9172091484069824\n","Iter #11080704:  Learning rate = 0.000003:   Batch Loss = 0.562326, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952062606811523, Accuracy = 0.9173760414123535\n","Iter #11081728:  Learning rate = 0.000003:   Batch Loss = 0.605114, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952005386352539, Accuracy = 0.9177098870277405\n","Iter #11082752:  Learning rate = 0.000003:   Batch Loss = 0.516272, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951814651489258, Accuracy = 0.9177098870277405\n","Iter #11083776:  Learning rate = 0.000003:   Batch Loss = 0.607297, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951814651489258, Accuracy = 0.9177098870277405\n","Iter #11084800:  Learning rate = 0.000003:   Batch Loss = 0.530420, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951880216598511, Accuracy = 0.9175429940223694\n","Iter #11085824:  Learning rate = 0.000003:   Batch Loss = 0.547671, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952023267745972, Accuracy = 0.9178768396377563\n","Iter #11086848:  Learning rate = 0.000003:   Batch Loss = 0.528065, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952274799346924, Accuracy = 0.9175429940223694\n","Iter #11087872:  Learning rate = 0.000003:   Batch Loss = 0.499462, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952037572860718, Accuracy = 0.9177098870277405\n","Iter #11088896:  Learning rate = 0.000003:   Batch Loss = 0.560964, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951894521713257, Accuracy = 0.9175429940223694\n","Iter #11089920:  Learning rate = 0.000003:   Batch Loss = 0.582700, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951938629150391, Accuracy = 0.9177098870277405\n","Iter #11090944:  Learning rate = 0.000003:   Batch Loss = 0.594899, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952365398406982, Accuracy = 0.9178768396377563\n","Iter #11091968:  Learning rate = 0.000003:   Batch Loss = 0.596706, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952481031417847, Accuracy = 0.9180437326431274\n","Iter #11092992:  Learning rate = 0.000003:   Batch Loss = 0.565867, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952266454696655, Accuracy = 0.9180437326431274\n","Iter #11094016:  Learning rate = 0.000003:   Batch Loss = 0.519251, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595215380191803, Accuracy = 0.9180437326431274\n","Iter #11095040:  Learning rate = 0.000003:   Batch Loss = 0.524106, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595220685005188, Accuracy = 0.9180437326431274\n","Iter #11096064:  Learning rate = 0.000003:   Batch Loss = 0.616976, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952619910240173, Accuracy = 0.9178768396377563\n","Iter #11097088:  Learning rate = 0.000003:   Batch Loss = 0.528358, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953205823898315, Accuracy = 0.9180437326431274\n","Iter #11098112:  Learning rate = 0.000003:   Batch Loss = 0.490681, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953272581100464, Accuracy = 0.9178768396377563\n","Iter #11099136:  Learning rate = 0.000003:   Batch Loss = 0.567575, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59529709815979, Accuracy = 0.9178768396377563\n","Iter #11100160:  Learning rate = 0.000003:   Batch Loss = 0.540561, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953016877174377, Accuracy = 0.9178768396377563\n","Iter #11101184:  Learning rate = 0.000003:   Batch Loss = 0.593793, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952829122543335, Accuracy = 0.9180437326431274\n","Iter #11102208:  Learning rate = 0.000003:   Batch Loss = 0.507094, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595217227935791, Accuracy = 0.9178768396377563\n","Iter #11103232:  Learning rate = 0.000003:   Batch Loss = 0.507265, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951933860778809, Accuracy = 0.9170422554016113\n","Iter #11104256:  Learning rate = 0.000003:   Batch Loss = 0.640260, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951786041259766, Accuracy = 0.9175429940223694\n","Iter #11105280:  Learning rate = 0.000003:   Batch Loss = 0.531210, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951969623565674, Accuracy = 0.9175429940223694\n","Iter #11106304:  Learning rate = 0.000003:   Batch Loss = 0.585595, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951806306838989, Accuracy = 0.9173760414123535\n","Iter #11107328:  Learning rate = 0.000003:   Batch Loss = 0.513333, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951385498046875, Accuracy = 0.9172091484069824\n","Iter #11108352:  Learning rate = 0.000003:   Batch Loss = 0.515310, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951287746429443, Accuracy = 0.9168753027915955\n","Iter #11109376:  Learning rate = 0.000003:   Batch Loss = 0.575801, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951189398765564, Accuracy = 0.9170422554016113\n","Iter #11110400:  Learning rate = 0.000003:   Batch Loss = 0.566122, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951348543167114, Accuracy = 0.9170422554016113\n","Iter #11111424:  Learning rate = 0.000003:   Batch Loss = 0.527912, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951783657073975, Accuracy = 0.9173760414123535\n","Iter #11112448:  Learning rate = 0.000003:   Batch Loss = 0.569718, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952070951461792, Accuracy = 0.9175429940223694\n","Iter #11113472:  Learning rate = 0.000003:   Batch Loss = 0.563961, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595209002494812, Accuracy = 0.9175429940223694\n","Iter #11114496:  Learning rate = 0.000003:   Batch Loss = 0.543541, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952261686325073, Accuracy = 0.9177098870277405\n","Iter #11115520:  Learning rate = 0.000003:   Batch Loss = 0.500470, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952301025390625, Accuracy = 0.9177098870277405\n","Iter #11116544:  Learning rate = 0.000003:   Batch Loss = 0.632497, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029228210449, Accuracy = 0.9175429940223694\n","Iter #11117568:  Learning rate = 0.000003:   Batch Loss = 0.655644, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952162742614746, Accuracy = 0.9177098870277405\n","Iter #11118592:  Learning rate = 0.000003:   Batch Loss = 0.532326, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952115058898926, Accuracy = 0.9178768396377563\n","Iter #11119616:  Learning rate = 0.000003:   Batch Loss = 0.588184, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951974391937256, Accuracy = 0.9178768396377563\n","Iter #11120640:  Learning rate = 0.000003:   Batch Loss = 0.567306, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048301696777, Accuracy = 0.9177098870277405\n","Iter #11121664:  Learning rate = 0.000003:   Batch Loss = 0.605209, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952228307723999, Accuracy = 0.9177098870277405\n","Iter #11122688:  Learning rate = 0.000003:   Batch Loss = 0.510329, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952091217041016, Accuracy = 0.9177098870277405\n","Iter #11123712:  Learning rate = 0.000003:   Batch Loss = 0.619006, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952145457267761, Accuracy = 0.9178768396377563\n","Iter #11124736:  Learning rate = 0.000003:   Batch Loss = 0.533551, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951992273330688, Accuracy = 0.9177098870277405\n","Iter #11125760:  Learning rate = 0.000003:   Batch Loss = 0.573046, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952111482620239, Accuracy = 0.9178768396377563\n","Iter #11126784:  Learning rate = 0.000003:   Batch Loss = 0.564436, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952185392379761, Accuracy = 0.9178768396377563\n","Iter #11127808:  Learning rate = 0.000003:   Batch Loss = 0.640150, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952391624450684, Accuracy = 0.9178768396377563\n","Iter #11128832:  Learning rate = 0.000003:   Batch Loss = 0.536430, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952297449111938, Accuracy = 0.9178768396377563\n","Iter #11129856:  Learning rate = 0.000003:   Batch Loss = 0.522241, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952339768409729, Accuracy = 0.9178768396377563\n","Iter #11130880:  Learning rate = 0.000003:   Batch Loss = 0.564460, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952388048171997, Accuracy = 0.9177098870277405\n","Iter #11131904:  Learning rate = 0.000003:   Batch Loss = 0.531913, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952492952346802, Accuracy = 0.9178768396377563\n","Iter #11132928:  Learning rate = 0.000003:   Batch Loss = 0.545710, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122807502747, Accuracy = 0.9175429940223694\n","Iter #11133952:  Learning rate = 0.000003:   Batch Loss = 0.536823, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595200777053833, Accuracy = 0.9178768396377563\n","Iter #11134976:  Learning rate = 0.000003:   Batch Loss = 0.519957, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951880216598511, Accuracy = 0.9175429940223694\n","Iter #11136000:  Learning rate = 0.000003:   Batch Loss = 0.560167, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951679944992065, Accuracy = 0.9170422554016113\n","Iter #11137024:  Learning rate = 0.000003:   Batch Loss = 0.559465, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951703190803528, Accuracy = 0.9170422554016113\n","Iter #11138048:  Learning rate = 0.000003:   Batch Loss = 0.503292, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951905250549316, Accuracy = 0.9170422554016113\n","Iter #11139072:  Learning rate = 0.000003:   Batch Loss = 0.592330, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952109694480896, Accuracy = 0.9173760414123535\n","Iter #11140096:  Learning rate = 0.000003:   Batch Loss = 0.561281, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952268838882446, Accuracy = 0.9178768396377563\n","Iter #11141120:  Learning rate = 0.000003:   Batch Loss = 0.591401, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952335596084595, Accuracy = 0.9175429940223694\n","Iter #11142144:  Learning rate = 0.000003:   Batch Loss = 0.610643, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952486991882324, Accuracy = 0.9178768396377563\n","Iter #11143168:  Learning rate = 0.000003:   Batch Loss = 0.584506, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952397584915161, Accuracy = 0.9175429940223694\n","Iter #11144192:  Learning rate = 0.000003:   Batch Loss = 0.521377, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952309370040894, Accuracy = 0.9177098870277405\n","Iter #11145216:  Learning rate = 0.000003:   Batch Loss = 0.530821, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952426195144653, Accuracy = 0.9175429940223694\n","Iter #11146240:  Learning rate = 0.000003:   Batch Loss = 0.560974, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952410101890564, Accuracy = 0.9175429940223694\n","Iter #11147264:  Learning rate = 0.000003:   Batch Loss = 0.540040, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952181816101074, Accuracy = 0.9177098870277405\n","Iter #11148288:  Learning rate = 0.000003:   Batch Loss = 0.591648, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951902270317078, Accuracy = 0.9172091484069824\n","Iter #11149312:  Learning rate = 0.000003:   Batch Loss = 0.500541, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595171332359314, Accuracy = 0.9170422554016113\n","Iter #11150336:  Learning rate = 0.000003:   Batch Loss = 0.591132, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951901078224182, Accuracy = 0.9175429940223694\n","Iter #11151360:  Learning rate = 0.000003:   Batch Loss = 0.550723, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952012538909912, Accuracy = 0.9175429940223694\n","Iter #11152384:  Learning rate = 0.000003:   Batch Loss = 0.553807, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951837301254272, Accuracy = 0.9175429940223694\n","Iter #11153408:  Learning rate = 0.000003:   Batch Loss = 0.537494, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951562523841858, Accuracy = 0.9173760414123535\n","Iter #11154432:  Learning rate = 0.000003:   Batch Loss = 0.641413, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951535701751709, Accuracy = 0.9173760414123535\n","Iter #11155456:  Learning rate = 0.000003:   Batch Loss = 0.660352, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951662063598633, Accuracy = 0.9170422554016113\n","Iter #11156480:  Learning rate = 0.000003:   Batch Loss = 0.580760, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951836705207825, Accuracy = 0.9172091484069824\n","Iter #11157504:  Learning rate = 0.000003:   Batch Loss = 0.567450, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952006578445435, Accuracy = 0.9172091484069824\n","Iter #11158528:  Learning rate = 0.000003:   Batch Loss = 0.564058, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952013731002808, Accuracy = 0.9170422554016113\n","Iter #11159552:  Learning rate = 0.000003:   Batch Loss = 0.591526, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952140092849731, Accuracy = 0.9175429940223694\n","Iter #11160576:  Learning rate = 0.000003:   Batch Loss = 0.548804, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952436327934265, Accuracy = 0.9180437326431274\n","Iter #11161600:  Learning rate = 0.000003:   Batch Loss = 0.637743, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952600836753845, Accuracy = 0.9180437326431274\n","Iter #11162624:  Learning rate = 0.000003:   Batch Loss = 0.567264, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952705144882202, Accuracy = 0.9180437326431274\n","Iter #11163648:  Learning rate = 0.000003:   Batch Loss = 0.616297, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595254123210907, Accuracy = 0.9182106256484985\n","Iter #11164672:  Learning rate = 0.000003:   Batch Loss = 0.559790, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952587127685547, Accuracy = 0.9180437326431274\n","Iter #11165696:  Learning rate = 0.000003:   Batch Loss = 0.572264, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952727794647217, Accuracy = 0.9180437326431274\n","Iter #11166720:  Learning rate = 0.000003:   Batch Loss = 0.551008, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952568054199219, Accuracy = 0.9180437326431274\n","Iter #11167744:  Learning rate = 0.000003:   Batch Loss = 0.523469, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952228903770447, Accuracy = 0.9178768396377563\n","Iter #11168768:  Learning rate = 0.000003:   Batch Loss = 0.501167, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951747298240662, Accuracy = 0.9170422554016113\n","Iter #11169792:  Learning rate = 0.000003:   Batch Loss = 0.562177, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951426029205322, Accuracy = 0.9172091484069824\n","Iter #11170816:  Learning rate = 0.000003:   Batch Loss = 0.541037, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951334834098816, Accuracy = 0.9172091484069824\n","Iter #11171840:  Learning rate = 0.000003:   Batch Loss = 0.586976, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595147967338562, Accuracy = 0.9173760414123535\n","Iter #11172864:  Learning rate = 0.000003:   Batch Loss = 0.661926, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951657295227051, Accuracy = 0.9173760414123535\n","Iter #11173888:  Learning rate = 0.000003:   Batch Loss = 0.599539, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595201849937439, Accuracy = 0.9178768396377563\n","Iter #11174912:  Learning rate = 0.000003:   Batch Loss = 0.549417, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595257580280304, Accuracy = 0.9180437326431274\n","Iter #11175936:  Learning rate = 0.000003:   Batch Loss = 0.612738, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595277726650238, Accuracy = 0.9180437326431274\n","Iter #11176960:  Learning rate = 0.000003:   Batch Loss = 0.586880, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595325231552124, Accuracy = 0.9180437326431274\n","Iter #11177984:  Learning rate = 0.000003:   Batch Loss = 0.638451, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953311324119568, Accuracy = 0.9180437326431274\n","Iter #11179008:  Learning rate = 0.000003:   Batch Loss = 0.578278, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952967405319214, Accuracy = 0.9180437326431274\n","Iter #11180032:  Learning rate = 0.000003:   Batch Loss = 0.543072, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952552556991577, Accuracy = 0.9182106256484985\n","Iter #11181056:  Learning rate = 0.000003:   Batch Loss = 0.524372, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595216691493988, Accuracy = 0.9180437326431274\n","Iter #11182080:  Learning rate = 0.000003:   Batch Loss = 0.519831, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952135920524597, Accuracy = 0.9180437326431274\n","Iter #11183104:  Learning rate = 0.000003:   Batch Loss = 0.549018, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595221757888794, Accuracy = 0.9175429940223694\n","Iter #11184128:  Learning rate = 0.000003:   Batch Loss = 0.554842, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595217227935791, Accuracy = 0.9175429940223694\n","Iter #11185152:  Learning rate = 0.000003:   Batch Loss = 0.568765, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952095985412598, Accuracy = 0.9173760414123535\n","Iter #11186176:  Learning rate = 0.000003:   Batch Loss = 0.614689, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952015519142151, Accuracy = 0.9177098870277405\n","Iter #11187200:  Learning rate = 0.000003:   Batch Loss = 0.647441, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073931694031, Accuracy = 0.9178768396377563\n","Iter #11188224:  Learning rate = 0.000003:   Batch Loss = 0.604821, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952755808830261, Accuracy = 0.9180437326431274\n","Iter #11189248:  Learning rate = 0.000003:   Batch Loss = 0.509992, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952919125556946, Accuracy = 0.9180437326431274\n","Iter #11190272:  Learning rate = 0.000003:   Batch Loss = 0.651398, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952482223510742, Accuracy = 0.9178768396377563\n","Iter #11191296:  Learning rate = 0.000003:   Batch Loss = 0.530017, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952186584472656, Accuracy = 0.9175429940223694\n","Iter #11192320:  Learning rate = 0.000003:   Batch Loss = 0.562600, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952249765396118, Accuracy = 0.9175429940223694\n","Iter #11193344:  Learning rate = 0.000003:   Batch Loss = 0.504827, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952133536338806, Accuracy = 0.9173760414123535\n","Iter #11194368:  Learning rate = 0.000003:   Batch Loss = 0.537660, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595199465751648, Accuracy = 0.9173760414123535\n","Iter #11195392:  Learning rate = 0.000003:   Batch Loss = 0.558302, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951647758483887, Accuracy = 0.9172091484069824\n","Iter #11196416:  Learning rate = 0.000003:   Batch Loss = 0.609768, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951516628265381, Accuracy = 0.9170422554016113\n","Iter #11197440:  Learning rate = 0.000003:   Batch Loss = 0.564484, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951552391052246, Accuracy = 0.9172091484069824\n","Iter #11198464:  Learning rate = 0.000003:   Batch Loss = 0.535119, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951985120773315, Accuracy = 0.9178768396377563\n","Iter #11199488:  Learning rate = 0.000003:   Batch Loss = 0.617510, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952375531196594, Accuracy = 0.9178768396377563\n","Iter #11200512:  Learning rate = 0.000003:   Batch Loss = 0.508213, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595274806022644, Accuracy = 0.9178768396377563\n","Iter #11201536:  Learning rate = 0.000003:   Batch Loss = 0.575496, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952787399291992, Accuracy = 0.9180437326431274\n","Iter #11202560:  Learning rate = 0.000003:   Batch Loss = 0.539266, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953047275543213, Accuracy = 0.9180437326431274\n","Iter #11203584:  Learning rate = 0.000003:   Batch Loss = 0.615540, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952937006950378, Accuracy = 0.9178768396377563\n","Iter #11204608:  Learning rate = 0.000003:   Batch Loss = 0.586172, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952585935592651, Accuracy = 0.9180437326431274\n","Iter #11205632:  Learning rate = 0.000003:   Batch Loss = 0.552700, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952169895172119, Accuracy = 0.9177098870277405\n","Iter #11206656:  Learning rate = 0.000003:   Batch Loss = 0.522000, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952141284942627, Accuracy = 0.9177098870277405\n","Iter #11207680:  Learning rate = 0.000003:   Batch Loss = 0.566783, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952249765396118, Accuracy = 0.9178768396377563\n","Iter #11208704:  Learning rate = 0.000003:   Batch Loss = 0.500098, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952343344688416, Accuracy = 0.9178768396377563\n","Iter #11209728:  Learning rate = 0.000003:   Batch Loss = 0.618898, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952223539352417, Accuracy = 0.9180437326431274\n","Iter #11210752:  Learning rate = 0.000003:   Batch Loss = 0.571303, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952128171920776, Accuracy = 0.9177098870277405\n","Iter #11211776:  Learning rate = 0.000003:   Batch Loss = 0.556113, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952222347259521, Accuracy = 0.9178768396377563\n","Iter #11212800:  Learning rate = 0.000003:   Batch Loss = 0.493038, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952166318893433, Accuracy = 0.9177098870277405\n","Iter #11213824:  Learning rate = 0.000003:   Batch Loss = 0.579286, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952115058898926, Accuracy = 0.9177098870277405\n","Iter #11214848:  Learning rate = 0.000003:   Batch Loss = 0.558251, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951820611953735, Accuracy = 0.9173760414123535\n","Iter #11215872:  Learning rate = 0.000003:   Batch Loss = 0.528696, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951341390609741, Accuracy = 0.9170422554016113\n","Iter #11216896:  Learning rate = 0.000003:   Batch Loss = 0.572098, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59510338306427, Accuracy = 0.9172091484069824\n","Iter #11217920:  Learning rate = 0.000003:   Batch Loss = 0.593553, Accuracy = 0.8671875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951196551322937, Accuracy = 0.9172091484069824\n","Iter #11218944:  Learning rate = 0.000003:   Batch Loss = 0.474064, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951621532440186, Accuracy = 0.9175429940223694\n","Iter #11219968:  Learning rate = 0.000003:   Batch Loss = 0.586948, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952291488647461, Accuracy = 0.9177098870277405\n","Iter #11220992:  Learning rate = 0.000002:   Batch Loss = 0.599861, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952792167663574, Accuracy = 0.9175429940223694\n","Iter #11222016:  Learning rate = 0.000002:   Batch Loss = 0.524143, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59527188539505, Accuracy = 0.9177098870277405\n","Iter #11223040:  Learning rate = 0.000002:   Batch Loss = 0.552704, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952588319778442, Accuracy = 0.9178768396377563\n","Iter #11224064:  Learning rate = 0.000002:   Batch Loss = 0.553863, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952637195587158, Accuracy = 0.9180437326431274\n","Iter #11225088:  Learning rate = 0.000002:   Batch Loss = 0.587812, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952709913253784, Accuracy = 0.9178768396377563\n","Iter #11226112:  Learning rate = 0.000002:   Batch Loss = 0.547385, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952951908111572, Accuracy = 0.9177098870277405\n","Iter #11227136:  Learning rate = 0.000002:   Batch Loss = 0.518060, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952766537666321, Accuracy = 0.9178768396377563\n","Iter #11228160:  Learning rate = 0.000002:   Batch Loss = 0.524990, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952813625335693, Accuracy = 0.9177098870277405\n","Iter #11229184:  Learning rate = 0.000002:   Batch Loss = 0.571896, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952891111373901, Accuracy = 0.9178768396377563\n","Iter #11230208:  Learning rate = 0.000002:   Batch Loss = 0.536606, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595261812210083, Accuracy = 0.9177098870277405\n","Iter #11231232:  Learning rate = 0.000002:   Batch Loss = 0.572985, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952397584915161, Accuracy = 0.9178768396377563\n","Iter #11232256:  Learning rate = 0.000002:   Batch Loss = 0.514527, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952426195144653, Accuracy = 0.9180437326431274\n","Iter #11233280:  Learning rate = 0.000002:   Batch Loss = 0.492185, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952531695365906, Accuracy = 0.9180437326431274\n","Iter #11234304:  Learning rate = 0.000002:   Batch Loss = 0.531065, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408909797668, Accuracy = 0.9182106256484985\n","Iter #11235328:  Learning rate = 0.000002:   Batch Loss = 0.556053, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952279567718506, Accuracy = 0.9182106256484985\n","Iter #11236352:  Learning rate = 0.000002:   Batch Loss = 0.516760, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952236652374268, Accuracy = 0.9180437326431274\n","Iter #11237376:  Learning rate = 0.000002:   Batch Loss = 0.622136, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595209538936615, Accuracy = 0.9180437326431274\n","Iter #11238400:  Learning rate = 0.000002:   Batch Loss = 0.595275, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951991081237793, Accuracy = 0.9178768396377563\n","Iter #11239424:  Learning rate = 0.000002:   Batch Loss = 0.525597, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952215194702148, Accuracy = 0.9178768396377563\n","Iter #11240448:  Learning rate = 0.000002:   Batch Loss = 0.480693, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952413082122803, Accuracy = 0.9178768396377563\n","Iter #11241472:  Learning rate = 0.000002:   Batch Loss = 0.482634, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595250129699707, Accuracy = 0.9180437326431274\n","Iter #11242496:  Learning rate = 0.000002:   Batch Loss = 0.572988, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952574014663696, Accuracy = 0.9182106256484985\n","Iter #11243520:  Learning rate = 0.000002:   Batch Loss = 0.519240, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952514410018921, Accuracy = 0.9178768396377563\n","Iter #11244544:  Learning rate = 0.000002:   Batch Loss = 0.550565, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952275991439819, Accuracy = 0.9178768396377563\n","Iter #11245568:  Learning rate = 0.000002:   Batch Loss = 0.511827, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59519362449646, Accuracy = 0.9177098870277405\n","Iter #11246592:  Learning rate = 0.000002:   Batch Loss = 0.537498, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951709747314453, Accuracy = 0.9177098870277405\n","Iter #11247616:  Learning rate = 0.000002:   Batch Loss = 0.507289, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952023267745972, Accuracy = 0.9178768396377563\n","Iter #11248640:  Learning rate = 0.000002:   Batch Loss = 0.647492, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952078104019165, Accuracy = 0.9177098870277405\n","Iter #11249664:  Learning rate = 0.000002:   Batch Loss = 0.486188, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951930284500122, Accuracy = 0.9173760414123535\n","Iter #11250688:  Learning rate = 0.000002:   Batch Loss = 0.543546, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951732397079468, Accuracy = 0.9172091484069824\n","Iter #11251712:  Learning rate = 0.000002:   Batch Loss = 0.628196, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595172107219696, Accuracy = 0.9173760414123535\n","Iter #11252736:  Learning rate = 0.000002:   Batch Loss = 0.584863, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951803922653198, Accuracy = 0.9177098870277405\n","Iter #11253760:  Learning rate = 0.000002:   Batch Loss = 0.530258, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952106714248657, Accuracy = 0.9178768396377563\n","Iter #11254784:  Learning rate = 0.000002:   Batch Loss = 0.537089, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952341556549072, Accuracy = 0.9177098870277405\n","Iter #11255808:  Learning rate = 0.000002:   Batch Loss = 0.574039, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952727794647217, Accuracy = 0.9177098870277405\n","Iter #11256832:  Learning rate = 0.000002:   Batch Loss = 0.605792, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953042507171631, Accuracy = 0.9175429940223694\n","Iter #11257856:  Learning rate = 0.000002:   Batch Loss = 0.503871, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952944755554199, Accuracy = 0.9177098870277405\n","Iter #11258880:  Learning rate = 0.000002:   Batch Loss = 0.546581, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952714681625366, Accuracy = 0.9178768396377563\n","Iter #11259904:  Learning rate = 0.000002:   Batch Loss = 0.587016, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408313751221, Accuracy = 0.9178768396377563\n","Iter #11260928:  Learning rate = 0.000002:   Batch Loss = 0.556338, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952333211898804, Accuracy = 0.9178768396377563\n","Iter #11261952:  Learning rate = 0.000002:   Batch Loss = 0.524819, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595229983329773, Accuracy = 0.9178768396377563\n","Iter #11262976:  Learning rate = 0.000002:   Batch Loss = 0.515538, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952274799346924, Accuracy = 0.9180437326431274\n","Iter #11264000:  Learning rate = 0.000002:   Batch Loss = 0.613075, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951991677284241, Accuracy = 0.9177098870277405\n","Iter #11265024:  Learning rate = 0.000002:   Batch Loss = 0.533263, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595196008682251, Accuracy = 0.9175429940223694\n","Iter #11266048:  Learning rate = 0.000002:   Batch Loss = 0.601702, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952003002166748, Accuracy = 0.9175429940223694\n","Iter #11267072:  Learning rate = 0.000002:   Batch Loss = 0.612505, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952373147010803, Accuracy = 0.9175429940223694\n","Iter #11268096:  Learning rate = 0.000002:   Batch Loss = 0.601921, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952495336532593, Accuracy = 0.9177098870277405\n","Iter #11269120:  Learning rate = 0.000002:   Batch Loss = 0.563150, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952385663986206, Accuracy = 0.9177098870277405\n","Iter #11270144:  Learning rate = 0.000002:   Batch Loss = 0.482202, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073335647583, Accuracy = 0.9177098870277405\n","Iter #11271168:  Learning rate = 0.000002:   Batch Loss = 0.573960, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951797962188721, Accuracy = 0.9172091484069824\n","Iter #11272192:  Learning rate = 0.000002:   Batch Loss = 0.572297, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951873064041138, Accuracy = 0.9175429940223694\n","Iter #11273216:  Learning rate = 0.000002:   Batch Loss = 0.534077, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952247381210327, Accuracy = 0.9175429940223694\n","Iter #11274240:  Learning rate = 0.000002:   Batch Loss = 0.567275, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952461957931519, Accuracy = 0.9175429940223694\n","Iter #11275264:  Learning rate = 0.000002:   Batch Loss = 0.502221, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952445268630981, Accuracy = 0.9172091484069824\n","Iter #11276288:  Learning rate = 0.000002:   Batch Loss = 0.534623, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952489376068115, Accuracy = 0.9172091484069824\n","Iter #11277312:  Learning rate = 0.000002:   Batch Loss = 0.513220, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952553749084473, Accuracy = 0.9170422554016113\n","Iter #11278336:  Learning rate = 0.000002:   Batch Loss = 0.551311, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952597260475159, Accuracy = 0.9172091484069824\n","Iter #11279360:  Learning rate = 0.000002:   Batch Loss = 0.500272, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952680110931396, Accuracy = 0.9173760414123535\n","Iter #11280384:  Learning rate = 0.000002:   Batch Loss = 0.498742, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952503681182861, Accuracy = 0.9175429940223694\n","Iter #11281408:  Learning rate = 0.000002:   Batch Loss = 0.509056, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952399969100952, Accuracy = 0.9178768396377563\n","Iter #11282432:  Learning rate = 0.000002:   Batch Loss = 0.610112, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951994061470032, Accuracy = 0.9175429940223694\n","Iter #11283456:  Learning rate = 0.000002:   Batch Loss = 0.574087, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951811075210571, Accuracy = 0.9175429940223694\n","Iter #11284480:  Learning rate = 0.000002:   Batch Loss = 0.663489, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595180869102478, Accuracy = 0.9170422554016113\n","Iter #11285504:  Learning rate = 0.000002:   Batch Loss = 0.505371, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951904058456421, Accuracy = 0.9177098870277405\n","Iter #11286528:  Learning rate = 0.000002:   Batch Loss = 0.591935, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952153205871582, Accuracy = 0.9177098870277405\n","Iter #11287552:  Learning rate = 0.000002:   Batch Loss = 0.537914, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595234751701355, Accuracy = 0.9180437326431274\n","Iter #11288576:  Learning rate = 0.000002:   Batch Loss = 0.560096, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952661037445068, Accuracy = 0.9182106256484985\n","Iter #11289600:  Learning rate = 0.000002:   Batch Loss = 0.534870, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952382683753967, Accuracy = 0.9182106256484985\n","Iter #11290624:  Learning rate = 0.000002:   Batch Loss = 0.460677, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122807502747, Accuracy = 0.9182106256484985\n","Iter #11291648:  Learning rate = 0.000002:   Batch Loss = 0.609658, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952087044715881, Accuracy = 0.9178768396377563\n","Iter #11292672:  Learning rate = 0.000002:   Batch Loss = 0.545436, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951967239379883, Accuracy = 0.9175429940223694\n","Iter #11293696:  Learning rate = 0.000002:   Batch Loss = 0.544260, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951880812644958, Accuracy = 0.9175429940223694\n","Iter #11294720:  Learning rate = 0.000002:   Batch Loss = 0.567502, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951772332191467, Accuracy = 0.9173760414123535\n","Iter #11295744:  Learning rate = 0.000002:   Batch Loss = 0.592909, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951817035675049, Accuracy = 0.9172091484069824\n","Iter #11296768:  Learning rate = 0.000002:   Batch Loss = 0.537256, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951980352401733, Accuracy = 0.9173760414123535\n","Iter #11297792:  Learning rate = 0.000002:   Batch Loss = 0.556183, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952105522155762, Accuracy = 0.9177098870277405\n","Iter #11298816:  Learning rate = 0.000002:   Batch Loss = 0.569308, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595199465751648, Accuracy = 0.9173760414123535\n","Iter #11299840:  Learning rate = 0.000002:   Batch Loss = 0.538384, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951991081237793, Accuracy = 0.9178768396377563\n","Iter #11300864:  Learning rate = 0.000002:   Batch Loss = 0.512472, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952205657958984, Accuracy = 0.9182106256484985\n","Iter #11301888:  Learning rate = 0.000002:   Batch Loss = 0.546732, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952299237251282, Accuracy = 0.9180437326431274\n","Iter #11302912:  Learning rate = 0.000002:   Batch Loss = 0.590501, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595231294631958, Accuracy = 0.9180437326431274\n","Iter #11303936:  Learning rate = 0.000002:   Batch Loss = 0.551098, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952292084693909, Accuracy = 0.9178768396377563\n","Iter #11304960:  Learning rate = 0.000002:   Batch Loss = 0.494817, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952475666999817, Accuracy = 0.9178768396377563\n","Iter #11305984:  Learning rate = 0.000002:   Batch Loss = 0.534409, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952581763267517, Accuracy = 0.9178768396377563\n","Iter #11307008:  Learning rate = 0.000002:   Batch Loss = 0.546787, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952615737915039, Accuracy = 0.9180437326431274\n","Iter #11308032:  Learning rate = 0.000002:   Batch Loss = 0.557811, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952247977256775, Accuracy = 0.9180437326431274\n","Iter #11309056:  Learning rate = 0.000002:   Batch Loss = 0.544531, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952106714248657, Accuracy = 0.9177098870277405\n","Iter #11310080:  Learning rate = 0.000002:   Batch Loss = 0.521688, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048897743225, Accuracy = 0.9177098870277405\n","Iter #11311104:  Learning rate = 0.000002:   Batch Loss = 0.593305, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952126383781433, Accuracy = 0.9175429940223694\n","Iter #11312128:  Learning rate = 0.000002:   Batch Loss = 0.558328, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952298045158386, Accuracy = 0.9178768396377563\n","Iter #11313152:  Learning rate = 0.000002:   Batch Loss = 0.654369, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952215194702148, Accuracy = 0.9173760414123535\n","Iter #11314176:  Learning rate = 0.000002:   Batch Loss = 0.576699, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952239036560059, Accuracy = 0.9173760414123535\n","Iter #11315200:  Learning rate = 0.000002:   Batch Loss = 0.545216, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952446460723877, Accuracy = 0.9175429940223694\n","Iter #11316224:  Learning rate = 0.000002:   Batch Loss = 0.547117, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952662229537964, Accuracy = 0.9177098870277405\n","Iter #11317248:  Learning rate = 0.000002:   Batch Loss = 0.580988, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952467918395996, Accuracy = 0.9177098870277405\n","Iter #11318272:  Learning rate = 0.000002:   Batch Loss = 0.654051, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952275991439819, Accuracy = 0.9175429940223694\n","Iter #11319296:  Learning rate = 0.000002:   Batch Loss = 0.541833, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952518582344055, Accuracy = 0.9178768396377563\n","Iter #11320320:  Learning rate = 0.000002:   Batch Loss = 0.537360, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952609777450562, Accuracy = 0.9180437326431274\n","Iter #11321344:  Learning rate = 0.000002:   Batch Loss = 0.576611, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952237844467163, Accuracy = 0.9178768396377563\n","Iter #11322368:  Learning rate = 0.000002:   Batch Loss = 0.641926, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951893925666809, Accuracy = 0.9173760414123535\n","Iter #11323392:  Learning rate = 0.000002:   Batch Loss = 0.516398, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951614379882812, Accuracy = 0.9173760414123535\n","Iter #11324416:  Learning rate = 0.000002:   Batch Loss = 0.491783, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951461791992188, Accuracy = 0.9172091484069824\n","Iter #11325440:  Learning rate = 0.000002:   Batch Loss = 0.543834, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951632261276245, Accuracy = 0.9173760414123535\n","Iter #11326464:  Learning rate = 0.000002:   Batch Loss = 0.550016, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951896905899048, Accuracy = 0.9177098870277405\n","Iter #11327488:  Learning rate = 0.000002:   Batch Loss = 0.489653, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951950550079346, Accuracy = 0.9178768396377563\n","Iter #11328512:  Learning rate = 0.000002:   Batch Loss = 0.569457, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952080488204956, Accuracy = 0.9178768396377563\n","Iter #11329536:  Learning rate = 0.000002:   Batch Loss = 0.528541, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952203869819641, Accuracy = 0.9180437326431274\n","Iter #11330560:  Learning rate = 0.000002:   Batch Loss = 0.555601, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952103734016418, Accuracy = 0.9178768396377563\n","Iter #11331584:  Learning rate = 0.000002:   Batch Loss = 0.570472, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952178239822388, Accuracy = 0.9178768396377563\n","Iter #11332608:  Learning rate = 0.000002:   Batch Loss = 0.580004, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952231287956238, Accuracy = 0.9180437326431274\n","Iter #11333632:  Learning rate = 0.000002:   Batch Loss = 0.524986, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952192544937134, Accuracy = 0.9177098870277405\n","Iter #11334656:  Learning rate = 0.000002:   Batch Loss = 0.563536, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595215916633606, Accuracy = 0.9170422554016113\n","Iter #11335680:  Learning rate = 0.000002:   Batch Loss = 0.592135, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952106714248657, Accuracy = 0.9170422554016113\n","Iter #11336704:  Learning rate = 0.000002:   Batch Loss = 0.553843, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952126979827881, Accuracy = 0.9172091484069824\n","Iter #11337728:  Learning rate = 0.000002:   Batch Loss = 0.578404, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952069163322449, Accuracy = 0.9172091484069824\n","Iter #11338752:  Learning rate = 0.000002:   Batch Loss = 0.539528, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952083468437195, Accuracy = 0.9167084097862244\n","Iter #11339776:  Learning rate = 0.000002:   Batch Loss = 0.609369, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952234268188477, Accuracy = 0.9170422554016113\n","Iter #11340800:  Learning rate = 0.000002:   Batch Loss = 0.573002, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952235460281372, Accuracy = 0.9172091484069824\n","Iter #11341824:  Learning rate = 0.000002:   Batch Loss = 0.535183, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952117443084717, Accuracy = 0.9172091484069824\n","Iter #11342848:  Learning rate = 0.000002:   Batch Loss = 0.612434, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595194935798645, Accuracy = 0.9168753027915955\n","Iter #11343872:  Learning rate = 0.000002:   Batch Loss = 0.547376, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951842069625854, Accuracy = 0.9172091484069824\n","Iter #11344896:  Learning rate = 0.000002:   Batch Loss = 0.539959, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952004194259644, Accuracy = 0.9175429940223694\n","Iter #11345920:  Learning rate = 0.000002:   Batch Loss = 0.593028, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952259302139282, Accuracy = 0.9178768396377563\n","Iter #11346944:  Learning rate = 0.000002:   Batch Loss = 0.520797, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952511429786682, Accuracy = 0.9177098870277405\n","Iter #11347968:  Learning rate = 0.000002:   Batch Loss = 0.514222, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952690243721008, Accuracy = 0.9178768396377563\n","Iter #11348992:  Learning rate = 0.000002:   Batch Loss = 0.547366, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952941179275513, Accuracy = 0.9178768396377563\n","Iter #11350016:  Learning rate = 0.000002:   Batch Loss = 0.544489, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952835083007812, Accuracy = 0.9177098870277405\n","Iter #11351040:  Learning rate = 0.000002:   Batch Loss = 0.570482, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952794551849365, Accuracy = 0.9178768396377563\n","Iter #11352064:  Learning rate = 0.000002:   Batch Loss = 0.583149, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952650308609009, Accuracy = 0.9180437326431274\n","Iter #11353088:  Learning rate = 0.000002:   Batch Loss = 0.504886, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952459573745728, Accuracy = 0.9180437326431274\n","Iter #11354112:  Learning rate = 0.000002:   Batch Loss = 0.658622, Accuracy = 0.859375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952280759811401, Accuracy = 0.9178768396377563\n","Iter #11355136:  Learning rate = 0.000002:   Batch Loss = 0.603332, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952107906341553, Accuracy = 0.9175429940223694\n","Iter #11356160:  Learning rate = 0.000002:   Batch Loss = 0.528971, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952118635177612, Accuracy = 0.9177098870277405\n","Iter #11357184:  Learning rate = 0.000002:   Batch Loss = 0.519118, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952197313308716, Accuracy = 0.9175429940223694\n","Iter #11358208:  Learning rate = 0.000002:   Batch Loss = 0.565013, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952221155166626, Accuracy = 0.9178768396377563\n","Iter #11359232:  Learning rate = 0.000002:   Batch Loss = 0.565849, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595214307308197, Accuracy = 0.9178768396377563\n","Iter #11360256:  Learning rate = 0.000002:   Batch Loss = 0.523396, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59520423412323, Accuracy = 0.9173760414123535\n","Iter #11361280:  Learning rate = 0.000002:   Batch Loss = 0.619019, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951952934265137, Accuracy = 0.9172091484069824\n","Iter #11362304:  Learning rate = 0.000002:   Batch Loss = 0.598595, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952030420303345, Accuracy = 0.9177098870277405\n","Iter #11363328:  Learning rate = 0.000002:   Batch Loss = 0.588714, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952141284942627, Accuracy = 0.9177098870277405\n","Iter #11364352:  Learning rate = 0.000002:   Batch Loss = 0.499868, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952180624008179, Accuracy = 0.9177098870277405\n","Iter #11365376:  Learning rate = 0.000002:   Batch Loss = 0.588083, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952229499816895, Accuracy = 0.9177098870277405\n","Iter #11366400:  Learning rate = 0.000002:   Batch Loss = 0.600391, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952236652374268, Accuracy = 0.9177098870277405\n","Iter #11367424:  Learning rate = 0.000002:   Batch Loss = 0.534616, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952318906784058, Accuracy = 0.9178768396377563\n","Iter #11368448:  Learning rate = 0.000002:   Batch Loss = 0.553588, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952352285385132, Accuracy = 0.9180437326431274\n","Iter #11369472:  Learning rate = 0.000002:   Batch Loss = 0.597102, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59522545337677, Accuracy = 0.9178768396377563\n","Iter #11370496:  Learning rate = 0.000002:   Batch Loss = 0.591957, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952223539352417, Accuracy = 0.9180437326431274\n","Iter #11371520:  Learning rate = 0.000002:   Batch Loss = 0.565715, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952166318893433, Accuracy = 0.9180437326431274\n","Iter #11372544:  Learning rate = 0.000002:   Batch Loss = 0.531048, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952233672142029, Accuracy = 0.9178768396377563\n","Iter #11373568:  Learning rate = 0.000002:   Batch Loss = 0.573143, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952113270759583, Accuracy = 0.9178768396377563\n","Iter #11374592:  Learning rate = 0.000002:   Batch Loss = 0.545301, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952168107032776, Accuracy = 0.9178768396377563\n","Iter #11375616:  Learning rate = 0.000002:   Batch Loss = 0.576838, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952236652374268, Accuracy = 0.9178768396377563\n","Iter #11376640:  Learning rate = 0.000002:   Batch Loss = 0.550843, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156782150269, Accuracy = 0.9178768396377563\n","Iter #11377664:  Learning rate = 0.000002:   Batch Loss = 0.520210, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951945781707764, Accuracy = 0.9178768396377563\n","Iter #11378688:  Learning rate = 0.000002:   Batch Loss = 0.529923, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951974391937256, Accuracy = 0.9178768396377563\n","Iter #11379712:  Learning rate = 0.000002:   Batch Loss = 0.495900, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951778292655945, Accuracy = 0.9175429940223694\n","Iter #11380736:  Learning rate = 0.000002:   Batch Loss = 0.539909, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951952934265137, Accuracy = 0.9182106256484985\n","Iter #11381760:  Learning rate = 0.000002:   Batch Loss = 0.549720, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952078104019165, Accuracy = 0.9178768396377563\n","Iter #11382784:  Learning rate = 0.000002:   Batch Loss = 0.576700, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952162146568298, Accuracy = 0.9180437326431274\n","Iter #11383808:  Learning rate = 0.000002:   Batch Loss = 0.608504, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952116250991821, Accuracy = 0.9180437326431274\n","Iter #11384832:  Learning rate = 0.000002:   Batch Loss = 0.531699, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952220559120178, Accuracy = 0.9180437326431274\n","Iter #11385856:  Learning rate = 0.000002:   Batch Loss = 0.582070, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595230221748352, Accuracy = 0.9180437326431274\n","Iter #11386880:  Learning rate = 0.000002:   Batch Loss = 0.569850, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952509641647339, Accuracy = 0.9177098870277405\n","Iter #11387904:  Learning rate = 0.000002:   Batch Loss = 0.529707, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952780842781067, Accuracy = 0.9182106256484985\n","Iter #11388928:  Learning rate = 0.000002:   Batch Loss = 0.562564, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595304012298584, Accuracy = 0.9180437326431274\n","Iter #11389952:  Learning rate = 0.000002:   Batch Loss = 0.596328, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953311324119568, Accuracy = 0.9178768396377563\n","Iter #11390976:  Learning rate = 0.000002:   Batch Loss = 0.658453, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953182578086853, Accuracy = 0.9178768396377563\n","Iter #11392000:  Learning rate = 0.000002:   Batch Loss = 0.556070, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595287561416626, Accuracy = 0.9180437326431274\n","Iter #11393024:  Learning rate = 0.000002:   Batch Loss = 0.552384, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952403545379639, Accuracy = 0.9180437326431274\n","Iter #11394048:  Learning rate = 0.000002:   Batch Loss = 0.557256, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595209002494812, Accuracy = 0.9180437326431274\n","Iter #11395072:  Learning rate = 0.000002:   Batch Loss = 0.609154, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952088832855225, Accuracy = 0.9177098870277405\n","Iter #11396096:  Learning rate = 0.000002:   Batch Loss = 0.521481, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952483415603638, Accuracy = 0.9178768396377563\n","Iter #11397120:  Learning rate = 0.000002:   Batch Loss = 0.589729, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952696800231934, Accuracy = 0.9178768396377563\n","Iter #11398144:  Learning rate = 0.000002:   Batch Loss = 0.532186, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952656865119934, Accuracy = 0.9178768396377563\n","Iter #11399168:  Learning rate = 0.000002:   Batch Loss = 0.563271, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952608585357666, Accuracy = 0.9178768396377563\n","Iter #11400192:  Learning rate = 0.000002:   Batch Loss = 0.567102, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952264070510864, Accuracy = 0.9177098870277405\n","Iter #11401216:  Learning rate = 0.000002:   Batch Loss = 0.580823, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073335647583, Accuracy = 0.9177098870277405\n","Iter #11402240:  Learning rate = 0.000002:   Batch Loss = 0.553427, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951838493347168, Accuracy = 0.9175429940223694\n","Iter #11403264:  Learning rate = 0.000002:   Batch Loss = 0.497073, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951612591743469, Accuracy = 0.9170422554016113\n","Iter #11404288:  Learning rate = 0.000002:   Batch Loss = 0.593933, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951723456382751, Accuracy = 0.9173760414123535\n","Iter #11405312:  Learning rate = 0.000002:   Batch Loss = 0.547076, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951762199401855, Accuracy = 0.9172091484069824\n","Iter #11406336:  Learning rate = 0.000002:   Batch Loss = 0.600234, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951845645904541, Accuracy = 0.9173760414123535\n","Iter #11407360:  Learning rate = 0.000002:   Batch Loss = 0.560299, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952006578445435, Accuracy = 0.9173760414123535\n","Iter #11408384:  Learning rate = 0.000002:   Batch Loss = 0.538688, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952260494232178, Accuracy = 0.9180437326431274\n","Iter #11409408:  Learning rate = 0.000002:   Batch Loss = 0.504136, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952242612838745, Accuracy = 0.9180437326431274\n","Iter #11410432:  Learning rate = 0.000002:   Batch Loss = 0.591601, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122211456299, Accuracy = 0.9178768396377563\n","Iter #11411456:  Learning rate = 0.000002:   Batch Loss = 0.619543, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952094793319702, Accuracy = 0.9177098870277405\n","Iter #11412480:  Learning rate = 0.000002:   Batch Loss = 0.599446, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029228210449, Accuracy = 0.9175429940223694\n","Iter #11413504:  Learning rate = 0.000002:   Batch Loss = 0.519579, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951945781707764, Accuracy = 0.9173760414123535\n","Iter #11414528:  Learning rate = 0.000002:   Batch Loss = 0.606881, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951807498931885, Accuracy = 0.9177098870277405\n","Iter #11415552:  Learning rate = 0.000002:   Batch Loss = 0.516850, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951667428016663, Accuracy = 0.9175429940223694\n","Iter #11416576:  Learning rate = 0.000002:   Batch Loss = 0.584329, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951674580574036, Accuracy = 0.9177098870277405\n","Iter #11417600:  Learning rate = 0.000002:   Batch Loss = 0.543534, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951790809631348, Accuracy = 0.9177098870277405\n","Iter #11418624:  Learning rate = 0.000002:   Batch Loss = 0.638005, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951862335205078, Accuracy = 0.9173760414123535\n","Iter #11419648:  Learning rate = 0.000002:   Batch Loss = 0.558764, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951951742172241, Accuracy = 0.9177098870277405\n","Iter #11420672:  Learning rate = 0.000002:   Batch Loss = 0.702044, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952039957046509, Accuracy = 0.9178768396377563\n","Iter #11421696:  Learning rate = 0.000002:   Batch Loss = 0.592853, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952205061912537, Accuracy = 0.9178768396377563\n","Iter #11422720:  Learning rate = 0.000002:   Batch Loss = 0.553860, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952800512313843, Accuracy = 0.9180437326431274\n","Iter #11423744:  Learning rate = 0.000002:   Batch Loss = 0.523898, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953440070152283, Accuracy = 0.9180437326431274\n","Iter #11424768:  Learning rate = 0.000002:   Batch Loss = 0.469834, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953027009963989, Accuracy = 0.9180437326431274\n","Iter #11425792:  Learning rate = 0.000002:   Batch Loss = 0.569892, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952578186988831, Accuracy = 0.9177098870277405\n","Iter #11426816:  Learning rate = 0.000002:   Batch Loss = 0.516348, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952340364456177, Accuracy = 0.9178768396377563\n","Iter #11427840:  Learning rate = 0.000002:   Batch Loss = 0.639136, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952289700508118, Accuracy = 0.9177098870277405\n","Iter #11428864:  Learning rate = 0.000002:   Batch Loss = 0.561121, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952135324478149, Accuracy = 0.9178768396377563\n","Iter #11429888:  Learning rate = 0.000002:   Batch Loss = 0.545629, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952075719833374, Accuracy = 0.9178768396377563\n","Iter #11430912:  Learning rate = 0.000002:   Batch Loss = 0.556738, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951846837997437, Accuracy = 0.9172091484069824\n","Iter #11431936:  Learning rate = 0.000002:   Batch Loss = 0.583975, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595162034034729, Accuracy = 0.9172091484069824\n","Iter #11432960:  Learning rate = 0.000002:   Batch Loss = 0.567170, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595150887966156, Accuracy = 0.9172091484069824\n","Iter #11433984:  Learning rate = 0.000002:   Batch Loss = 0.531690, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595156192779541, Accuracy = 0.9172091484069824\n","Iter #11435008:  Learning rate = 0.000002:   Batch Loss = 0.550297, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951508283615112, Accuracy = 0.9172091484069824\n","Iter #11436032:  Learning rate = 0.000002:   Batch Loss = 0.544885, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951634645462036, Accuracy = 0.9172091484069824\n","Iter #11437056:  Learning rate = 0.000002:   Batch Loss = 0.520178, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951648950576782, Accuracy = 0.9172091484069824\n","Iter #11438080:  Learning rate = 0.000002:   Batch Loss = 0.553656, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951656103134155, Accuracy = 0.9172091484069824\n","Iter #11439104:  Learning rate = 0.000002:   Batch Loss = 0.613835, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595165491104126, Accuracy = 0.9172091484069824\n","Iter #11440128:  Learning rate = 0.000002:   Batch Loss = 0.541404, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951775312423706, Accuracy = 0.9175429940223694\n","Iter #11441152:  Learning rate = 0.000002:   Batch Loss = 0.584961, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951823592185974, Accuracy = 0.9173760414123535\n","Iter #11442176:  Learning rate = 0.000002:   Batch Loss = 0.551042, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951961874961853, Accuracy = 0.9177098870277405\n","Iter #11443200:  Learning rate = 0.000002:   Batch Loss = 0.550588, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952053070068359, Accuracy = 0.9175429940223694\n","Iter #11444224:  Learning rate = 0.000002:   Batch Loss = 0.530172, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952349305152893, Accuracy = 0.9178768396377563\n","Iter #11445248:  Learning rate = 0.000002:   Batch Loss = 0.573560, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952550172805786, Accuracy = 0.9175429940223694\n","Iter #11446272:  Learning rate = 0.000002:   Batch Loss = 0.556564, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952782034873962, Accuracy = 0.9178768396377563\n","Iter #11447296:  Learning rate = 0.000002:   Batch Loss = 0.497840, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953071713447571, Accuracy = 0.9178768396377563\n","Iter #11448320:  Learning rate = 0.000002:   Batch Loss = 0.513267, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953282117843628, Accuracy = 0.9180437326431274\n","Iter #11449344:  Learning rate = 0.000002:   Batch Loss = 0.622600, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953414440155029, Accuracy = 0.9182106256484985\n","Iter #11450368:  Learning rate = 0.000002:   Batch Loss = 0.515211, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953173637390137, Accuracy = 0.9180437326431274\n","Iter #11451392:  Learning rate = 0.000002:   Batch Loss = 0.575992, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59526526927948, Accuracy = 0.9180437326431274\n","Iter #11452416:  Learning rate = 0.000002:   Batch Loss = 0.557282, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952517986297607, Accuracy = 0.9180437326431274\n","Iter #11453440:  Learning rate = 0.000002:   Batch Loss = 0.542009, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952396392822266, Accuracy = 0.9180437326431274\n","Iter #11454464:  Learning rate = 0.000002:   Batch Loss = 0.591526, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595221221446991, Accuracy = 0.9180437326431274\n","Iter #11455488:  Learning rate = 0.000002:   Batch Loss = 0.589920, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952033996582031, Accuracy = 0.9178768396377563\n","Iter #11456512:  Learning rate = 0.000002:   Batch Loss = 0.595305, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952103137969971, Accuracy = 0.9180437326431274\n","Iter #11457536:  Learning rate = 0.000002:   Batch Loss = 0.567118, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952140092849731, Accuracy = 0.9180437326431274\n","Iter #11458560:  Learning rate = 0.000002:   Batch Loss = 0.654805, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952283143997192, Accuracy = 0.9180437326431274\n","Iter #11459584:  Learning rate = 0.000002:   Batch Loss = 0.485708, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952260494232178, Accuracy = 0.9180437326431274\n","Iter #11460608:  Learning rate = 0.000002:   Batch Loss = 0.551228, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952043533325195, Accuracy = 0.9180437326431274\n","Iter #11461632:  Learning rate = 0.000002:   Batch Loss = 0.527564, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951849818229675, Accuracy = 0.9177098870277405\n","Iter #11462656:  Learning rate = 0.000002:   Batch Loss = 0.579530, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951736569404602, Accuracy = 0.9173760414123535\n","Iter #11463680:  Learning rate = 0.000002:   Batch Loss = 0.642590, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951701402664185, Accuracy = 0.9173760414123535\n","Iter #11464704:  Learning rate = 0.000002:   Batch Loss = 0.504448, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951595306396484, Accuracy = 0.9175429940223694\n","Iter #11465728:  Learning rate = 0.000002:   Batch Loss = 0.567782, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951640605926514, Accuracy = 0.9170422554016113\n","Iter #11466752:  Learning rate = 0.000002:   Batch Loss = 0.642317, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951735973358154, Accuracy = 0.9173760414123535\n","Iter #11467776:  Learning rate = 0.000002:   Batch Loss = 0.536455, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951761603355408, Accuracy = 0.9175429940223694\n","Iter #11468800:  Learning rate = 0.000002:   Batch Loss = 0.541164, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951710939407349, Accuracy = 0.9173760414123535\n","Iter #11469824:  Learning rate = 0.000002:   Batch Loss = 0.581299, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951749086380005, Accuracy = 0.9173760414123535\n","Iter #11470848:  Learning rate = 0.000002:   Batch Loss = 0.505358, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952044725418091, Accuracy = 0.9175429940223694\n","Iter #11471872:  Learning rate = 0.000002:   Batch Loss = 0.483096, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952123403549194, Accuracy = 0.9175429940223694\n","Iter #11472896:  Learning rate = 0.000002:   Batch Loss = 0.634367, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952097177505493, Accuracy = 0.9175429940223694\n","Iter #11473920:  Learning rate = 0.000002:   Batch Loss = 0.518638, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59520423412323, Accuracy = 0.9175429940223694\n","Iter #11474944:  Learning rate = 0.000002:   Batch Loss = 0.601984, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952063798904419, Accuracy = 0.9175429940223694\n","Iter #11475968:  Learning rate = 0.000002:   Batch Loss = 0.611581, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952132344245911, Accuracy = 0.9175429940223694\n","Iter #11476992:  Learning rate = 0.000002:   Batch Loss = 0.638276, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952489972114563, Accuracy = 0.9180437326431274\n","Iter #11478016:  Learning rate = 0.000002:   Batch Loss = 0.520697, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952746272087097, Accuracy = 0.9178768396377563\n","Iter #11479040:  Learning rate = 0.000002:   Batch Loss = 0.586813, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952517986297607, Accuracy = 0.9180437326431274\n","Iter #11480064:  Learning rate = 0.000002:   Batch Loss = 0.529119, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952092409133911, Accuracy = 0.9178768396377563\n","Iter #11481088:  Learning rate = 0.000002:   Batch Loss = 0.577665, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952203869819641, Accuracy = 0.9175429940223694\n","Iter #11482112:  Learning rate = 0.000002:   Batch Loss = 0.566580, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952218174934387, Accuracy = 0.9177098870277405\n","Iter #11483136:  Learning rate = 0.000002:   Batch Loss = 0.576696, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952352285385132, Accuracy = 0.9178768396377563\n","Iter #11484160:  Learning rate = 0.000002:   Batch Loss = 0.606951, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952490568161011, Accuracy = 0.9178768396377563\n","Iter #11485184:  Learning rate = 0.000002:   Batch Loss = 0.611068, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952810049057007, Accuracy = 0.9178768396377563\n","Iter #11486208:  Learning rate = 0.000002:   Batch Loss = 0.538894, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953050851821899, Accuracy = 0.9178768396377563\n","Iter #11487232:  Learning rate = 0.000002:   Batch Loss = 0.550261, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952921509742737, Accuracy = 0.9180437326431274\n","Iter #11488256:  Learning rate = 0.000002:   Batch Loss = 0.567582, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952672958374023, Accuracy = 0.9180437326431274\n","Iter #11489280:  Learning rate = 0.000002:   Batch Loss = 0.503474, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952608585357666, Accuracy = 0.9178768396377563\n","Iter #11490304:  Learning rate = 0.000002:   Batch Loss = 0.585674, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952253341674805, Accuracy = 0.9178768396377563\n","Iter #11491328:  Learning rate = 0.000002:   Batch Loss = 0.512481, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951995849609375, Accuracy = 0.9175429940223694\n","Iter #11492352:  Learning rate = 0.000002:   Batch Loss = 0.607267, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951966643333435, Accuracy = 0.9175429940223694\n","Iter #11493376:  Learning rate = 0.000002:   Batch Loss = 0.554471, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073931694031, Accuracy = 0.9172091484069824\n","Iter #11494400:  Learning rate = 0.000002:   Batch Loss = 0.559740, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952033400535583, Accuracy = 0.9178768396377563\n","Iter #11495424:  Learning rate = 0.000002:   Batch Loss = 0.563894, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951934456825256, Accuracy = 0.9175429940223694\n","Iter #11496448:  Learning rate = 0.000002:   Batch Loss = 0.517641, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951889753341675, Accuracy = 0.9175429940223694\n","Iter #11497472:  Learning rate = 0.000002:   Batch Loss = 0.589440, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951801538467407, Accuracy = 0.9172091484069824\n","Iter #11498496:  Learning rate = 0.000002:   Batch Loss = 0.520902, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595172643661499, Accuracy = 0.9173760414123535\n","Iter #11499520:  Learning rate = 0.000002:   Batch Loss = 0.576081, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951719880104065, Accuracy = 0.9173760414123535\n","Iter #11500544:  Learning rate = 0.000002:   Batch Loss = 0.500203, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951751470565796, Accuracy = 0.9175429940223694\n","Iter #11501568:  Learning rate = 0.000002:   Batch Loss = 0.707041, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951811671257019, Accuracy = 0.9175429940223694\n","Iter #11502592:  Learning rate = 0.000002:   Batch Loss = 0.613363, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595170795917511, Accuracy = 0.9175429940223694\n","Iter #11503616:  Learning rate = 0.000002:   Batch Loss = 0.626744, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951862335205078, Accuracy = 0.9173760414123535\n","Iter #11504640:  Learning rate = 0.000002:   Batch Loss = 0.473051, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988101005554, Accuracy = 0.9175429940223694\n","Iter #11505664:  Learning rate = 0.000002:   Batch Loss = 0.528946, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951875448226929, Accuracy = 0.9178768396377563\n","Iter #11506688:  Learning rate = 0.000002:   Batch Loss = 0.563997, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951964259147644, Accuracy = 0.9178768396377563\n","Iter #11507712:  Learning rate = 0.000002:   Batch Loss = 0.513028, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951887369155884, Accuracy = 0.9177098870277405\n","Iter #11508736:  Learning rate = 0.000002:   Batch Loss = 0.596326, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951996445655823, Accuracy = 0.9177098870277405\n","Iter #11509760:  Learning rate = 0.000002:   Batch Loss = 0.607486, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952162742614746, Accuracy = 0.9178768396377563\n","Iter #11510784:  Learning rate = 0.000002:   Batch Loss = 0.587488, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952133536338806, Accuracy = 0.9180437326431274\n","Iter #11511808:  Learning rate = 0.000002:   Batch Loss = 0.575962, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952054262161255, Accuracy = 0.9178768396377563\n","Iter #11512832:  Learning rate = 0.000002:   Batch Loss = 0.539127, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951922535896301, Accuracy = 0.9177098870277405\n","Iter #11513856:  Learning rate = 0.000002:   Batch Loss = 0.576108, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951985120773315, Accuracy = 0.9177098870277405\n","Iter #11514880:  Learning rate = 0.000002:   Batch Loss = 0.595015, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952091217041016, Accuracy = 0.9177098870277405\n","Iter #11515904:  Learning rate = 0.000002:   Batch Loss = 0.616869, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595210075378418, Accuracy = 0.9178768396377563\n","Iter #11516928:  Learning rate = 0.000002:   Batch Loss = 0.595438, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988697052002, Accuracy = 0.9177098870277405\n","Iter #11517952:  Learning rate = 0.000002:   Batch Loss = 0.585739, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951954126358032, Accuracy = 0.9175429940223694\n","Iter #11518976:  Learning rate = 0.000002:   Batch Loss = 0.495610, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951967835426331, Accuracy = 0.9177098870277405\n","Iter #11520000:  Learning rate = 0.000002:   Batch Loss = 0.535127, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951959490776062, Accuracy = 0.9173760414123535\n","Iter #11521024:  Learning rate = 0.000002:   Batch Loss = 0.534513, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951905250549316, Accuracy = 0.9173760414123535\n","Iter #11522048:  Learning rate = 0.000002:   Batch Loss = 0.508673, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595194935798645, Accuracy = 0.9170422554016113\n","Iter #11523072:  Learning rate = 0.000002:   Batch Loss = 0.496820, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951970815658569, Accuracy = 0.9170422554016113\n","Iter #11524096:  Learning rate = 0.000002:   Batch Loss = 0.590039, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951919555664062, Accuracy = 0.9170422554016113\n","Iter #11525120:  Learning rate = 0.000002:   Batch Loss = 0.557516, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951888561248779, Accuracy = 0.9173760414123535\n","Iter #11526144:  Learning rate = 0.000002:   Batch Loss = 0.565321, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595193088054657, Accuracy = 0.9177098870277405\n","Iter #11527168:  Learning rate = 0.000002:   Batch Loss = 0.485451, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951915979385376, Accuracy = 0.9180437326431274\n","Iter #11528192:  Learning rate = 0.000002:   Batch Loss = 0.552086, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952081680297852, Accuracy = 0.9180437326431274\n","Iter #11529216:  Learning rate = 0.000002:   Batch Loss = 0.518842, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073931694031, Accuracy = 0.9177098870277405\n","Iter #11530240:  Learning rate = 0.000002:   Batch Loss = 0.531748, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952247977256775, Accuracy = 0.9180437326431274\n","Iter #11531264:  Learning rate = 0.000002:   Batch Loss = 0.504235, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952470302581787, Accuracy = 0.9180437326431274\n","Iter #11532288:  Learning rate = 0.000002:   Batch Loss = 0.480278, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952489972114563, Accuracy = 0.9180437326431274\n","Iter #11533312:  Learning rate = 0.000002:   Batch Loss = 0.601332, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952246189117432, Accuracy = 0.9182106256484985\n","Iter #11534336:  Learning rate = 0.000002:   Batch Loss = 0.531121, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952292680740356, Accuracy = 0.9180437326431274\n","Iter #11535360:  Learning rate = 0.000002:   Batch Loss = 0.553143, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595227837562561, Accuracy = 0.9177098870277405\n","Iter #11536384:  Learning rate = 0.000002:   Batch Loss = 0.583830, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952301025390625, Accuracy = 0.9178768396377563\n","Iter #11537408:  Learning rate = 0.000002:   Batch Loss = 0.580449, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408313751221, Accuracy = 0.9177098870277405\n","Iter #11538432:  Learning rate = 0.000002:   Batch Loss = 0.646335, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952383279800415, Accuracy = 0.9180437326431274\n","Iter #11539456:  Learning rate = 0.000002:   Batch Loss = 0.558264, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952274799346924, Accuracy = 0.9180437326431274\n","Iter #11540480:  Learning rate = 0.000002:   Batch Loss = 0.496365, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595218300819397, Accuracy = 0.9180437326431274\n","Iter #11541504:  Learning rate = 0.000002:   Batch Loss = 0.521850, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595203697681427, Accuracy = 0.9180437326431274\n","Iter #11542528:  Learning rate = 0.000002:   Batch Loss = 0.572976, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595181941986084, Accuracy = 0.9177098870277405\n","Iter #11543552:  Learning rate = 0.000002:   Batch Loss = 0.545237, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951635837554932, Accuracy = 0.9175429940223694\n","Iter #11544576:  Learning rate = 0.000002:   Batch Loss = 0.566299, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951659679412842, Accuracy = 0.9175429940223694\n","Iter #11545600:  Learning rate = 0.000002:   Batch Loss = 0.552896, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951678156852722, Accuracy = 0.9177098870277405\n","Iter #11546624:  Learning rate = 0.000002:   Batch Loss = 0.520655, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951691269874573, Accuracy = 0.9177098870277405\n","Iter #11547648:  Learning rate = 0.000002:   Batch Loss = 0.528891, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951646566390991, Accuracy = 0.9175429940223694\n","Iter #11548672:  Learning rate = 0.000002:   Batch Loss = 0.558860, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951623916625977, Accuracy = 0.9175429940223694\n","Iter #11549696:  Learning rate = 0.000002:   Batch Loss = 0.599715, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951488614082336, Accuracy = 0.9175429940223694\n","Iter #11550720:  Learning rate = 0.000002:   Batch Loss = 0.547454, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951604843139648, Accuracy = 0.9175429940223694\n","Iter #11551744:  Learning rate = 0.000002:   Batch Loss = 0.518036, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951851606369019, Accuracy = 0.9175429940223694\n","Iter #11552768:  Learning rate = 0.000002:   Batch Loss = 0.547405, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951955914497375, Accuracy = 0.9177098870277405\n","Iter #11553792:  Learning rate = 0.000002:   Batch Loss = 0.574308, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952184796333313, Accuracy = 0.9178768396377563\n","Iter #11554816:  Learning rate = 0.000002:   Batch Loss = 0.553285, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952239036560059, Accuracy = 0.9177098870277405\n","Iter #11555840:  Learning rate = 0.000002:   Batch Loss = 0.496754, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952121019363403, Accuracy = 0.9178768396377563\n","Iter #11556864:  Learning rate = 0.000002:   Batch Loss = 0.616642, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952138900756836, Accuracy = 0.9178768396377563\n","Iter #11557888:  Learning rate = 0.000002:   Batch Loss = 0.509923, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952187180519104, Accuracy = 0.9178768396377563\n","Iter #11558912:  Learning rate = 0.000002:   Batch Loss = 0.546852, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952259302139282, Accuracy = 0.9180437326431274\n","Iter #11559936:  Learning rate = 0.000002:   Batch Loss = 0.545026, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952371954917908, Accuracy = 0.9180437326431274\n","Iter #11560960:  Learning rate = 0.000002:   Batch Loss = 0.622229, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952410697937012, Accuracy = 0.9178768396377563\n","Iter #11561984:  Learning rate = 0.000002:   Batch Loss = 0.549049, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952516794204712, Accuracy = 0.9178768396377563\n","Iter #11563008:  Learning rate = 0.000002:   Batch Loss = 0.503372, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595242977142334, Accuracy = 0.9180437326431274\n","Iter #11564032:  Learning rate = 0.000002:   Batch Loss = 0.533094, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952339172363281, Accuracy = 0.9180437326431274\n","Iter #11565056:  Learning rate = 0.000002:   Batch Loss = 0.573265, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952276587486267, Accuracy = 0.9180437326431274\n","Iter #11566080:  Learning rate = 0.000002:   Batch Loss = 0.602444, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952262282371521, Accuracy = 0.9178768396377563\n","Iter #11567104:  Learning rate = 0.000002:   Batch Loss = 0.598702, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952070355415344, Accuracy = 0.9178768396377563\n","Iter #11568128:  Learning rate = 0.000002:   Batch Loss = 0.603303, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952030420303345, Accuracy = 0.9178768396377563\n","Iter #11569152:  Learning rate = 0.000002:   Batch Loss = 0.541103, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952005386352539, Accuracy = 0.9178768396377563\n","Iter #11570176:  Learning rate = 0.000002:   Batch Loss = 0.591076, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952112674713135, Accuracy = 0.9178768396377563\n","Iter #11571200:  Learning rate = 0.000002:   Batch Loss = 0.518841, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59522545337677, Accuracy = 0.9177098870277405\n","Iter #11572224:  Learning rate = 0.000002:   Batch Loss = 0.530537, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952473282814026, Accuracy = 0.9177098870277405\n","Iter #11573248:  Learning rate = 0.000002:   Batch Loss = 0.551085, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952592492103577, Accuracy = 0.9178768396377563\n","Iter #11574272:  Learning rate = 0.000002:   Batch Loss = 0.586788, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952508449554443, Accuracy = 0.9177098870277405\n","Iter #11575296:  Learning rate = 0.000002:   Batch Loss = 0.572954, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952303409576416, Accuracy = 0.9178768396377563\n","Iter #11576320:  Learning rate = 0.000002:   Batch Loss = 0.579705, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952082872390747, Accuracy = 0.9178768396377563\n","Iter #11577344:  Learning rate = 0.000002:   Batch Loss = 0.541165, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951899290084839, Accuracy = 0.9173760414123535\n","Iter #11578368:  Learning rate = 0.000002:   Batch Loss = 0.511425, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951892137527466, Accuracy = 0.9175429940223694\n","Iter #11579392:  Learning rate = 0.000002:   Batch Loss = 0.601222, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951815843582153, Accuracy = 0.9173760414123535\n","Iter #11580416:  Learning rate = 0.000002:   Batch Loss = 0.541471, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951784253120422, Accuracy = 0.9173760414123535\n","Iter #11581440:  Learning rate = 0.000002:   Batch Loss = 0.571629, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951811075210571, Accuracy = 0.9172091484069824\n","Iter #11582464:  Learning rate = 0.000002:   Batch Loss = 0.563837, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951947569847107, Accuracy = 0.9173760414123535\n","Iter #11583488:  Learning rate = 0.000002:   Batch Loss = 0.586314, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595221996307373, Accuracy = 0.9177098870277405\n","Iter #11584512:  Learning rate = 0.000002:   Batch Loss = 0.535579, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952312350273132, Accuracy = 0.9177098870277405\n","Iter #11585536:  Learning rate = 0.000002:   Batch Loss = 0.531134, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952363014221191, Accuracy = 0.9178768396377563\n","Iter #11586560:  Learning rate = 0.000002:   Batch Loss = 0.568199, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952435731887817, Accuracy = 0.9178768396377563\n","Iter #11587584:  Learning rate = 0.000002:   Batch Loss = 0.525503, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952438116073608, Accuracy = 0.9178768396377563\n","Iter #11588608:  Learning rate = 0.000002:   Batch Loss = 0.527317, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952202081680298, Accuracy = 0.9178768396377563\n","Iter #11589632:  Learning rate = 0.000002:   Batch Loss = 0.532429, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952139496803284, Accuracy = 0.9178768396377563\n","Iter #11590656:  Learning rate = 0.000002:   Batch Loss = 0.626519, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952099561691284, Accuracy = 0.9178768396377563\n","Iter #11591680:  Learning rate = 0.000002:   Batch Loss = 0.551343, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952171683311462, Accuracy = 0.9182106256484985\n","Iter #11592704:  Learning rate = 0.000002:   Batch Loss = 0.564428, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952394008636475, Accuracy = 0.9178768396377563\n","Iter #11593728:  Learning rate = 0.000002:   Batch Loss = 0.530674, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952534675598145, Accuracy = 0.9178768396377563\n","Iter #11594752:  Learning rate = 0.000002:   Batch Loss = 0.529875, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952414870262146, Accuracy = 0.9182106256484985\n","Iter #11595776:  Learning rate = 0.000002:   Batch Loss = 0.573862, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952415466308594, Accuracy = 0.9180437326431274\n","Iter #11596800:  Learning rate = 0.000002:   Batch Loss = 0.556544, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952337980270386, Accuracy = 0.9180437326431274\n","Iter #11597824:  Learning rate = 0.000002:   Batch Loss = 0.530453, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952407121658325, Accuracy = 0.9180437326431274\n","Iter #11598848:  Learning rate = 0.000002:   Batch Loss = 0.669821, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952343940734863, Accuracy = 0.9180437326431274\n","Iter #11599872:  Learning rate = 0.000002:   Batch Loss = 0.522919, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595220685005188, Accuracy = 0.9178768396377563\n","Iter #11600896:  Learning rate = 0.000002:   Batch Loss = 0.532828, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951982140541077, Accuracy = 0.9178768396377563\n","Iter #11601920:  Learning rate = 0.000002:   Batch Loss = 0.493943, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952011346817017, Accuracy = 0.9178768396377563\n","Iter #11602944:  Learning rate = 0.000002:   Batch Loss = 0.504050, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951894521713257, Accuracy = 0.9178768396377563\n","Iter #11603968:  Learning rate = 0.000002:   Batch Loss = 0.496133, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951884984970093, Accuracy = 0.9177098870277405\n","Iter #11604992:  Learning rate = 0.000002:   Batch Loss = 0.611203, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988697052002, Accuracy = 0.9178768396377563\n","Iter #11606016:  Learning rate = 0.000002:   Batch Loss = 0.578114, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951871871948242, Accuracy = 0.9180437326431274\n","Iter #11607040:  Learning rate = 0.000002:   Batch Loss = 0.500794, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951873064041138, Accuracy = 0.9180437326431274\n","Iter #11608064:  Learning rate = 0.000002:   Batch Loss = 0.491121, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951960682868958, Accuracy = 0.9180437326431274\n","Iter #11609088:  Learning rate = 0.000002:   Batch Loss = 0.524837, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951977968215942, Accuracy = 0.9180437326431274\n","Iter #11610112:  Learning rate = 0.000002:   Batch Loss = 0.514723, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595162034034729, Accuracy = 0.9172091484069824\n","Iter #11611136:  Learning rate = 0.000002:   Batch Loss = 0.546717, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951435565948486, Accuracy = 0.9173760414123535\n","Iter #11612160:  Learning rate = 0.000002:   Batch Loss = 0.541140, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951780080795288, Accuracy = 0.9173760414123535\n","Iter #11613184:  Learning rate = 0.000002:   Batch Loss = 0.556821, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951905250549316, Accuracy = 0.9177098870277405\n","Iter #11614208:  Learning rate = 0.000002:   Batch Loss = 0.556301, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951846837997437, Accuracy = 0.9175429940223694\n","Iter #11615232:  Learning rate = 0.000002:   Batch Loss = 0.556059, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951921343803406, Accuracy = 0.9175429940223694\n","Iter #11616256:  Learning rate = 0.000002:   Batch Loss = 0.510327, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952034592628479, Accuracy = 0.9177098870277405\n","Iter #11617280:  Learning rate = 0.000002:   Batch Loss = 0.521347, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952088832855225, Accuracy = 0.9175429940223694\n","Iter #11618304:  Learning rate = 0.000002:   Batch Loss = 0.647067, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952343940734863, Accuracy = 0.9177098870277405\n","Iter #11619328:  Learning rate = 0.000002:   Batch Loss = 0.517679, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952388048171997, Accuracy = 0.9178768396377563\n","Iter #11620352:  Learning rate = 0.000002:   Batch Loss = 0.507687, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952421426773071, Accuracy = 0.9178768396377563\n","Iter #11621376:  Learning rate = 0.000002:   Batch Loss = 0.604094, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595226526260376, Accuracy = 0.9180437326431274\n","Iter #11622400:  Learning rate = 0.000002:   Batch Loss = 0.602036, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952082872390747, Accuracy = 0.9178768396377563\n","Iter #11623424:  Learning rate = 0.000002:   Batch Loss = 0.548255, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952061414718628, Accuracy = 0.9178768396377563\n","Iter #11624448:  Learning rate = 0.000002:   Batch Loss = 0.523775, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952020883560181, Accuracy = 0.9180437326431274\n","Iter #11625472:  Learning rate = 0.000002:   Batch Loss = 0.613895, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029228210449, Accuracy = 0.9175429940223694\n","Iter #11626496:  Learning rate = 0.000002:   Batch Loss = 0.651517, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952322483062744, Accuracy = 0.9178768396377563\n","Iter #11627520:  Learning rate = 0.000002:   Batch Loss = 0.595181, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952693223953247, Accuracy = 0.9178768396377563\n","Iter #11628544:  Learning rate = 0.000002:   Batch Loss = 0.545396, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953117609024048, Accuracy = 0.9178768396377563\n","Iter #11629568:  Learning rate = 0.000002:   Batch Loss = 0.566466, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953258275985718, Accuracy = 0.9177098870277405\n","Iter #11630592:  Learning rate = 0.000002:   Batch Loss = 0.607990, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953278541564941, Accuracy = 0.9178768396377563\n","Iter #11631616:  Learning rate = 0.000002:   Batch Loss = 0.497865, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952919721603394, Accuracy = 0.9177098870277405\n","Iter #11632640:  Learning rate = 0.000002:   Batch Loss = 0.599524, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952534675598145, Accuracy = 0.9178768396377563\n","Iter #11633664:  Learning rate = 0.000002:   Batch Loss = 0.523446, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952328443527222, Accuracy = 0.9178768396377563\n","Iter #11634688:  Learning rate = 0.000002:   Batch Loss = 0.548062, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952174663543701, Accuracy = 0.9177098870277405\n","Iter #11635712:  Learning rate = 0.000002:   Batch Loss = 0.606824, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595185399055481, Accuracy = 0.9177098870277405\n","Iter #11636736:  Learning rate = 0.000002:   Batch Loss = 0.526076, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951780676841736, Accuracy = 0.9172091484069824\n","Iter #11637760:  Learning rate = 0.000002:   Batch Loss = 0.583834, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951690077781677, Accuracy = 0.9173760414123535\n","Iter #11638784:  Learning rate = 0.000002:   Batch Loss = 0.565256, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951576232910156, Accuracy = 0.9172091484069824\n","Iter #11639808:  Learning rate = 0.000002:   Batch Loss = 0.560158, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595152735710144, Accuracy = 0.9170422554016113\n","Iter #11640832:  Learning rate = 0.000002:   Batch Loss = 0.596348, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951473712921143, Accuracy = 0.9167084097862244\n","Iter #11641856:  Learning rate = 0.000002:   Batch Loss = 0.589640, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951356291770935, Accuracy = 0.9168753027915955\n","Iter #11642880:  Learning rate = 0.000002:   Batch Loss = 0.501436, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951501131057739, Accuracy = 0.9172091484069824\n","Iter #11643904:  Learning rate = 0.000002:   Batch Loss = 0.494516, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951671600341797, Accuracy = 0.9175429940223694\n","Iter #11644928:  Learning rate = 0.000002:   Batch Loss = 0.573417, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595192551612854, Accuracy = 0.9177098870277405\n","Iter #11645952:  Learning rate = 0.000002:   Batch Loss = 0.519960, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952057838439941, Accuracy = 0.9178768396377563\n","Iter #11646976:  Learning rate = 0.000002:   Batch Loss = 0.532968, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952252149581909, Accuracy = 0.9182106256484985\n","Iter #11648000:  Learning rate = 0.000002:   Batch Loss = 0.517944, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952322483062744, Accuracy = 0.9180437326431274\n","Iter #11649024:  Learning rate = 0.000002:   Batch Loss = 0.560667, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952174067497253, Accuracy = 0.9180437326431274\n","Iter #11650048:  Learning rate = 0.000002:   Batch Loss = 0.544979, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952181816101074, Accuracy = 0.9180437326431274\n","Iter #11651072:  Learning rate = 0.000002:   Batch Loss = 0.566848, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952214002609253, Accuracy = 0.9180437326431274\n","Iter #11652096:  Learning rate = 0.000002:   Batch Loss = 0.596761, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952203273773193, Accuracy = 0.9180437326431274\n","Iter #11653120:  Learning rate = 0.000002:   Batch Loss = 0.583495, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952019691467285, Accuracy = 0.9178768396377563\n","Iter #11654144:  Learning rate = 0.000002:   Batch Loss = 0.477087, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951796770095825, Accuracy = 0.9175429940223694\n","Iter #11655168:  Learning rate = 0.000002:   Batch Loss = 0.573294, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59515380859375, Accuracy = 0.9172091484069824\n","Iter #11656192:  Learning rate = 0.000002:   Batch Loss = 0.550458, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951530933380127, Accuracy = 0.9173760414123535\n","Iter #11657216:  Learning rate = 0.000002:   Batch Loss = 0.565294, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951619744300842, Accuracy = 0.9173760414123535\n","Iter #11658240:  Learning rate = 0.000002:   Batch Loss = 0.619497, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951604843139648, Accuracy = 0.9175429940223694\n","Iter #11659264:  Learning rate = 0.000002:   Batch Loss = 0.594777, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951788425445557, Accuracy = 0.9173760414123535\n","Iter #11660288:  Learning rate = 0.000002:   Batch Loss = 0.587350, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951939821243286, Accuracy = 0.9178768396377563\n","Iter #11661312:  Learning rate = 0.000002:   Batch Loss = 0.495854, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952111482620239, Accuracy = 0.9178768396377563\n","Iter #11662336:  Learning rate = 0.000002:   Batch Loss = 0.652658, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156186103821, Accuracy = 0.9178768396377563\n","Iter #11663360:  Learning rate = 0.000002:   Batch Loss = 0.511756, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952210426330566, Accuracy = 0.9180437326431274\n","Iter #11664384:  Learning rate = 0.000002:   Batch Loss = 0.597552, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952206254005432, Accuracy = 0.9177098870277405\n","Iter #11665408:  Learning rate = 0.000002:   Batch Loss = 0.546970, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951970815658569, Accuracy = 0.9178768396377563\n","Iter #11666432:  Learning rate = 0.000002:   Batch Loss = 0.560823, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951814651489258, Accuracy = 0.9180437326431274\n","Iter #11667456:  Learning rate = 0.000002:   Batch Loss = 0.588247, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951687097549438, Accuracy = 0.9173760414123535\n","Iter #11668480:  Learning rate = 0.000002:   Batch Loss = 0.589018, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951669216156006, Accuracy = 0.9173760414123535\n","Iter #11669504:  Learning rate = 0.000002:   Batch Loss = 0.513753, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951598882675171, Accuracy = 0.9173760414123535\n","Iter #11670528:  Learning rate = 0.000002:   Batch Loss = 0.570493, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951579213142395, Accuracy = 0.9175429940223694\n","Iter #11671552:  Learning rate = 0.000002:   Batch Loss = 0.678881, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951727032661438, Accuracy = 0.9173760414123535\n","Iter #11672576:  Learning rate = 0.000002:   Batch Loss = 0.582406, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951805114746094, Accuracy = 0.9177098870277405\n","Iter #11673600:  Learning rate = 0.000002:   Batch Loss = 0.563629, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952101945877075, Accuracy = 0.9180437326431274\n","Iter #11674624:  Learning rate = 0.000002:   Batch Loss = 0.565180, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952264070510864, Accuracy = 0.9180437326431274\n","Iter #11675648:  Learning rate = 0.000002:   Batch Loss = 0.496958, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952463150024414, Accuracy = 0.9180437326431274\n","Iter #11676672:  Learning rate = 0.000002:   Batch Loss = 0.611931, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952737927436829, Accuracy = 0.9178768396377563\n","Iter #11677696:  Learning rate = 0.000002:   Batch Loss = 0.522780, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952605605125427, Accuracy = 0.9180437326431274\n","Iter #11678720:  Learning rate = 0.000002:   Batch Loss = 0.573294, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952486991882324, Accuracy = 0.9180437326431274\n","Iter #11679744:  Learning rate = 0.000002:   Batch Loss = 0.504341, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952194929122925, Accuracy = 0.9180437326431274\n","Iter #11680768:  Learning rate = 0.000002:   Batch Loss = 0.572425, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952208638191223, Accuracy = 0.9180437326431274\n","Iter #11681792:  Learning rate = 0.000002:   Batch Loss = 0.517709, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595212459564209, Accuracy = 0.9178768396377563\n","Iter #11682816:  Learning rate = 0.000002:   Batch Loss = 0.523199, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952170491218567, Accuracy = 0.9180437326431274\n","Iter #11683840:  Learning rate = 0.000002:   Batch Loss = 0.530643, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952253341674805, Accuracy = 0.9180437326431274\n","Iter #11684864:  Learning rate = 0.000002:   Batch Loss = 0.590023, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952271223068237, Accuracy = 0.9178768396377563\n","Iter #11685888:  Learning rate = 0.000002:   Batch Loss = 0.608068, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952169895172119, Accuracy = 0.9178768396377563\n","Iter #11686912:  Learning rate = 0.000002:   Batch Loss = 0.511328, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952001214027405, Accuracy = 0.9178768396377563\n","Iter #11687936:  Learning rate = 0.000002:   Batch Loss = 0.579997, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951879024505615, Accuracy = 0.9178768396377563\n","Iter #11688960:  Learning rate = 0.000002:   Batch Loss = 0.536769, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951861143112183, Accuracy = 0.9178768396377563\n","Iter #11689984:  Learning rate = 0.000002:   Batch Loss = 0.547510, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952050685882568, Accuracy = 0.9178768396377563\n","Iter #11691008:  Learning rate = 0.000002:   Batch Loss = 0.553435, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952261686325073, Accuracy = 0.9178768396377563\n","Iter #11692032:  Learning rate = 0.000002:   Batch Loss = 0.575634, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952329039573669, Accuracy = 0.9177098870277405\n","Iter #11693056:  Learning rate = 0.000002:   Batch Loss = 0.539681, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.59523606300354, Accuracy = 0.9177098870277405\n","Iter #11694080:  Learning rate = 0.000002:   Batch Loss = 0.537662, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952138900756836, Accuracy = 0.9178768396377563\n","Iter #11695104:  Learning rate = 0.000002:   Batch Loss = 0.620298, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951851606369019, Accuracy = 0.9170422554016113\n","Iter #11696128:  Learning rate = 0.000002:   Batch Loss = 0.613256, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951758623123169, Accuracy = 0.9170422554016113\n","Iter #11697152:  Learning rate = 0.000002:   Batch Loss = 0.567804, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951974391937256, Accuracy = 0.9178768396377563\n","Iter #11698176:  Learning rate = 0.000002:   Batch Loss = 0.581274, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595210075378418, Accuracy = 0.9178768396377563\n","Iter #11699200:  Learning rate = 0.000002:   Batch Loss = 0.579745, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952314734458923, Accuracy = 0.9180437326431274\n","Iter #11700224:  Learning rate = 0.000002:   Batch Loss = 0.528824, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952631235122681, Accuracy = 0.9180437326431274\n","Iter #11701248:  Learning rate = 0.000002:   Batch Loss = 0.537819, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952796936035156, Accuracy = 0.9180437326431274\n","Iter #11702272:  Learning rate = 0.000002:   Batch Loss = 0.567683, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952659845352173, Accuracy = 0.9178768396377563\n","Iter #11703296:  Learning rate = 0.000002:   Batch Loss = 0.567243, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952422618865967, Accuracy = 0.9180437326431274\n","Iter #11704320:  Learning rate = 0.000002:   Batch Loss = 0.579629, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952354669570923, Accuracy = 0.9177098870277405\n","Iter #11705344:  Learning rate = 0.000002:   Batch Loss = 0.564209, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952008366584778, Accuracy = 0.9173760414123535\n","Iter #11706368:  Learning rate = 0.000002:   Batch Loss = 0.567287, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951815247535706, Accuracy = 0.9173760414123535\n","Iter #11707392:  Learning rate = 0.000002:   Batch Loss = 0.637726, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951592922210693, Accuracy = 0.9172091484069824\n","Iter #11708416:  Learning rate = 0.000002:   Batch Loss = 0.534102, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951594710350037, Accuracy = 0.9172091484069824\n","Iter #11709440:  Learning rate = 0.000002:   Batch Loss = 0.554661, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951485633850098, Accuracy = 0.9173760414123535\n","Iter #11710464:  Learning rate = 0.000002:   Batch Loss = 0.536591, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951822996139526, Accuracy = 0.9175429940223694\n","Iter #11711488:  Learning rate = 0.000002:   Batch Loss = 0.542515, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048301696777, Accuracy = 0.9178768396377563\n","Iter #11712512:  Learning rate = 0.000002:   Batch Loss = 0.537575, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952075719833374, Accuracy = 0.9177098870277405\n","Iter #11713536:  Learning rate = 0.000002:   Batch Loss = 0.522927, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952203869819641, Accuracy = 0.9180437326431274\n","Iter #11714560:  Learning rate = 0.000002:   Batch Loss = 0.570228, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952403545379639, Accuracy = 0.9180437326431274\n","Iter #11715584:  Learning rate = 0.000002:   Batch Loss = 0.575433, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952361822128296, Accuracy = 0.9180437326431274\n","Iter #11716608:  Learning rate = 0.000002:   Batch Loss = 0.560772, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952098369598389, Accuracy = 0.9178768396377563\n","Iter #11717632:  Learning rate = 0.000002:   Batch Loss = 0.540595, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952061414718628, Accuracy = 0.9175429940223694\n","Iter #11718656:  Learning rate = 0.000002:   Batch Loss = 0.572286, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952065587043762, Accuracy = 0.9177098870277405\n","Iter #11719680:  Learning rate = 0.000002:   Batch Loss = 0.595642, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952150821685791, Accuracy = 0.9177098870277405\n","Iter #11720704:  Learning rate = 0.000002:   Batch Loss = 0.520496, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952122211456299, Accuracy = 0.9173760414123535\n","Iter #11721728:  Learning rate = 0.000002:   Batch Loss = 0.579970, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595214307308197, Accuracy = 0.9178768396377563\n","Iter #11722752:  Learning rate = 0.000002:   Batch Loss = 0.541528, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952138304710388, Accuracy = 0.9180437326431274\n","Iter #11723776:  Learning rate = 0.000002:   Batch Loss = 0.488263, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595235288143158, Accuracy = 0.9177098870277405\n","Iter #11724800:  Learning rate = 0.000002:   Batch Loss = 0.553402, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595237135887146, Accuracy = 0.9175429940223694\n","Iter #11725824:  Learning rate = 0.000002:   Batch Loss = 0.533689, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952358245849609, Accuracy = 0.9178768396377563\n","Iter #11726848:  Learning rate = 0.000002:   Batch Loss = 0.509696, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952215194702148, Accuracy = 0.9180437326431274\n","Iter #11727872:  Learning rate = 0.000002:   Batch Loss = 0.493342, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952296257019043, Accuracy = 0.9180437326431274\n","Iter #11728896:  Learning rate = 0.000002:   Batch Loss = 0.583148, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952317714691162, Accuracy = 0.9180437326431274\n","Iter #11729920:  Learning rate = 0.000002:   Batch Loss = 0.542014, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952466726303101, Accuracy = 0.9178768396377563\n","Iter #11730944:  Learning rate = 0.000002:   Batch Loss = 0.561252, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952518582344055, Accuracy = 0.9180437326431274\n","Iter #11731968:  Learning rate = 0.000002:   Batch Loss = 0.594141, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952551960945129, Accuracy = 0.9178768396377563\n","Iter #11732992:  Learning rate = 0.000002:   Batch Loss = 0.518929, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952629446983337, Accuracy = 0.9178768396377563\n","Iter #11734016:  Learning rate = 0.000002:   Batch Loss = 0.512733, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952455401420593, Accuracy = 0.9178768396377563\n","Iter #11735040:  Learning rate = 0.000002:   Batch Loss = 0.525934, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595230221748352, Accuracy = 0.9178768396377563\n","Iter #11736064:  Learning rate = 0.000002:   Batch Loss = 0.624038, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595233678817749, Accuracy = 0.9178768396377563\n","Iter #11737088:  Learning rate = 0.000002:   Batch Loss = 0.515123, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952244400978088, Accuracy = 0.9177098870277405\n","Iter #11738112:  Learning rate = 0.000002:   Batch Loss = 0.499379, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952293872833252, Accuracy = 0.9177098870277405\n","Iter #11739136:  Learning rate = 0.000002:   Batch Loss = 0.607745, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952000617980957, Accuracy = 0.9177098870277405\n","Iter #11740160:  Learning rate = 0.000002:   Batch Loss = 0.544922, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595187783241272, Accuracy = 0.9172091484069824\n","Iter #11741184:  Learning rate = 0.000002:   Batch Loss = 0.614710, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595185399055481, Accuracy = 0.9172091484069824\n","Iter #11742208:  Learning rate = 0.000002:   Batch Loss = 0.556400, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951873064041138, Accuracy = 0.9172091484069824\n","Iter #11743232:  Learning rate = 0.000002:   Batch Loss = 0.555901, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951772928237915, Accuracy = 0.9172091484069824\n","Iter #11744256:  Learning rate = 0.000002:   Batch Loss = 0.485423, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951688289642334, Accuracy = 0.9173760414123535\n","Iter #11745280:  Learning rate = 0.000002:   Batch Loss = 0.552163, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951601266860962, Accuracy = 0.9170422554016113\n","Iter #11746304:  Learning rate = 0.000002:   Batch Loss = 0.555899, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951625108718872, Accuracy = 0.9172091484069824\n","Iter #11747328:  Learning rate = 0.000002:   Batch Loss = 0.588122, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951801538467407, Accuracy = 0.9177098870277405\n","Iter #11748352:  Learning rate = 0.000002:   Batch Loss = 0.604962, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951757431030273, Accuracy = 0.9177098870277405\n","Iter #11749376:  Learning rate = 0.000002:   Batch Loss = 0.550273, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951869487762451, Accuracy = 0.9177098870277405\n","Iter #11750400:  Learning rate = 0.000002:   Batch Loss = 0.533282, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951962471008301, Accuracy = 0.9180437326431274\n","Iter #11751424:  Learning rate = 0.000002:   Batch Loss = 0.545510, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951932072639465, Accuracy = 0.9178768396377563\n","Iter #11752448:  Learning rate = 0.000002:   Batch Loss = 0.592387, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951933860778809, Accuracy = 0.9180437326431274\n","Iter #11753472:  Learning rate = 0.000002:   Batch Loss = 0.590419, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952039361000061, Accuracy = 0.9177098870277405\n","Iter #11754496:  Learning rate = 0.000002:   Batch Loss = 0.509034, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952324867248535, Accuracy = 0.9180437326431274\n","Iter #11755520:  Learning rate = 0.000002:   Batch Loss = 0.520699, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952423810958862, Accuracy = 0.9178768396377563\n","Iter #11756544:  Learning rate = 0.000002:   Batch Loss = 0.551357, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595248281955719, Accuracy = 0.9178768396377563\n","Iter #11757568:  Learning rate = 0.000002:   Batch Loss = 0.550293, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952466130256653, Accuracy = 0.9178768396377563\n","Iter #11758592:  Learning rate = 0.000002:   Batch Loss = 0.639176, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952316522598267, Accuracy = 0.9178768396377563\n","Iter #11759616:  Learning rate = 0.000002:   Batch Loss = 0.551439, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952131748199463, Accuracy = 0.9177098870277405\n","Iter #11760640:  Learning rate = 0.000002:   Batch Loss = 0.601837, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952110886573792, Accuracy = 0.9177098870277405\n","Iter #11761664:  Learning rate = 0.000002:   Batch Loss = 0.521628, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952160358428955, Accuracy = 0.9177098870277405\n","Iter #11762688:  Learning rate = 0.000002:   Batch Loss = 0.594161, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952175855636597, Accuracy = 0.9177098870277405\n","Iter #11763712:  Learning rate = 0.000002:   Batch Loss = 0.515060, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595226526260376, Accuracy = 0.9178768396377563\n","Iter #11764736:  Learning rate = 0.000002:   Batch Loss = 0.594010, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952423810958862, Accuracy = 0.9180437326431274\n","Iter #11765760:  Learning rate = 0.000002:   Batch Loss = 0.491425, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952675342559814, Accuracy = 0.9178768396377563\n","Iter #11766784:  Learning rate = 0.000002:   Batch Loss = 0.548046, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952628254890442, Accuracy = 0.9177098870277405\n","Iter #11767808:  Learning rate = 0.000002:   Batch Loss = 0.539039, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952388644218445, Accuracy = 0.9177098870277405\n","Iter #11768832:  Learning rate = 0.000002:   Batch Loss = 0.574527, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595221996307373, Accuracy = 0.9177098870277405\n","Iter #11769856:  Learning rate = 0.000002:   Batch Loss = 0.521531, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952150225639343, Accuracy = 0.9175429940223694\n","Iter #11770880:  Learning rate = 0.000002:   Batch Loss = 0.550451, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951969027519226, Accuracy = 0.9175429940223694\n","Iter #11771904:  Learning rate = 0.000002:   Batch Loss = 0.570245, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951879024505615, Accuracy = 0.9175429940223694\n","Iter #11772928:  Learning rate = 0.000002:   Batch Loss = 0.557017, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951985120773315, Accuracy = 0.9175429940223694\n","Iter #11773952:  Learning rate = 0.000002:   Batch Loss = 0.530648, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952141284942627, Accuracy = 0.9173760414123535\n","Iter #11774976:  Learning rate = 0.000002:   Batch Loss = 0.479877, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952234268188477, Accuracy = 0.9170422554016113\n","Iter #11776000:  Learning rate = 0.000002:   Batch Loss = 0.550254, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595227837562561, Accuracy = 0.9172091484069824\n","Iter #11777024:  Learning rate = 0.000002:   Batch Loss = 0.487202, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952154397964478, Accuracy = 0.9177098870277405\n","Iter #11778048:  Learning rate = 0.000002:   Batch Loss = 0.524364, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952112674713135, Accuracy = 0.9178768396377563\n","Iter #11779072:  Learning rate = 0.000002:   Batch Loss = 0.623340, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952190160751343, Accuracy = 0.9177098870277405\n","Iter #11780096:  Learning rate = 0.000002:   Batch Loss = 0.590138, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952128767967224, Accuracy = 0.9180437326431274\n","Iter #11781120:  Learning rate = 0.000002:   Batch Loss = 0.532004, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952011942863464, Accuracy = 0.9180437326431274\n","Iter #11782144:  Learning rate = 0.000002:   Batch Loss = 0.574724, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951955318450928, Accuracy = 0.9180437326431274\n","Iter #11783168:  Learning rate = 0.000002:   Batch Loss = 0.611935, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951956510543823, Accuracy = 0.9180437326431274\n","Iter #11784192:  Learning rate = 0.000002:   Batch Loss = 0.562003, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951887369155884, Accuracy = 0.9177098870277405\n","Iter #11785216:  Learning rate = 0.000002:   Batch Loss = 0.628206, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952004790306091, Accuracy = 0.9178768396377563\n","Iter #11786240:  Learning rate = 0.000002:   Batch Loss = 0.580174, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951977372169495, Accuracy = 0.9177098870277405\n","Iter #11787264:  Learning rate = 0.000002:   Batch Loss = 0.619684, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951951742172241, Accuracy = 0.9177098870277405\n","Iter #11788288:  Learning rate = 0.000002:   Batch Loss = 0.625906, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952190160751343, Accuracy = 0.9178768396377563\n","Iter #11789312:  Learning rate = 0.000002:   Batch Loss = 0.592877, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952394008636475, Accuracy = 0.9180437326431274\n","Iter #11790336:  Learning rate = 0.000002:   Batch Loss = 0.489673, Accuracy = 0.9921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952385663986206, Accuracy = 0.9178768396377563\n","Iter #11791360:  Learning rate = 0.000002:   Batch Loss = 0.530562, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952309966087341, Accuracy = 0.9182106256484985\n","Iter #11792384:  Learning rate = 0.000002:   Batch Loss = 0.505611, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952318906784058, Accuracy = 0.9182106256484985\n","Iter #11793408:  Learning rate = 0.000002:   Batch Loss = 0.530535, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952250957489014, Accuracy = 0.9180437326431274\n","Iter #11794432:  Learning rate = 0.000002:   Batch Loss = 0.561360, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952215194702148, Accuracy = 0.9178768396377563\n","Iter #11795456:  Learning rate = 0.000002:   Batch Loss = 0.631829, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952075719833374, Accuracy = 0.9178768396377563\n","Iter #11796480:  Learning rate = 0.000002:   Batch Loss = 0.527325, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952227115631104, Accuracy = 0.9178768396377563\n","Iter #11797504:  Learning rate = 0.000002:   Batch Loss = 0.595117, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952275395393372, Accuracy = 0.9178768396377563\n","Iter #11798528:  Learning rate = 0.000002:   Batch Loss = 0.551080, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952188372612, Accuracy = 0.9178768396377563\n","Iter #11799552:  Learning rate = 0.000002:   Batch Loss = 0.622474, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952155590057373, Accuracy = 0.9178768396377563\n","Iter #11800576:  Learning rate = 0.000002:   Batch Loss = 0.512935, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952081680297852, Accuracy = 0.9178768396377563\n","Iter #11801600:  Learning rate = 0.000002:   Batch Loss = 0.611939, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952081680297852, Accuracy = 0.9175429940223694\n","Iter #11802624:  Learning rate = 0.000002:   Batch Loss = 0.580728, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952258110046387, Accuracy = 0.9178768396377563\n","Iter #11803648:  Learning rate = 0.000002:   Batch Loss = 0.557720, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952320694923401, Accuracy = 0.9178768396377563\n","Iter #11804672:  Learning rate = 0.000002:   Batch Loss = 0.544788, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952268838882446, Accuracy = 0.9178768396377563\n","Iter #11805696:  Learning rate = 0.000002:   Batch Loss = 0.549701, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952248573303223, Accuracy = 0.9178768396377563\n","Iter #11806720:  Learning rate = 0.000002:   Batch Loss = 0.507803, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952162742614746, Accuracy = 0.9180437326431274\n","Iter #11807744:  Learning rate = 0.000002:   Batch Loss = 0.507240, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952001810073853, Accuracy = 0.9180437326431274\n","Iter #11808768:  Learning rate = 0.000002:   Batch Loss = 0.489653, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952067375183105, Accuracy = 0.9178768396377563\n","Iter #11809792:  Learning rate = 0.000002:   Batch Loss = 0.547527, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952023267745972, Accuracy = 0.9177098870277405\n","Iter #11810816:  Learning rate = 0.000002:   Batch Loss = 0.579998, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951839685440063, Accuracy = 0.9178768396377563\n","Iter #11811840:  Learning rate = 0.000002:   Batch Loss = 0.594581, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951695442199707, Accuracy = 0.9177098870277405\n","Iter #11812864:  Learning rate = 0.000002:   Batch Loss = 0.556112, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951817035675049, Accuracy = 0.9175429940223694\n","Iter #11813888:  Learning rate = 0.000002:   Batch Loss = 0.578594, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952006578445435, Accuracy = 0.9175429940223694\n","Iter #11814912:  Learning rate = 0.000002:   Batch Loss = 0.517116, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951990485191345, Accuracy = 0.9177098870277405\n","Iter #11815936:  Learning rate = 0.000002:   Batch Loss = 0.553314, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952128171920776, Accuracy = 0.9177098870277405\n","Iter #11816960:  Learning rate = 0.000002:   Batch Loss = 0.554382, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952080488204956, Accuracy = 0.9175429940223694\n","Iter #11817984:  Learning rate = 0.000002:   Batch Loss = 0.604328, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952118635177612, Accuracy = 0.9178768396377563\n","Iter #11819008:  Learning rate = 0.000002:   Batch Loss = 0.615128, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073335647583, Accuracy = 0.9180437326431274\n","Iter #11820032:  Learning rate = 0.000002:   Batch Loss = 0.550886, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595189094543457, Accuracy = 0.9177098870277405\n","Iter #11821056:  Learning rate = 0.000002:   Batch Loss = 0.516416, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951727628707886, Accuracy = 0.9173760414123535\n","Iter #11822080:  Learning rate = 0.000002:   Batch Loss = 0.576893, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951602458953857, Accuracy = 0.9173760414123535\n","Iter #11823104:  Learning rate = 0.000002:   Batch Loss = 0.562497, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951607823371887, Accuracy = 0.9173760414123535\n","Iter #11824128:  Learning rate = 0.000002:   Batch Loss = 0.470453, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951535701751709, Accuracy = 0.9170422554016113\n","Iter #11825152:  Learning rate = 0.000002:   Batch Loss = 0.593229, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951516628265381, Accuracy = 0.9173760414123535\n","Iter #11826176:  Learning rate = 0.000002:   Batch Loss = 0.508212, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951579809188843, Accuracy = 0.9173760414123535\n","Iter #11827200:  Learning rate = 0.000002:   Batch Loss = 0.553137, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951740741729736, Accuracy = 0.9172091484069824\n","Iter #11828224:  Learning rate = 0.000002:   Batch Loss = 0.569661, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951933860778809, Accuracy = 0.9177098870277405\n","Iter #11829248:  Learning rate = 0.000002:   Batch Loss = 0.498649, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952022075653076, Accuracy = 0.9175429940223694\n","Iter #11830272:  Learning rate = 0.000002:   Batch Loss = 0.512553, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029228210449, Accuracy = 0.9175429940223694\n","Iter #11831296:  Learning rate = 0.000002:   Batch Loss = 0.592377, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951985120773315, Accuracy = 0.9178768396377563\n","Iter #11832320:  Learning rate = 0.000002:   Batch Loss = 0.657124, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951987504959106, Accuracy = 0.9178768396377563\n","Iter #11833344:  Learning rate = 0.000002:   Batch Loss = 0.509719, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951983332633972, Accuracy = 0.9178768396377563\n","Iter #11834368:  Learning rate = 0.000002:   Batch Loss = 0.587454, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951968431472778, Accuracy = 0.9178768396377563\n","Iter #11835392:  Learning rate = 0.000002:   Batch Loss = 0.578619, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952004194259644, Accuracy = 0.9177098870277405\n","Iter #11836416:  Learning rate = 0.000002:   Batch Loss = 0.536034, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595197319984436, Accuracy = 0.9177098870277405\n","Iter #11837440:  Learning rate = 0.000002:   Batch Loss = 0.595691, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951820015907288, Accuracy = 0.9175429940223694\n","Iter #11838464:  Learning rate = 0.000002:   Batch Loss = 0.613001, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951669216156006, Accuracy = 0.9177098870277405\n","Iter #11839488:  Learning rate = 0.000002:   Batch Loss = 0.565781, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595170795917511, Accuracy = 0.9175429940223694\n","Iter #11840512:  Learning rate = 0.000002:   Batch Loss = 0.561906, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951693058013916, Accuracy = 0.9178768396377563\n","Iter #11841536:  Learning rate = 0.000002:   Batch Loss = 0.576273, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951707363128662, Accuracy = 0.9175429940223694\n","Iter #11842560:  Learning rate = 0.000002:   Batch Loss = 0.590298, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951794981956482, Accuracy = 0.9178768396377563\n","Iter #11843584:  Learning rate = 0.000002:   Batch Loss = 0.633265, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951906442642212, Accuracy = 0.9178768396377563\n","Iter #11844608:  Learning rate = 0.000002:   Batch Loss = 0.540659, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988697052002, Accuracy = 0.9178768396377563\n","Iter #11845632:  Learning rate = 0.000002:   Batch Loss = 0.562884, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952060222625732, Accuracy = 0.9178768396377563\n","Iter #11846656:  Learning rate = 0.000002:   Batch Loss = 0.504816, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952059030532837, Accuracy = 0.9178768396377563\n","Iter #11847680:  Learning rate = 0.000002:   Batch Loss = 0.520992, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951847434043884, Accuracy = 0.9173760414123535\n","Iter #11848704:  Learning rate = 0.000002:   Batch Loss = 0.491409, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595172643661499, Accuracy = 0.9175429940223694\n","Iter #11849728:  Learning rate = 0.000002:   Batch Loss = 0.537781, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951898694038391, Accuracy = 0.9172091484069824\n","Iter #11850752:  Learning rate = 0.000002:   Batch Loss = 0.579901, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951991081237793, Accuracy = 0.9172091484069824\n","Iter #11851776:  Learning rate = 0.000002:   Batch Loss = 0.516089, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952267646789551, Accuracy = 0.9172091484069824\n","Iter #11852800:  Learning rate = 0.000002:   Batch Loss = 0.517778, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952498912811279, Accuracy = 0.9175429940223694\n","Iter #11853824:  Learning rate = 0.000002:   Batch Loss = 0.629480, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952557325363159, Accuracy = 0.9177098870277405\n","Iter #11854848:  Learning rate = 0.000002:   Batch Loss = 0.663012, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952535271644592, Accuracy = 0.9177098870277405\n","Iter #11855872:  Learning rate = 0.000002:   Batch Loss = 0.583034, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952392816543579, Accuracy = 0.9178768396377563\n","Iter #11856896:  Learning rate = 0.000002:   Batch Loss = 0.555514, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952222347259521, Accuracy = 0.9180437326431274\n","Iter #11857920:  Learning rate = 0.000002:   Batch Loss = 0.508529, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952081680297852, Accuracy = 0.9180437326431274\n","Iter #11858944:  Learning rate = 0.000002:   Batch Loss = 0.567318, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951911211013794, Accuracy = 0.9178768396377563\n","Iter #11859968:  Learning rate = 0.000002:   Batch Loss = 0.565234, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951800346374512, Accuracy = 0.9177098870277405\n","Iter #11860992:  Learning rate = 0.000002:   Batch Loss = 0.516043, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951739549636841, Accuracy = 0.9173760414123535\n","Iter #11862016:  Learning rate = 0.000002:   Batch Loss = 0.551157, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951776504516602, Accuracy = 0.9175429940223694\n","Iter #11863040:  Learning rate = 0.000002:   Batch Loss = 0.575968, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951945185661316, Accuracy = 0.9180437326431274\n","Iter #11864064:  Learning rate = 0.000002:   Batch Loss = 0.635756, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952028036117554, Accuracy = 0.9180437326431274\n","Iter #11865088:  Learning rate = 0.000002:   Batch Loss = 0.537833, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595204770565033, Accuracy = 0.9178768396377563\n","Iter #11866112:  Learning rate = 0.000002:   Batch Loss = 0.527226, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951982736587524, Accuracy = 0.9178768396377563\n","Iter #11867136:  Learning rate = 0.000002:   Batch Loss = 0.617205, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952070951461792, Accuracy = 0.9180437326431274\n","Iter #11868160:  Learning rate = 0.000002:   Batch Loss = 0.583279, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952129364013672, Accuracy = 0.9182106256484985\n","Iter #11869184:  Learning rate = 0.000002:   Batch Loss = 0.626337, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951995849609375, Accuracy = 0.9178768396377563\n","Iter #11870208:  Learning rate = 0.000002:   Batch Loss = 0.522988, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029228210449, Accuracy = 0.9178768396377563\n","Iter #11871232:  Learning rate = 0.000002:   Batch Loss = 0.608911, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952134132385254, Accuracy = 0.9177098870277405\n","Iter #11872256:  Learning rate = 0.000002:   Batch Loss = 0.557385, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952194929122925, Accuracy = 0.9175429940223694\n","Iter #11873280:  Learning rate = 0.000002:   Batch Loss = 0.516068, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952410697937012, Accuracy = 0.9177098870277405\n","Iter #11874304:  Learning rate = 0.000002:   Batch Loss = 0.474477, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952417850494385, Accuracy = 0.9175429940223694\n","Iter #11875328:  Learning rate = 0.000002:   Batch Loss = 0.543008, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952315330505371, Accuracy = 0.9178768396377563\n","Iter #11876352:  Learning rate = 0.000002:   Batch Loss = 0.543925, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952220559120178, Accuracy = 0.9177098870277405\n","Iter #11877376:  Learning rate = 0.000002:   Batch Loss = 0.590295, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952044725418091, Accuracy = 0.9177098870277405\n","Iter #11878400:  Learning rate = 0.000002:   Batch Loss = 0.560053, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952054262161255, Accuracy = 0.9173760414123535\n","Iter #11879424:  Learning rate = 0.000002:   Batch Loss = 0.545341, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952188968658447, Accuracy = 0.9178768396377563\n","Iter #11880448:  Learning rate = 0.000002:   Batch Loss = 0.604787, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952208042144775, Accuracy = 0.9178768396377563\n","Iter #11881472:  Learning rate = 0.000002:   Batch Loss = 0.532252, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952300429344177, Accuracy = 0.9178768396377563\n","Iter #11882496:  Learning rate = 0.000002:   Batch Loss = 0.559964, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952205657958984, Accuracy = 0.9178768396377563\n","Iter #11883520:  Learning rate = 0.000002:   Batch Loss = 0.526943, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156782150269, Accuracy = 0.9177098870277405\n","Iter #11884544:  Learning rate = 0.000002:   Batch Loss = 0.571297, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952145457267761, Accuracy = 0.9178768396377563\n","Iter #11885568:  Learning rate = 0.000002:   Batch Loss = 0.546751, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952010154724121, Accuracy = 0.9178768396377563\n","Iter #11886592:  Learning rate = 0.000002:   Batch Loss = 0.529551, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951969623565674, Accuracy = 0.9178768396377563\n","Iter #11887616:  Learning rate = 0.000002:   Batch Loss = 0.537029, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951894521713257, Accuracy = 0.9175429940223694\n","Iter #11888640:  Learning rate = 0.000002:   Batch Loss = 0.592778, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951763987541199, Accuracy = 0.9173760414123535\n","Iter #11889664:  Learning rate = 0.000002:   Batch Loss = 0.618540, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595184862613678, Accuracy = 0.9175429940223694\n","Iter #11890688:  Learning rate = 0.000002:   Batch Loss = 0.576755, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952012538909912, Accuracy = 0.9178768396377563\n","Iter #11891712:  Learning rate = 0.000002:   Batch Loss = 0.614260, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952061414718628, Accuracy = 0.9178768396377563\n","Iter #11892736:  Learning rate = 0.000002:   Batch Loss = 0.542528, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952117443084717, Accuracy = 0.9178768396377563\n","Iter #11893760:  Learning rate = 0.000002:   Batch Loss = 0.556480, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952243804931641, Accuracy = 0.9182106256484985\n","Iter #11894784:  Learning rate = 0.000002:   Batch Loss = 0.597146, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952416658401489, Accuracy = 0.9180437326431274\n","Iter #11895808:  Learning rate = 0.000002:   Batch Loss = 0.589558, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952436327934265, Accuracy = 0.9180437326431274\n","Iter #11896832:  Learning rate = 0.000002:   Batch Loss = 0.558249, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595241904258728, Accuracy = 0.9180437326431274\n","Iter #11897856:  Learning rate = 0.000002:   Batch Loss = 0.568358, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156782150269, Accuracy = 0.9178768396377563\n","Iter #11898880:  Learning rate = 0.000002:   Batch Loss = 0.558295, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595194399356842, Accuracy = 0.9177098870277405\n","Iter #11899904:  Learning rate = 0.000002:   Batch Loss = 0.518458, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951857566833496, Accuracy = 0.9177098870277405\n","Iter #11900928:  Learning rate = 0.000002:   Batch Loss = 0.549598, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951772332191467, Accuracy = 0.9177098870277405\n","Iter #11901952:  Learning rate = 0.000002:   Batch Loss = 0.588689, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951951146125793, Accuracy = 0.9177098870277405\n","Iter #11902976:  Learning rate = 0.000002:   Batch Loss = 0.581880, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952001810073853, Accuracy = 0.9178768396377563\n","Iter #11904000:  Learning rate = 0.000002:   Batch Loss = 0.555733, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951952934265137, Accuracy = 0.9180437326431274\n","Iter #11905024:  Learning rate = 0.000002:   Batch Loss = 0.512249, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951920747756958, Accuracy = 0.9178768396377563\n","Iter #11906048:  Learning rate = 0.000002:   Batch Loss = 0.554208, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595206618309021, Accuracy = 0.9178768396377563\n","Iter #11907072:  Learning rate = 0.000002:   Batch Loss = 0.580831, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951924324035645, Accuracy = 0.9180437326431274\n","Iter #11908096:  Learning rate = 0.000002:   Batch Loss = 0.537162, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951822400093079, Accuracy = 0.9177098870277405\n","Iter #11909120:  Learning rate = 0.000002:   Batch Loss = 0.517900, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951803922653198, Accuracy = 0.9178768396377563\n","Iter #11910144:  Learning rate = 0.000002:   Batch Loss = 0.547375, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951821804046631, Accuracy = 0.9178768396377563\n","Iter #11911168:  Learning rate = 0.000002:   Batch Loss = 0.593866, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988697052002, Accuracy = 0.9180437326431274\n","Iter #11912192:  Learning rate = 0.000002:   Batch Loss = 0.488245, Accuracy = 1.0\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952175259590149, Accuracy = 0.9178768396377563\n","Iter #11913216:  Learning rate = 0.000002:   Batch Loss = 0.536635, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952111482620239, Accuracy = 0.9180437326431274\n","Iter #11914240:  Learning rate = 0.000002:   Batch Loss = 0.569647, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952135324478149, Accuracy = 0.9180437326431274\n","Iter #11915264:  Learning rate = 0.000002:   Batch Loss = 0.513147, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952249765396118, Accuracy = 0.9180437326431274\n","Iter #11916288:  Learning rate = 0.000002:   Batch Loss = 0.556979, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952111482620239, Accuracy = 0.9178768396377563\n","Iter #11917312:  Learning rate = 0.000002:   Batch Loss = 0.527771, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952029824256897, Accuracy = 0.9178768396377563\n","Iter #11918336:  Learning rate = 0.000002:   Batch Loss = 0.563854, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951811671257019, Accuracy = 0.9175429940223694\n","Iter #11919360:  Learning rate = 0.000002:   Batch Loss = 0.515346, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951807498931885, Accuracy = 0.9173760414123535\n","Iter #11920384:  Learning rate = 0.000002:   Batch Loss = 0.526863, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951806902885437, Accuracy = 0.9175429940223694\n","Iter #11921408:  Learning rate = 0.000002:   Batch Loss = 0.630691, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951921939849854, Accuracy = 0.9178768396377563\n","Iter #11922432:  Learning rate = 0.000002:   Batch Loss = 0.487677, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952011346817017, Accuracy = 0.9180437326431274\n","Iter #11923456:  Learning rate = 0.000002:   Batch Loss = 0.577735, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952049493789673, Accuracy = 0.9180437326431274\n","Iter #11924480:  Learning rate = 0.000002:   Batch Loss = 0.562112, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951999425888062, Accuracy = 0.9180437326431274\n","Iter #11925504:  Learning rate = 0.000002:   Batch Loss = 0.539086, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951898097991943, Accuracy = 0.9178768396377563\n","Iter #11926528:  Learning rate = 0.000002:   Batch Loss = 0.577725, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951855182647705, Accuracy = 0.9177098870277405\n","Iter #11927552:  Learning rate = 0.000002:   Batch Loss = 0.550184, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951861143112183, Accuracy = 0.9175429940223694\n","Iter #11928576:  Learning rate = 0.000002:   Batch Loss = 0.524839, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951917171478271, Accuracy = 0.9178768396377563\n","Iter #11929600:  Learning rate = 0.000002:   Batch Loss = 0.552834, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951979756355286, Accuracy = 0.9177098870277405\n","Iter #11930624:  Learning rate = 0.000002:   Batch Loss = 0.545174, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951995849609375, Accuracy = 0.9175429940223694\n","Iter #11931648:  Learning rate = 0.000002:   Batch Loss = 0.567558, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951952934265137, Accuracy = 0.9173760414123535\n","Iter #11932672:  Learning rate = 0.000002:   Batch Loss = 0.537447, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951853394508362, Accuracy = 0.9173760414123535\n","Iter #11933696:  Learning rate = 0.000002:   Batch Loss = 0.587176, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952069163322449, Accuracy = 0.9175429940223694\n","Iter #11934720:  Learning rate = 0.000002:   Batch Loss = 0.575041, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952244997024536, Accuracy = 0.9177098870277405\n","Iter #11935744:  Learning rate = 0.000002:   Batch Loss = 0.569834, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595231831073761, Accuracy = 0.9175429940223694\n","Iter #11936768:  Learning rate = 0.000002:   Batch Loss = 0.539605, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952556133270264, Accuracy = 0.9173760414123535\n","Iter #11937792:  Learning rate = 0.000002:   Batch Loss = 0.614323, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952698588371277, Accuracy = 0.9173760414123535\n","Iter #11938816:  Learning rate = 0.000002:   Batch Loss = 0.582539, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952589511871338, Accuracy = 0.9175429940223694\n","Iter #11939840:  Learning rate = 0.000002:   Batch Loss = 0.585980, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952330827713013, Accuracy = 0.9178768396377563\n","Iter #11940864:  Learning rate = 0.000001:   Batch Loss = 0.631329, Accuracy = 0.875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952097177505493, Accuracy = 0.9177098870277405\n","Iter #11941888:  Learning rate = 0.000001:   Batch Loss = 0.625882, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952078104019165, Accuracy = 0.9178768396377563\n","Iter #11942912:  Learning rate = 0.000001:   Batch Loss = 0.582697, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951981544494629, Accuracy = 0.9182106256484985\n","Iter #11943936:  Learning rate = 0.000001:   Batch Loss = 0.615410, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952051877975464, Accuracy = 0.9182106256484985\n","Iter #11944960:  Learning rate = 0.000001:   Batch Loss = 0.590085, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952372550964355, Accuracy = 0.9180437326431274\n","Iter #11945984:  Learning rate = 0.000001:   Batch Loss = 0.552734, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952601432800293, Accuracy = 0.9180437326431274\n","Iter #11947008:  Learning rate = 0.000001:   Batch Loss = 0.610436, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952655076980591, Accuracy = 0.9178768396377563\n","Iter #11948032:  Learning rate = 0.000001:   Batch Loss = 0.555747, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952460169792175, Accuracy = 0.9180437326431274\n","Iter #11949056:  Learning rate = 0.000001:   Batch Loss = 0.579593, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952309370040894, Accuracy = 0.9182106256484985\n","Iter #11950080:  Learning rate = 0.000001:   Batch Loss = 0.594437, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595201849937439, Accuracy = 0.9182106256484985\n","Iter #11951104:  Learning rate = 0.000001:   Batch Loss = 0.541387, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951807498931885, Accuracy = 0.9178768396377563\n","Iter #11952128:  Learning rate = 0.000001:   Batch Loss = 0.546655, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951694250106812, Accuracy = 0.9175429940223694\n","Iter #11953152:  Learning rate = 0.000001:   Batch Loss = 0.686630, Accuracy = 0.890625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951811075210571, Accuracy = 0.9178768396377563\n","Iter #11954176:  Learning rate = 0.000001:   Batch Loss = 0.572576, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951765775680542, Accuracy = 0.9177098870277405\n","Iter #11955200:  Learning rate = 0.000001:   Batch Loss = 0.545483, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951882600784302, Accuracy = 0.9177098870277405\n","Iter #11956224:  Learning rate = 0.000001:   Batch Loss = 0.643823, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951826572418213, Accuracy = 0.9177098870277405\n","Iter #11957248:  Learning rate = 0.000001:   Batch Loss = 0.573713, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951887369155884, Accuracy = 0.9177098870277405\n","Iter #11958272:  Learning rate = 0.000001:   Batch Loss = 0.545721, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595185399055481, Accuracy = 0.9177098870277405\n","Iter #11959296:  Learning rate = 0.000001:   Batch Loss = 0.551904, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951732397079468, Accuracy = 0.9173760414123535\n","Iter #11960320:  Learning rate = 0.000001:   Batch Loss = 0.509228, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951746702194214, Accuracy = 0.9173760414123535\n","Iter #11961344:  Learning rate = 0.000001:   Batch Loss = 0.570205, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951790809631348, Accuracy = 0.9170422554016113\n","Iter #11962368:  Learning rate = 0.000001:   Batch Loss = 0.591479, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951862335205078, Accuracy = 0.9173760414123535\n","Iter #11963392:  Learning rate = 0.000001:   Batch Loss = 0.543147, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951945185661316, Accuracy = 0.9177098870277405\n","Iter #11964416:  Learning rate = 0.000001:   Batch Loss = 0.531141, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951962471008301, Accuracy = 0.9178768396377563\n","Iter #11965440:  Learning rate = 0.000001:   Batch Loss = 0.531621, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988101005554, Accuracy = 0.9178768396377563\n","Iter #11966464:  Learning rate = 0.000001:   Batch Loss = 0.516376, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595203161239624, Accuracy = 0.9178768396377563\n","Iter #11967488:  Learning rate = 0.000001:   Batch Loss = 0.590741, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952079892158508, Accuracy = 0.9177098870277405\n","Iter #11968512:  Learning rate = 0.000001:   Batch Loss = 0.515324, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595203161239624, Accuracy = 0.9177098870277405\n","Iter #11969536:  Learning rate = 0.000001:   Batch Loss = 0.595672, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952069759368896, Accuracy = 0.9177098870277405\n","Iter #11970560:  Learning rate = 0.000001:   Batch Loss = 0.583618, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951992273330688, Accuracy = 0.9178768396377563\n","Iter #11971584:  Learning rate = 0.000001:   Batch Loss = 0.610032, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951824188232422, Accuracy = 0.9177098870277405\n","Iter #11972608:  Learning rate = 0.000001:   Batch Loss = 0.583400, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951874852180481, Accuracy = 0.9180437326431274\n","Iter #11973632:  Learning rate = 0.000001:   Batch Loss = 0.577380, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951966047286987, Accuracy = 0.9178768396377563\n","Iter #11974656:  Learning rate = 0.000001:   Batch Loss = 0.556739, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595196008682251, Accuracy = 0.9178768396377563\n","Iter #11975680:  Learning rate = 0.000001:   Batch Loss = 0.569480, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595191478729248, Accuracy = 0.9177098870277405\n","Iter #11976704:  Learning rate = 0.000001:   Batch Loss = 0.544330, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595179557800293, Accuracy = 0.9173760414123535\n","Iter #11977728:  Learning rate = 0.000001:   Batch Loss = 0.516146, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951840877532959, Accuracy = 0.9175429940223694\n","Iter #11978752:  Learning rate = 0.000001:   Batch Loss = 0.578539, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951747894287109, Accuracy = 0.9173760414123535\n","Iter #11979776:  Learning rate = 0.000001:   Batch Loss = 0.542841, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595181405544281, Accuracy = 0.9173760414123535\n","Iter #11980800:  Learning rate = 0.000001:   Batch Loss = 0.569620, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952134132385254, Accuracy = 0.9175429940223694\n","Iter #11981824:  Learning rate = 0.000001:   Batch Loss = 0.512596, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952335596084595, Accuracy = 0.9178768396377563\n","Iter #11982848:  Learning rate = 0.000001:   Batch Loss = 0.574285, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952288508415222, Accuracy = 0.9177098870277405\n","Iter #11983872:  Learning rate = 0.000001:   Batch Loss = 0.548924, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952193737030029, Accuracy = 0.9177098870277405\n","Iter #11984896:  Learning rate = 0.000001:   Batch Loss = 0.540587, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952089428901672, Accuracy = 0.9175429940223694\n","Iter #11985920:  Learning rate = 0.000001:   Batch Loss = 0.480131, Accuracy = 0.9921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952075123786926, Accuracy = 0.9175429940223694\n","Iter #11986944:  Learning rate = 0.000001:   Batch Loss = 0.541827, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952206254005432, Accuracy = 0.9177098870277405\n","Iter #11987968:  Learning rate = 0.000001:   Batch Loss = 0.529004, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952156782150269, Accuracy = 0.9175429940223694\n","Iter #11988992:  Learning rate = 0.000001:   Batch Loss = 0.525605, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952231884002686, Accuracy = 0.9177098870277405\n","Iter #11990016:  Learning rate = 0.000001:   Batch Loss = 0.553927, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952210426330566, Accuracy = 0.9178768396377563\n","Iter #11991040:  Learning rate = 0.000001:   Batch Loss = 0.509398, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952179431915283, Accuracy = 0.9180437326431274\n","Iter #11992064:  Learning rate = 0.000001:   Batch Loss = 0.575837, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952221155166626, Accuracy = 0.9180437326431274\n","Iter #11993088:  Learning rate = 0.000001:   Batch Loss = 0.566936, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952333211898804, Accuracy = 0.9178768396377563\n","Iter #11994112:  Learning rate = 0.000001:   Batch Loss = 0.601705, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952407717704773, Accuracy = 0.9180437326431274\n","Iter #11995136:  Learning rate = 0.000001:   Batch Loss = 0.550092, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952568054199219, Accuracy = 0.9180437326431274\n","Iter #11996160:  Learning rate = 0.000001:   Batch Loss = 0.614991, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595252275466919, Accuracy = 0.9178768396377563\n","Iter #11997184:  Learning rate = 0.000001:   Batch Loss = 0.514143, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952411890029907, Accuracy = 0.9177098870277405\n","Iter #11998208:  Learning rate = 0.000001:   Batch Loss = 0.544259, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952308177947998, Accuracy = 0.9178768396377563\n","Iter #11999232:  Learning rate = 0.000001:   Batch Loss = 0.612484, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952298641204834, Accuracy = 0.9178768396377563\n","Iter #12000256:  Learning rate = 0.000001:   Batch Loss = 0.549815, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952310562133789, Accuracy = 0.9180437326431274\n","Iter #12001280:  Learning rate = 0.000001:   Batch Loss = 0.589172, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952116847038269, Accuracy = 0.9178768396377563\n","Iter #12002304:  Learning rate = 0.000001:   Batch Loss = 0.541571, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951900482177734, Accuracy = 0.9177098870277405\n","Iter #12003328:  Learning rate = 0.000001:   Batch Loss = 0.541051, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951802730560303, Accuracy = 0.9177098870277405\n","Iter #12004352:  Learning rate = 0.000001:   Batch Loss = 0.536421, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951859951019287, Accuracy = 0.9180437326431274\n","Iter #12005376:  Learning rate = 0.000001:   Batch Loss = 0.539265, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951669216156006, Accuracy = 0.9175429940223694\n","Iter #12006400:  Learning rate = 0.000001:   Batch Loss = 0.581299, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951510667800903, Accuracy = 0.9175429940223694\n","Iter #12007424:  Learning rate = 0.000001:   Batch Loss = 0.583283, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951540470123291, Accuracy = 0.9175429940223694\n","Iter #12008448:  Learning rate = 0.000001:   Batch Loss = 0.614165, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951725244522095, Accuracy = 0.9175429940223694\n","Iter #12009472:  Learning rate = 0.000001:   Batch Loss = 0.528982, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952046513557434, Accuracy = 0.9178768396377563\n","Iter #12010496:  Learning rate = 0.000001:   Batch Loss = 0.609488, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952324867248535, Accuracy = 0.9178768396377563\n","Iter #12011520:  Learning rate = 0.000001:   Batch Loss = 0.549910, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595244824886322, Accuracy = 0.9180437326431274\n","Iter #12012544:  Learning rate = 0.000001:   Batch Loss = 0.522506, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952421426773071, Accuracy = 0.9178768396377563\n","Iter #12013568:  Learning rate = 0.000001:   Batch Loss = 0.557742, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952231884002686, Accuracy = 0.9178768396377563\n","Iter #12014592:  Learning rate = 0.000001:   Batch Loss = 0.520303, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952064394950867, Accuracy = 0.9178768396377563\n","Iter #12015616:  Learning rate = 0.000001:   Batch Loss = 0.510171, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951845645904541, Accuracy = 0.9177098870277405\n","Iter #12016640:  Learning rate = 0.000001:   Batch Loss = 0.577329, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951827168464661, Accuracy = 0.9178768396377563\n","Iter #12017664:  Learning rate = 0.000001:   Batch Loss = 0.573314, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952022671699524, Accuracy = 0.9177098870277405\n","Iter #12018688:  Learning rate = 0.000001:   Batch Loss = 0.589243, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048301696777, Accuracy = 0.9178768396377563\n","Iter #12019712:  Learning rate = 0.000001:   Batch Loss = 0.570119, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952082872390747, Accuracy = 0.9178768396377563\n","Iter #12020736:  Learning rate = 0.000001:   Batch Loss = 0.573287, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952110290527344, Accuracy = 0.9178768396377563\n","Iter #12021760:  Learning rate = 0.000001:   Batch Loss = 0.621460, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951926708221436, Accuracy = 0.9178768396377563\n","Iter #12022784:  Learning rate = 0.000001:   Batch Loss = 0.578115, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951697826385498, Accuracy = 0.9173760414123535\n","Iter #12023808:  Learning rate = 0.000001:   Batch Loss = 0.551833, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951722860336304, Accuracy = 0.9175429940223694\n","Iter #12024832:  Learning rate = 0.000001:   Batch Loss = 0.574401, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951839685440063, Accuracy = 0.9175429940223694\n","Iter #12025856:  Learning rate = 0.000001:   Batch Loss = 0.557120, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951826572418213, Accuracy = 0.9173760414123535\n","Iter #12026880:  Learning rate = 0.000001:   Batch Loss = 0.592649, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951793789863586, Accuracy = 0.9173760414123535\n","Iter #12027904:  Learning rate = 0.000001:   Batch Loss = 0.482537, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595177173614502, Accuracy = 0.9173760414123535\n","Iter #12028928:  Learning rate = 0.000001:   Batch Loss = 0.562505, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951693058013916, Accuracy = 0.9173760414123535\n","Iter #12029952:  Learning rate = 0.000001:   Batch Loss = 0.502588, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951704978942871, Accuracy = 0.9175429940223694\n","Iter #12030976:  Learning rate = 0.000001:   Batch Loss = 0.503899, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951602458953857, Accuracy = 0.9175429940223694\n","Iter #12032000:  Learning rate = 0.000001:   Batch Loss = 0.526619, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951576232910156, Accuracy = 0.9173760414123535\n","Iter #12033024:  Learning rate = 0.000001:   Batch Loss = 0.476981, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951650142669678, Accuracy = 0.9172091484069824\n","Iter #12034048:  Learning rate = 0.000001:   Batch Loss = 0.619363, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951675772666931, Accuracy = 0.9170422554016113\n","Iter #12035072:  Learning rate = 0.000001:   Batch Loss = 0.619831, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951781272888184, Accuracy = 0.9175429940223694\n","Iter #12036096:  Learning rate = 0.000001:   Batch Loss = 0.569047, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951876044273376, Accuracy = 0.9177098870277405\n","Iter #12037120:  Learning rate = 0.000001:   Batch Loss = 0.566819, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951905846595764, Accuracy = 0.9178768396377563\n","Iter #12038144:  Learning rate = 0.000001:   Batch Loss = 0.519117, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951969027519226, Accuracy = 0.9178768396377563\n","Iter #12039168:  Learning rate = 0.000001:   Batch Loss = 0.530055, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951975584030151, Accuracy = 0.9178768396377563\n","Iter #12040192:  Learning rate = 0.000001:   Batch Loss = 0.527714, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952019691467285, Accuracy = 0.9180437326431274\n","Iter #12041216:  Learning rate = 0.000001:   Batch Loss = 0.527469, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951927304267883, Accuracy = 0.9178768396377563\n","Iter #12042240:  Learning rate = 0.000001:   Batch Loss = 0.561285, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951767563819885, Accuracy = 0.9178768396377563\n","Iter #12043264:  Learning rate = 0.000001:   Batch Loss = 0.537914, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951809883117676, Accuracy = 0.9177098870277405\n","Iter #12044288:  Learning rate = 0.000001:   Batch Loss = 0.549354, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951764583587646, Accuracy = 0.9177098870277405\n","Iter #12045312:  Learning rate = 0.000001:   Batch Loss = 0.592375, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951799154281616, Accuracy = 0.9177098870277405\n","Iter #12046336:  Learning rate = 0.000001:   Batch Loss = 0.580476, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951927900314331, Accuracy = 0.9177098870277405\n","Iter #12047360:  Learning rate = 0.000001:   Batch Loss = 0.533165, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952070951461792, Accuracy = 0.9177098870277405\n","Iter #12048384:  Learning rate = 0.000001:   Batch Loss = 0.582475, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952184200286865, Accuracy = 0.9178768396377563\n","Iter #12049408:  Learning rate = 0.000001:   Batch Loss = 0.586496, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952305793762207, Accuracy = 0.9180437326431274\n","Iter #12050432:  Learning rate = 0.000001:   Batch Loss = 0.512855, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952332615852356, Accuracy = 0.9180437326431274\n","Iter #12051456:  Learning rate = 0.000001:   Batch Loss = 0.502046, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952343940734863, Accuracy = 0.9180437326431274\n","Iter #12052480:  Learning rate = 0.000001:   Batch Loss = 0.535334, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952632427215576, Accuracy = 0.9180437326431274\n","Iter #12053504:  Learning rate = 0.000001:   Batch Loss = 0.539476, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952772498130798, Accuracy = 0.9180437326431274\n","Iter #12054528:  Learning rate = 0.000001:   Batch Loss = 0.551897, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952761173248291, Accuracy = 0.9180437326431274\n","Iter #12055552:  Learning rate = 0.000001:   Batch Loss = 0.558492, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952847599983215, Accuracy = 0.9180437326431274\n","Iter #12056576:  Learning rate = 0.000001:   Batch Loss = 0.614537, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953004360198975, Accuracy = 0.9180437326431274\n","Iter #12057600:  Learning rate = 0.000001:   Batch Loss = 0.542373, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5953248739242554, Accuracy = 0.9180437326431274\n","Iter #12058624:  Learning rate = 0.000001:   Batch Loss = 0.569128, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952836871147156, Accuracy = 0.9180437326431274\n","Iter #12059648:  Learning rate = 0.000001:   Batch Loss = 0.550168, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952728986740112, Accuracy = 0.9180437326431274\n","Iter #12060672:  Learning rate = 0.000001:   Batch Loss = 0.548940, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952521562576294, Accuracy = 0.9180437326431274\n","Iter #12061696:  Learning rate = 0.000001:   Batch Loss = 0.619202, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952328443527222, Accuracy = 0.9177098870277405\n","Iter #12062720:  Learning rate = 0.000001:   Batch Loss = 0.525369, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952115058898926, Accuracy = 0.9178768396377563\n","Iter #12063744:  Learning rate = 0.000001:   Batch Loss = 0.501955, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951938629150391, Accuracy = 0.9178768396377563\n","Iter #12064768:  Learning rate = 0.000001:   Batch Loss = 0.528671, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595182478427887, Accuracy = 0.9177098870277405\n","Iter #12065792:  Learning rate = 0.000001:   Batch Loss = 0.597852, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951771140098572, Accuracy = 0.9175429940223694\n","Iter #12066816:  Learning rate = 0.000001:   Batch Loss = 0.607158, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595162570476532, Accuracy = 0.9172091484069824\n","Iter #12067840:  Learning rate = 0.000001:   Batch Loss = 0.519051, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951643586158752, Accuracy = 0.9173760414123535\n","Iter #12068864:  Learning rate = 0.000001:   Batch Loss = 0.546028, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951747894287109, Accuracy = 0.9172091484069824\n","Iter #12069888:  Learning rate = 0.000001:   Batch Loss = 0.595078, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952007174491882, Accuracy = 0.9178768396377563\n","Iter #12070912:  Learning rate = 0.000001:   Batch Loss = 0.475923, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952024459838867, Accuracy = 0.9178768396377563\n","Iter #12071936:  Learning rate = 0.000001:   Batch Loss = 0.513666, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951914191246033, Accuracy = 0.9178768396377563\n","Iter #12072960:  Learning rate = 0.000001:   Batch Loss = 0.520429, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951898097991943, Accuracy = 0.9180437326431274\n","Iter #12073984:  Learning rate = 0.000001:   Batch Loss = 0.484972, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951999425888062, Accuracy = 0.9180437326431274\n","Iter #12075008:  Learning rate = 0.000001:   Batch Loss = 0.509349, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951923131942749, Accuracy = 0.9178768396377563\n","Iter #12076032:  Learning rate = 0.000001:   Batch Loss = 0.571844, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951947569847107, Accuracy = 0.9178768396377563\n","Iter #12077056:  Learning rate = 0.000001:   Batch Loss = 0.500152, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951817631721497, Accuracy = 0.9173760414123535\n","Iter #12078080:  Learning rate = 0.000001:   Batch Loss = 0.550543, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951642394065857, Accuracy = 0.9175429940223694\n","Iter #12079104:  Learning rate = 0.000001:   Batch Loss = 0.533841, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951681137084961, Accuracy = 0.9173760414123535\n","Iter #12080128:  Learning rate = 0.000001:   Batch Loss = 0.510124, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951825380325317, Accuracy = 0.9175429940223694\n","Iter #12081152:  Learning rate = 0.000001:   Batch Loss = 0.575767, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952186584472656, Accuracy = 0.9178768396377563\n","Iter #12082176:  Learning rate = 0.000001:   Batch Loss = 0.569860, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952281355857849, Accuracy = 0.9178768396377563\n","Iter #12083200:  Learning rate = 0.000001:   Batch Loss = 0.617113, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952408313751221, Accuracy = 0.9177098870277405\n","Iter #12084224:  Learning rate = 0.000001:   Batch Loss = 0.631060, Accuracy = 0.90625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952337384223938, Accuracy = 0.9178768396377563\n","Iter #12085248:  Learning rate = 0.000001:   Batch Loss = 0.549076, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952114462852478, Accuracy = 0.9178768396377563\n","Iter #12086272:  Learning rate = 0.000001:   Batch Loss = 0.552377, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595201849937439, Accuracy = 0.9177098870277405\n","Iter #12087296:  Learning rate = 0.000001:   Batch Loss = 0.654988, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951926708221436, Accuracy = 0.9177098870277405\n","Iter #12088320:  Learning rate = 0.000001:   Batch Loss = 0.523684, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951859951019287, Accuracy = 0.9175429940223694\n","Iter #12089344:  Learning rate = 0.000001:   Batch Loss = 0.609900, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595183253288269, Accuracy = 0.9175429940223694\n","Iter #12090368:  Learning rate = 0.000001:   Batch Loss = 0.567123, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951740741729736, Accuracy = 0.9173760414123535\n","Iter #12091392:  Learning rate = 0.000001:   Batch Loss = 0.567590, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951589345932007, Accuracy = 0.9175429940223694\n","Iter #12092416:  Learning rate = 0.000001:   Batch Loss = 0.582185, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951587557792664, Accuracy = 0.9173760414123535\n","Iter #12093440:  Learning rate = 0.000001:   Batch Loss = 0.563308, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951530933380127, Accuracy = 0.9175429940223694\n","Iter #12094464:  Learning rate = 0.000001:   Batch Loss = 0.530583, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951591730117798, Accuracy = 0.9173760414123535\n","Iter #12095488:  Learning rate = 0.000001:   Batch Loss = 0.551219, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951524972915649, Accuracy = 0.9173760414123535\n","Iter #12096512:  Learning rate = 0.000001:   Batch Loss = 0.647386, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951620936393738, Accuracy = 0.9175429940223694\n","Iter #12097536:  Learning rate = 0.000001:   Batch Loss = 0.524018, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951789617538452, Accuracy = 0.9175429940223694\n","Iter #12098560:  Learning rate = 0.000001:   Batch Loss = 0.608290, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951822400093079, Accuracy = 0.9175429940223694\n","Iter #12099584:  Learning rate = 0.000001:   Batch Loss = 0.570006, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951932072639465, Accuracy = 0.9178768396377563\n","Iter #12100608:  Learning rate = 0.000001:   Batch Loss = 0.660535, Accuracy = 0.8828125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951906442642212, Accuracy = 0.9177098870277405\n","Iter #12101632:  Learning rate = 0.000001:   Batch Loss = 0.593797, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951838493347168, Accuracy = 0.9178768396377563\n","Iter #12102656:  Learning rate = 0.000001:   Batch Loss = 0.557476, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951746702194214, Accuracy = 0.9178768396377563\n","Iter #12103680:  Learning rate = 0.000001:   Batch Loss = 0.527860, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951775312423706, Accuracy = 0.9178768396377563\n","Iter #12104704:  Learning rate = 0.000001:   Batch Loss = 0.558917, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951805114746094, Accuracy = 0.9177098870277405\n","Iter #12105728:  Learning rate = 0.000001:   Batch Loss = 0.501301, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951776504516602, Accuracy = 0.9175429940223694\n","Iter #12106752:  Learning rate = 0.000001:   Batch Loss = 0.520989, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951933860778809, Accuracy = 0.9177098870277405\n","Iter #12107776:  Learning rate = 0.000001:   Batch Loss = 0.642036, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951966047286987, Accuracy = 0.9178768396377563\n","Iter #12108800:  Learning rate = 0.000001:   Batch Loss = 0.596293, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952010154724121, Accuracy = 0.9178768396377563\n","Iter #12109824:  Learning rate = 0.000001:   Batch Loss = 0.601798, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952048897743225, Accuracy = 0.9178768396377563\n","Iter #12110848:  Learning rate = 0.000001:   Batch Loss = 0.567831, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952028036117554, Accuracy = 0.9178768396377563\n","Iter #12111872:  Learning rate = 0.000001:   Batch Loss = 0.508102, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952110290527344, Accuracy = 0.9178768396377563\n","Iter #12112896:  Learning rate = 0.000001:   Batch Loss = 0.529222, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952171683311462, Accuracy = 0.9178768396377563\n","Iter #12113920:  Learning rate = 0.000001:   Batch Loss = 0.500606, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952349901199341, Accuracy = 0.9182106256484985\n","Iter #12114944:  Learning rate = 0.000001:   Batch Loss = 0.537599, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952435731887817, Accuracy = 0.9182106256484985\n","Iter #12115968:  Learning rate = 0.000001:   Batch Loss = 0.508601, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952526330947876, Accuracy = 0.9180437326431274\n","Iter #12116992:  Learning rate = 0.000001:   Batch Loss = 0.581748, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952631235122681, Accuracy = 0.9180437326431274\n","Iter #12118016:  Learning rate = 0.000001:   Batch Loss = 0.563246, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595268964767456, Accuracy = 0.9178768396377563\n","Iter #12119040:  Learning rate = 0.000001:   Batch Loss = 0.593412, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952733755111694, Accuracy = 0.9180437326431274\n","Iter #12120064:  Learning rate = 0.000001:   Batch Loss = 0.558445, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952600836753845, Accuracy = 0.9178768396377563\n","Iter #12121088:  Learning rate = 0.000001:   Batch Loss = 0.602381, Accuracy = 0.8984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595238983631134, Accuracy = 0.9180437326431274\n","Iter #12122112:  Learning rate = 0.000001:   Batch Loss = 0.561937, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952227115631104, Accuracy = 0.9180437326431274\n","Iter #12123136:  Learning rate = 0.000001:   Batch Loss = 0.519119, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952073335647583, Accuracy = 0.9182106256484985\n","Iter #12124160:  Learning rate = 0.000001:   Batch Loss = 0.536816, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951971411705017, Accuracy = 0.9178768396377563\n","Iter #12125184:  Learning rate = 0.000001:   Batch Loss = 0.640304, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951988697052002, Accuracy = 0.9177098870277405\n","Iter #12126208:  Learning rate = 0.000001:   Batch Loss = 0.525675, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951990485191345, Accuracy = 0.9178768396377563\n","Iter #12127232:  Learning rate = 0.000001:   Batch Loss = 0.565488, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952080488204956, Accuracy = 0.9178768396377563\n","Iter #12128256:  Learning rate = 0.000001:   Batch Loss = 0.472112, Accuracy = 0.984375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952279567718506, Accuracy = 0.9177098870277405\n","Iter #12129280:  Learning rate = 0.000001:   Batch Loss = 0.485611, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952396392822266, Accuracy = 0.9177098870277405\n","Iter #12130304:  Learning rate = 0.000001:   Batch Loss = 0.591616, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952253341674805, Accuracy = 0.9177098870277405\n","Iter #12131328:  Learning rate = 0.000001:   Batch Loss = 0.585837, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5952038764953613, Accuracy = 0.9177098870277405\n","Iter #12132352:  Learning rate = 0.000001:   Batch Loss = 0.564964, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951883792877197, Accuracy = 0.9175429940223694\n","Iter #12133376:  Learning rate = 0.000001:   Batch Loss = 0.539659, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951793193817139, Accuracy = 0.9175429940223694\n","Iter #12134400:  Learning rate = 0.000001:   Batch Loss = 0.517506, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951811075210571, Accuracy = 0.9175429940223694\n","Iter #12135424:  Learning rate = 0.000001:   Batch Loss = 0.557513, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951772928237915, Accuracy = 0.9178768396377563\n","Iter #12136448:  Learning rate = 0.000001:   Batch Loss = 0.534549, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951851606369019, Accuracy = 0.9175429940223694\n","Iter #12137472:  Learning rate = 0.000001:   Batch Loss = 0.603828, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951865315437317, Accuracy = 0.9177098870277405\n","Iter #12138496:  Learning rate = 0.000001:   Batch Loss = 0.545183, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951852798461914, Accuracy = 0.9177098870277405\n","Iter #12139520:  Learning rate = 0.000001:   Batch Loss = 0.562667, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951849222183228, Accuracy = 0.9173760414123535\n","Iter #12140544:  Learning rate = 0.000001:   Batch Loss = 0.507708, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595182478427887, Accuracy = 0.9175429940223694\n","Iter #12141568:  Learning rate = 0.000001:   Batch Loss = 0.508253, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951875448226929, Accuracy = 0.9177098870277405\n","Iter #12142592:  Learning rate = 0.000001:   Batch Loss = 0.555283, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951873064041138, Accuracy = 0.9177098870277405\n","Iter #12143616:  Learning rate = 0.000001:   Batch Loss = 0.497620, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951902866363525, Accuracy = 0.9175429940223694\n","Iter #12144640:  Learning rate = 0.000001:   Batch Loss = 0.499565, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951849222183228, Accuracy = 0.9175429940223694\n","Iter #12145664:  Learning rate = 0.000001:   Batch Loss = 0.553210, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595180869102478, Accuracy = 0.9177098870277405\n","Iter #12146688:  Learning rate = 0.000001:   Batch Loss = 0.514785, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951747894287109, Accuracy = 0.9177098870277405\n","Iter #12147712:  Learning rate = 0.000001:   Batch Loss = 0.570202, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951767563819885, Accuracy = 0.9177098870277405\n","Iter #12148736:  Learning rate = 0.000001:   Batch Loss = 0.560908, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951820611953735, Accuracy = 0.9178768396377563\n","Iter #12149760:  Learning rate = 0.000001:   Batch Loss = 0.554667, Accuracy = 0.953125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951758623123169, Accuracy = 0.9175429940223694\n","Iter #12150784:  Learning rate = 0.000001:   Batch Loss = 0.491238, Accuracy = 0.96875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595166802406311, Accuracy = 0.9173760414123535\n","Iter #12151808:  Learning rate = 0.000001:   Batch Loss = 0.600237, Accuracy = 0.921875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951627492904663, Accuracy = 0.9173760414123535\n","Iter #12152832:  Learning rate = 0.000001:   Batch Loss = 0.542185, Accuracy = 0.9375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951665639877319, Accuracy = 0.9172091484069824\n","Iter #12153856:  Learning rate = 0.000001:   Batch Loss = 0.524328, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951762199401855, Accuracy = 0.9173760414123535\n","Iter #12154880:  Learning rate = 0.000001:   Batch Loss = 0.540211, Accuracy = 0.9453125\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951734781265259, Accuracy = 0.9172091484069824\n","Iter #12155904:  Learning rate = 0.000001:   Batch Loss = 0.488324, Accuracy = 0.9609375\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951632261276245, Accuracy = 0.9175429940223694\n","Iter #12156928:  Learning rate = 0.000001:   Batch Loss = 0.597457, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951578617095947, Accuracy = 0.9175429940223694\n","Iter #12157952:  Learning rate = 0.000001:   Batch Loss = 0.577318, Accuracy = 0.9140625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.595146894454956, Accuracy = 0.9172091484069824\n","Iter #12158976:  Learning rate = 0.000001:   Batch Loss = 0.566758, Accuracy = 0.9296875\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951555967330933, Accuracy = 0.9172091484069824\n","Iter #12160000:  Learning rate = 0.000001:   Batch Loss = 0.490488, Accuracy = 0.9765625\n","PERFORMANCE ON TEST SET:             Batch Loss = 0.5951792001724243, Accuracy = 0.9177098870277405\n","Optimization Finished!\n","FINAL RESULT: Batch Loss = 0.5951933860778809, Accuracy = 0.9178768396377563\n","TOTAL TIME:  1284.0809588432312\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCVpMbSX0ey5","executionInfo":{"status":"ok","timestamp":1619792340351,"user_tz":-420,"elapsed":1292571,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"b05fa1b5-c28a-4537-cbe7-3881ea5d3865"},"source":["#create saver before training\n","\n","saver = tf.train.Saver()\n","retrain = True\n","\n","#check if you want to retrain or import a saved model\n","if not retrain:\n","    saver.restore(sess, \"/content/drive/MyDrive/Train-Openpose-lstm/Saved model/model.ckpt\")\n","    print(\"Model restored.\")\n","\n","# code to run inference...\n","\n","# Check if you want to save your current model\n","if retrain:\n","    save_path = saver.save(sess, \"/content/drive/MyDrive/Train-Openpose-lstm/Saved model/model.ckpt\")\n","    print(\"Model saved in file: %s\" % save_path)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model saved in file: /content/drive/MyDrive/Train-Openpose-lstm/Saved model/model.ckpt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A_XuAVlW6kb_"},"source":["## Results:\n","\n"]},{"cell_type":"code","metadata":{"id":"T1_-xs2Q6kb_","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1619793439753,"user_tz":-420,"elapsed":4074,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"bfab0df4-6de6-44c3-c1f1-104de19f7c8f"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","# (Inline plots: )\n","%matplotlib inline\n","\n","font = {\n","    'family' : 'Bitstream Vera Sans',\n","    'weight' : 'bold',\n","    'size'   : 18\n","}\n","matplotlib.rc('font', **font)\n","\n","width = 12\n","height = 12\n","plt.figure(figsize=(width, height))\n","\n","indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n","# plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n","plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n","\n","indep_test_axis = np.append(\n","    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n","    [training_iters]\n",")\n","# plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n","plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n","print (len(test_accuracies))\n","print (len(train_accuracies))\n","\n","plt.title(\"Training session's Accuracy over Iterations\")\n","plt.legend(loc='lower right', shadow=True)\n","plt.ylabel('Training Accuracy')\n","plt.xlabel('Training Iteration')\n","\n","plt.show()\n","\n","# Results\n","\n","predictions = one_hot_predictions.argmax(1)\n","\n","print(\"Testing Accuracy: {}%\".format(100*accuracy))\n","\n","print(\"\")\n","print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n","print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n","print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n","\n","print(\"\")\n","print(\"Confusion Matrix:\")\n","print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n","from matplotlib.pyplot import figure\n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues,\n","                          size=None):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    # classes = classes[unique_labels(y_true, y_pred)]\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    fig, ax = plt.subplots()\n","    if size is None:\n","        size = (12, 8)\n","    fig.set_size_inches(size[0], size[1])\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","   \n","    return ax\n","\n","\n","np.set_printoptions(precision=2)\n","\n","# Plot non-normalized confusion matrix\n","plot_confusion_matrix(y_test, predictions, classes=LABELS,\n","                      title='Confusion matrix, without normalization')\n","\n","# Plot normalized confusion matrix\n","plot_confusion_matrix(y_test, predictions, classes=LABELS, normalize=True,\n","                      title='Normalized confusion matrix')\n","\n","plt.show()\n","\n","# def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","#     if normalize:\n","#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","#         print(\"Normalized confusion matrix\")\n","#     else:\n","#         print('Confusion matrix, without normalization')\n"," \n","#     print(cm)\n","\n","#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","#     plt.title(title)\n","#     plt.colorbar()\n","#     tick_marks = np.arange(len(classes))\n","#     plt.xticks(tick_marks, classes, rotation=45)\n","#     plt.yticks(tick_marks, classes)\n"," \n","#     fmt = '.2f' if normalize else 'd'\n","#     thresh = cm.max() / 2.\n","#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","#         plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" \n","#         if cm[i, j] &gt thresh \n","#         else \"black\")\n"," \n","#     plt.tight_layout()\n","#     plt.ylabel('True label')\n","#     plt.xlabel('Predicted label')\n","\n","\n","\n","'''confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n","\n","\n","print(confusion_matrix)\n","normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n","\n","\n","# Plot Results: \n","width = 12\n","height = 12\n","plt.figure(figsize=(width, height))\n","plt.imshow(\n","    normalised_confusion_matrix, \n","    interpolation='nearest', \n","    cmap=plt.cm.Blues\n",")\n","plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n","plt.colorbar()\n","tick_marks = np.arange(n_classes)\n","plt.xticks(tick_marks, LABELS, rotation=90)\n","plt.yticks(tick_marks, LABELS)\n","plt.tight_layout()\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11877\n","95007\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAukAAALfCAYAAADc51y7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5wURfbAvwVsYMlLzrtkRXKSjOApYEAQE566xlNPUM/T8/TuxPt55gyIAiKgopjAgICgREGiJMk5h13CLsvm7d8f1T3TM9MzO7PsMrPwvp/PfGamqrrqdap+/erVK2UYBoIgCIIgCIIgRA6lwi2AIAiCIAiCIAieiJIuCIIgCIIgCBGGKOmCIAiCIAiCEGGIki4IgiAIgiAIEYYo6YIgCIIgCIIQYYiSLgiCIAiCIAgRhijpQolCKWUU4pNUDHKMNOseWUT1LTDr61MU9QlulFIJ5rHdE2Y5DKXUgvPUVnml1BmzzVXno01BUEpNKsp+MZKRPls4H5QJtwCCECKTHdKaAN2Bo8Bsh/wdxSqRIEQeNwHlzN8dlFKXGYaxMZwCCRcvpqHkI2CyYRhJ4ZWmYEzFez6w0DCMPuGVRriYESVdKFE4dfDmA6A7sOU8PgBGA58DyUVU351AHLCviOoT3BwELgFywi3IeSTJ/D4E1DH//z1cwgjCBYj02UKxI0q6IBQCwzCSKToFHcMwpKMvJgzDyAG2hFuO84VSqhHQE0gH7gbmALcrpZ42DCM3rMIJwgWC9NnC+UB80oULGrvfoFLqSqXUT0qpE2ZaW7NMS6XU/ymllimlDiulspVSR5RS05VS3f3U6+iTbk9XStVRSn1k1pWplNqklHqkIDkDyN9VKTVbKXVKKXVWKbVEKdUvwL53VErNNMunKaWWKqWGFNZHWynVWyn1rVJqj1IqSymVopT6Qyk1VinV2KF8eaXUM0qpNWb7Z5VSa5VSf1dKRTuUL6uUGq6UWqmUOm4es0NKqUVKqWccyg9SSs1VSh0w5Tlm1v+mUqq6rVzA/VVKtVZKfaqUOmie+6MFnPs9Zn0JSqmBSqnF5v6lmuenfYjHta1SaqpSaodSKkMpdVIptc307w2pLpO7AAV8YxjGT8A2oBbQvwA5rlFKfWder9nmsZ+vlBpxLuX9Xdu2fNfx9JeulLrZvN5Pm2mVzTJdlFJvKKVWm+c/Sym1Xyn1iVLqsnPdX6X7C0MpNTRAPTPMMrcHas9rm0Sl1Dive2mOUupar3JlTPkMp3vMVm6TWeZyr/RQ70HLpzxJKdXe3LdjSql8pdQNwe6fV50L0K4uAHcpz/lCk7zKRiulHlG6rzpl9gGble6fKzjUbe9vG5vn/bBSKk8p9ZhZpoZS6jHzXO4x6zypdL9yp9MxQLu6APT2kneBfb/8XddKqQpKqeeUUhvMY56mdL82QikVVcB+hPLcqKyU+pdSap25Txnm9f+TUuoBxxMilCwMw5CPfEr0Bz2UbwALHPIWmHnvA/nA78BUYDHQ2iwzwczbCMwEvgTWmdvlArc61DvSzB/pJ30icBjYjXaLWQDkmXnPBJCzj5/019DuGqvM+jaY6TlAL4f6rgKyzDLrzX3+1fz/hvm9J4RjfI+5TZ5Zz2fmsfrDTL/Vq3x9tPXaMI/DTOAH9OiDgX4IRtvKlzLTDOCkWXaqmXYUyPSq/79m2WzgF7PsbGC7mX65rWyCv/0FhtiO01qznmW2fX3IYZs9Zv5LZpnFwBfouQ8GcAZo5rCdzzVqnqccM28VMA34Fn2d5gFPh3gvKJt8fc20Z8z/XwXYZrxtn5eZx2EecAQwzrH8AhyubYfjmeAn/T3ze6nZziqgkllmnnn81prH7Rtgq1n+LM73RtDyA4PMcvP8yF4P3UccB2KCPEfdgNNmvdvQ99J8sx4DeMmr/Jtm+vN+6utk5m89l3vQ3GaSmTcBfV9sNeWbC1wTxL5Z24+0pT0NLDHTd5hlrM99tnKVzXNsACnAT8AMtLuage6f473aG2nmTUX3G/vQ99APwANmmT+bZfaa5/gzYJHteI/xqvM+dF9imNeDXd6nbeUW4Nxn18DdLx4HvkJfm2m24x7rZz+Cfm6g55xstp3fb81tFpvHYksofYd8IvMTdgHkI59z/RCckm4ASX627w00dEgfiFYCTwBxXnlWpzrST7oBjAJK2/KGmulpQDk/cvbxk56PTRFGKxqjzLxfvLYpZ3baBvB3r7xBtofTnhCO8W68lF9bXhMg0Uu238zyr2NTXtAPYusB+F+vc2Apqt7HpjSmwmn+jwUyzOPYxEGeNkAN2/8Ep/0FagOpZt5fvPIGm8cpB/Nlzpa3x9wmA+htS48Cppt5E4M8rvPN8rc45NUBLg3xXuiLWyFRZlo99IM+Cy8lx8x/wtxmH9DO4dhfd47lHa9th+OZ4Cc9G7jKz7b97efaln6fue1m6zgURn7z/x70/dfUoR3rZfGVIM9PLLDf3OZ/dtnQyrulyA3wup4NYKf3vpj5Vj/w7Lncg2beJNz913+c2itg/6ztvfvFJDN9UoBtvzDLfApU9DpmVr1TvLYZaZN3PBDlUO8lQCeH9Mbo+8SnXwP64OeZUtB1jVbKDbSbWQVbem30i4bP9UIhnhvoETMD+B4o41VfDA4vqPIpeZ+wCyAf+Zzrh+CU9NmFrPtTc/trvNKtTnWkn/Q9OFjWbJ10bz9y9vGT/rlDXdXMvCz7w8nWea/1s0/TLBlDOA7pwMkgyw60zgfOSkVtU+Zk3IrkTeY2bwdRf/VA++dQPsFpf9FKiAHM9bPdJDN/glf6HjP9ZYdtOpp5u4OUzbK4VS6ie2GKWd8LXuk/memPeKVH4basFvhQD7V8oGvb4Xgm+El/v5DHwho5anmO8j9tln/dK70M2sqbDzQKsq47zbq2AKUc8kfiYLnHPbLX0+F8HDdlaGBLD/ke9LrmNznJF8T+WduP9EpPIoCSDrTEPbLg1G/Goa3aOdheNG3HKxkoXwh57ze3f80rvY91/EK5roGG5rnI9r6evepNw2ZNpxDPDeBJM+2xwtwf8ikZH/FJFy4WZgTKVEpVUkrdrpR6VSk13vTNnARYfq3NQmxvvmEYWQ7pW83vOiHWN8s7wdCTV08A0WiF3aKX+f2Fn7qmhtg2aAt3ZfO4tFFKqQBlB5jfXxnm08SOYRiH0W4pVYGmZrLl3nGPUupBpVQNf5UbhnEcbQVtY56vUM+NhXWcnMJ6gh56Bm3ld8LnnBD6+bVimH+i9JyD0kFu54Pps3uj+dd7n6z/SV7pHdHnYYdhGIuCaCbU8kVBQfduDaXUvUr7pk+w3bu1zCL266Mw8luuH0lKqRhb+iD0ef7JMIxdQdZlXXOfGIaR75BvXXPdva4F6/x5+1APQN/78w3PiYyFuQftfOdHvuLCmi/xnVO/aRjGWfS9UgZ9Dr2ZZxjGGX+VK6WilFIDlFLPK6XeN32+J6Gt1BB6/+6PnuhRjEWGYezxzjQMYwF6VLI80MFh+1CeG1bf8ZRSaphSqlJhhRYiF4nuIlws7PWXoZQajH44Vg6wfcUQ29vvJz3N/I7xk1+Y+uK96qtrfvvbZ7/HIgAPof197zI/J5VSv6GHdKcYhnHSVraR+T1KKTWqgHqrA9sMw9ihlHoUPTQ/FhirlNqB9q/8GvjRS9n4M9r/8kngSaXUUbQ/64/AVPOhXhDWcdrtJ3+XVzlvfM6JYRhp5vuLz6Q8PzwNtACuMT9nlFIr0L6zkw3DOBRkPQA3oy2OSw3D2O6V9w3atcc7ZnoD83srwRFq+aIg0L37MHqORWyA7e33bsjyG4aRrJSahlaQbwI+MbMeMr/HBlsXBV9zB9BW2Fi0An3MTP8UeAW4SSk13DCMTDPdUtqneNUT8j3olVaYPuJcsOR9Qin1RAFlqzukBbpGWqD9tQMp4qH27/4o6PyC7lcSce5Xgn5uGIYxXyn1EvAU+vrIV0ptBhYC087jS7RQjIiSLlwsZDglKqXqoy3LsWgf0c/QQ45nDcMwlFIvAv9EW0dCoaitUIWpz8eCVti6DMPYpJRqBfRDW716AlejLXb/UUpdZRjGarO4ZQH8Bf8PHYsUWxtjlFJfA9ea7fREhxC8G/hZKdXfMEMIGoaxWCnV1JTharPsYPPzH6VUT8MwilvROOdzbBjGYaVUV6AH+lj2Qu9LX+DfSqmbDMOYGWR1SeZ3Q6XUEqfmbOX+7pUWtMghlg+GgkZ0/d27ndDrFeQCf0NPFjxgGEaGmT8VuA3Pe7ew8o9BK8QPokc9mqLP0QGz3WLFMIyjSqmf0G4sg4BpSke4uRbtiva11yaFugdtOB7zYsSSdwV6HkEgnO7rQPJ+hVbQZ6BfdLYCqYZh5CmlrkIbGkLt34uLkPoUwzCeUUqNA65DX489gIeBh5VSUwzDuKsYZBTOI6KkCxc716AV9K8Nw/iXQ36T8yxPUWBZXxv4yU8oTKWGjjc+2/xguqS8irasjwa6mkUtpWCqYRgfhtjGEbR7wQSzjS7oF6d+wL3AB7ayZ9ETNaebZRuio/j0B15GK2iBOIi2YjdC+y9708hWrtgw3QoWmR+UUhXRL4ZPoyfDFeg6o3R4vh7m37r4t/6DZ8x0y0Ui2OH+UMuDtgyDHuL3QClVBu0fXRhuRCtX7xqG8ZZDvtO9Wxj5MQxjhVJqJdoNpRX6RUcB4wzDyAuhKutaauQnvx56FCYT7cpmZzJaSb8TPa/kFrRldZqDq0eh78EwYcn7k2EY/y6qSk0rekt0hKihDueqqPv3gs6vPa9I+hXTrWYUetREAX9CjzLeqZSaahjGnKJoRwgP4pMuXOzEm98+1ialVDV0h1fSWGx+3+wnvyDlNSgMwziGDu8H0NqWNdv89htbOoQ2lmMq7F5tOJXdC/xfMGVNrOFgn1jJJneb3wuDqKvIMAwjFX1cs4HayhbzPQBJ5vd0wzCUvw/aD9keM3012pLaVCnVw7daH0ItD+6XxuYOeVdQeGNRoHu3BdDOYZvCyG8xxvx+DH28c3Ffm8FiXXO3K6Wcnr/WNfer4bvw1HfAKeAqpVRN/Lu6QBHeg0WE9aLm71xb8g72c1wKi3WNHPbzMnWrn+0KktcfizEnJSuvuP+g15pAu7qcQV+LRYqh+Qk9egDB9YNCBCNKunCxY61EeaP54ANAKVUO/QAO5KceqXyJ9mVtp5R63J6hlLoO7VcbNEqpOKXU4+ZLizfW4iv2SWvT0RNB+yul3jItw951Jiil/mz739ec2FXGq1w07helfWZaQ3OioM/iJn7k8cd49MPySqXU/V7tXo/2e88F3g2irkKhlHpCKVXPIetPaItqKloxC1RHKdwK2yeByqJ9V8FU6s3RkZetPKWUx0NdKVXavGYoTHmT+eb3w/YJwUqpJmgLYGGx7t07lVIuK715nX6Eg4JVSPktPkdHEbkHrfx9a07ADIUv0RbU5sDz9gnY5qiR5Y/9poPsmejJ4GWAf6NDNu7HfXzthHwPFjOW1fgSp0zTVe47tNX7U3tfbKGUqul9nwbBdrQLyWVKqZ62upTSC6T19LOdJW8T7z4pEKahYDr6HL3vdV3WxH29v2ebV1AolFKDlVI97NeQmV4J96iarIpa0gl3eBn5yOdcPwQXgrGPn22j0AuhGOgFRr5F+3ceRw+RTiRwqMWg0m35k8z8pGDkDEL+PTiHr+uPtgYZ6PBtU9ELiuQDb5vp24I8vpXN8rlo6880tMLyu5meA1zvtU0D3OEFT6Gt0Z+ax3ebmf6brfxjZtpJ9KRJq+wxM30rZphCoK2ZloleiOYztPJjLWKTBnS21Z2An5CTeC5mtMZs1wrdl0/gxYwS/BwvA23UCubYnsK9kNZXuBdTyjfreTiIOq60HbuAC+qgh/cNbDHT0W4b1nVpLVY1Fb2Ajb/FjEIpH4N78a3jaN/g+ejFhqb6O55BHOcquGOOH0Xft9+hX2y24I5Z732vhSS/17YvW+cX6BdqX2XW0R33YkZbzLZ/xs9iRl7bdrO1bwD/C1A2pHswUP8Uwr5Z23v3izG4125YhXbdmQDc7dXPWJbodNt5+QZ9f+QDR7zqHenUnleZMbj7L2sxo23m/9fw/+xYgzsc5cemvE/a8hfg3GfXMLcx0P3Xl+hr3lqTYT7+FzNy3A+n84K7Hz+KjjT1CXrBKuvaWoJD3Hj5lKyPWNKFixpDW9Z6AW+hO9SrgS7oh317SqglwjCM2Whrymy0kno92rpzC+6h0OQgqzuDnoz0NdqveADaYl0W/RLTzjCM77za34cOlfYY+gHbGm3B74h2N3gBsC9b/QN6YZi1aCvjjWgf973oCC6dDMOwLMo70RMF56AfiNehV+7MRT+4WhmGsSKYHTMM4xugM/rBXduUsRlakellGEYokTsKwyNoBUCh/e5vQEev+ALobhjGe0HUkWR+f2k4h29zYRjGDvTkvGhgmJlmGIaRhH5hmYs+/kOBS9FK3iNedYRaPsvct4/QitYA9LH+D3q0olAYOqJQJ/Q1mIGeX9IK+BC4HK2sOG0XkvxezDW/t6EnZRZG7l/RrjgT0PfQUPP/L8AgwzD+GWDbpWjrsIWTq4tVNtR7sNgwr4H+aCUyEX3e78UW3tS8v69Au/wsQ5+Xm9AvJpnoPnpIIZofDvwVfW67okeptqGt6IEmZQ9B34fxaBfBe9HXWEAM7QbYBXge/Uy5Fv0ivQ19Lq42ztGKbjIJPSdoJ3rBq5vQz6wNwF/QL5E5RdCOEEashUQEQbhIUEo9i35AjzEMI5BCIgiCDaXUePRqpn8znCerCoIgFBmipAvCBYhSqhZ6qegDXulXo4eP44AuwVqcBeFixwy7uAHtKtTAMAxHS70gCEJRISEYBeHCpCPwnVJqPdq3Nx/txtHSzH9JFHRBKBil1MtAfbQrXAzwH1HQBUE4H4glXRAuQMyY4f9E+3zWQvuSn0RP/HzfMIxvwyieIJQYlFJ70JMwD6D93f/P0LHtBUEQihVR0gVBEARBEAQhwhB3Fy+qVatmJCQkhFsMQRAEQRAE4QJn9erVyYZhOC5aJ0q6FwkJCaxatSrcYgiCIAiCIAgXOEqpvf7yJE66IAiCIAiCIEQYoqQLgiAIgiAIQoQhSrogCIIgCIIgRBiipAuCIAiCIAhChCFKuiAIgiAIgiBEGKKkC4IgCIIgCEKEIUq6IAiCIAiCIEQYoqQLgiAIgiAIQoQhSrogCIIgCIIgRBiipAuCIAiCIAhChCFKuiAIgiAIgiBEGKKkC4IgCIIgCEKEIUq6IAiCIAiCIEQYoqQLgiAIgiAIQoQhSrogCIIgCIIgRBiipAuCIAiCIAhChCFKuiAIgiAIgiBEGKKkC4IgCIIgCEKEIUq6IAiCIAiCIEQYoqQLgiAIgiAIQoQhSrogCIIgCIIgRBiipAuCIAiCIAhChCFKuiAIgiAIgiBEGGFV0pVSjymlvlRK7VZKGbZPUiHqaqiUGqeU2quUylJKHVNKfauU6l4MoguCIAiCIAhCsVEmzO2PBCqdayVKqfbAPKCKLbk6cD1wrVLqHsMwJp9rO4IgCIIgCIJwPgi3u8sGYCLwMHCsMBUopcoAU3Er6D+ilfM3zP+lgLFKqUbnJqogCIIgCIIgnB/Cakk3DKOn9Vsp9Y9CVjMAaG7+TgWGGoaRAXyvlGoDXAmUBR4CnjwHcQVBEARBEAThvBBuS3pR0Nf2e42poFv86qecIAiCUEIxDINXf32VY+n+B2C/3vQ1y/YvO49SnRtO+3TN1Gv4cM2HRd7W0TNHee3X1zAMo8jrzjfyeWHRC5zOPF3kdRc1m49vZsKaCUGVnbJuCuuPri9micLHJ+s/Ye2RtX7zl+xbwvTN04ukrclrJ7Ph6IYiqcuJkQtG8uAPDxZb/eeTC0FJt7uxHPHKs/9v7K8CpdQDSqlVSqlVx48fL1LhBEEQArHi4Apmbpvpkbb31F5OZJwIk0ThId/IJ9/ID6rsykMr+ce8f3Dn9Dv9lhn65VC6TexWVOIFRV5+HgM+HcCUdVMYtXwUaw6vCXrbVYdWeexTenY6P27/kfu+vy/oOib+PpFhXw8rsNzt39zOU/OeCqiUOfHR7x8xY8uMgGW+2/od/57/bx6f87hj/qzts/jXL/8Kqd3i4p3l73D/9/cHVfauGXfR5v02xSxR+Lhj+h20+6Cd3/yeH/VkyBdDiqStpG+TaP1+6yKpy4nnFz7PB6s/KLb6zycXgpJezvY72yvP/r+8vwoMwxhnGEZHwzA6Vq9evUiFEwRBCMToFaN5aOZDHmkJ7yTwlx/+EiaJwsMVk6+g9H9LB1U2qlQUADXK1fBbpml8U2677LYikS1YDAxm75jNpuObGDF7BL/s/qXAbZLPJqOeV9zz3T0ApGalAlBKhf543nB0AzO3zyyw3OksbeXOzc8Nqf43f3uTj9d/HLBM/Yr1Aehar6tj/sK9C3l96eshtWsxa/ss1POqyEYXKsVUIqZ0TJHUJQjFwYWgpKfbfnvfbfb/Z86DLIIgCCHx8fqP2Z+63yf9q01fhUGa8LFo76Kgy1aNqwpAn4Q+xSRN4bCU3s82fgbAeyvfK3CbjBztobnx2EYA/tToTwCULhXcC4udKeunuJT8QHSo3QGACjEVQqp/47GNfLP5m4BlYsvEAlAp1jlw26gVo8jKywqpXYsdJ3YAhDwC4I/1x9YHLUuV2CoM7zy8SNqNRGqUq8GDHc6fi8i97e49b22VZMIdgrEo2GX7Xcsrr7bt987zIIsgCIIQAWw/sZ3tJ7Yz9cap561Ny11n3+l9AGTkZgQq7kGF6AqM6DKC65pf51FXw0oNg64jWBepp7o/xZ8a/SmkukOVwVKovTmbc/ac2zAoGl/62TtmB122QaUGxJeNL5J2I5EW1VpQu0Jtv/m/3vMraVlpRdLWDS1uoGOdjkVS14XOhWBJt48ntldKxdn+9/JTThAihrSsNA6mHnT9Tz6bTMrZlKC3TzmbQvLZZAC2p2z38euds2MOWbmFs1wVNXtP7XVZDouC/af3s/PEzgKth2dzzrL/tK+1uiCOpR/jZMZJv/lbk7c6pqdnpzu2dzLjpN/JjvN3zw/qIXgw9SAHUw9y5Ix7ys3pzNMcTjsM6AmI21K2kZGT4VIWC2LhnoV+J/rl5OWw66S2hdjrPHrmKKcyTwHaF3vHiR1+93v3yd1sOr4pKPeKnLwcj/8HUg+Qnp3ukZaXnwfgat8b+4TRzcc3szV5K9tTtnuUOZFxguPpeg5Sbn4uO0+47Tgbj21k1aFVrnyL3Pxcv8ongEL5pG1N3ophGGTlZrHn1B5A37Pedadlp9GqRitiSsdwOO2wax8rxlR0bGvHiR3k5ed5nAf7/jv595/KPMWivYuoUa4Gnet2Jqp0lCvvTPYZ1hxe43O9p2WlcSjtkEdaRk4GhmE4Xv+WEn4g9YBH+pEzR/yeL8CjvjPZZzz6RAul9PHNzc9l1aFVHD1z1G993mxL2caW5C1Blc3Oy2b3yd2u/9tTttO2VlsaVmrod7+3pWzDMAxy8nKYtX1WwPp3ndxFTl6OY13+6gc8riF/+Nt2z6k9Ps+BU5mnXP1I25ptSayc6LpeAf449ofrvmga35RWNVtxKO0QaVlpHEs/xo4TO9h1cper7wHf68X+Pzsvm10nd9GpTieXW5SFdfzsHEw96OoTnZ5tVt/g/Qz1Ji0rjT2n9rj6MdB98apDq8jIySjw2gwn4V5x9Cql1A1KqRsAu3Ld3kpXSlUzy06yrUg60lZ2FmD1vhWAr5RS1yml3gJ6m+mZwPvFuzeCEDypWal8tuEz9pzaQ+cJnan3Vj0Ajqcfp/pr1an2WrWg6xo8bTADPh3AxmMbaTa6GS8tfsmVt2DPAvp/2p/Y/8UW+T4UhoR3Eops8hFAg7cb0GRUE5qPbh6w3CtLXqHV2Fau/1uSt/DKkld8FCVvuk/szm1fO/s1f7f1O1qMacEXf3zhk9dvSj8avN3AI233yd3EvxpP5/GdHevrO6UvFV92Vsjs1HurHvXeqkftN9xWr9u+vo3EdxIBeHPZmzQf3ZyEdxJo+HbBltI1h9fQZ3IfKr9S2TH/sdmP0fjdxhw9c5TrPrvOVWePj3pw33d6UuOzvzxL01FNGfDpAJ/9PpN9hkbvNqLley35x9yCI+2OmDXC43/9t+rTe1JvjzRLuXRSYFccXOGaMNonoQ+XvncpLca0oNnoZmw+vpm8/Dx+2f0LVV+tSo3XtU/7W8veosmoJhxPP86OEztoNbYVncZ3cuVbPPuz3k+7AheII2eO0GJMC8r+ryxJ3yaR+E4iGTkZVHutmqvub7d+6yp/69e30vr91tR5s47L3WXDMd8oGLtP7qbpqKa8uPhFar1Rizpv1PHI7zaxm6OrTZVXqtB7Um8avt2QBm838HiJe/KnJ+kwrgPxr8az8uBKV3qb99tQ9826HvVc99l1TPx9Ii3GtPDxu69bUZcdu2qsR3rtN2rT5N0mHvtgZ8m+JbQY04L1R9fTeby7T3Tig9Uf0Gl8J2q94T147smo5aOYukGPpDQf3ZxLxlzikX9Zjcsct3vg+wdo9G4jTmeeZmvyVpqNbsbkdZNZe2Qtk9dNpsWYFszdOddVfs3hNTQf3ZzXl75O4juJDJw60LFfAK0wNn63MZ+s/4Qftv1AizEtPF4qv9/2PS3GtHC5P01YM4FpG6cBcOeMO0l8J5HM3EzHumdtn0WLMS18XOUycjJIfCeRpG+TPNKrvFKF2m/U5lTmKaZunMqrS1+lxZgWjFs9jmkbp3HZ2MtoMqoJh9MO021iN+q+WZe6b9alzfttaPh2Q5qOakrjdxtT50339dfug3Ye18s9391D3TfrsuPEDp786Ukav9uYZ395lh+3/0h2np42uPzAcpqPbs6oFaMAbczZnrKdem/Vo9P4Th7Ptn2n97Hq0CpA9w1XTL6CTuM7BbxeOozrQOI7iTR+t7HrJbTFmBZ0Gt+JwdMGU/uN2jR4q4Hf7cNJuC3p44Dp5sc+Y3O4Ld35LjIxDCMXGAZYZqABwHfAY1YR4K+GYYi7ixAxHDlzhGHfDGPp/qVUjnUrR9d9dl3IdUWVjiK2TKzLmpyS4bbCF1WYq7/N+RvvLj3TWxAAACAASURBVH+3SOpad2RdkdRjp6BQcmdzzpKT77bQbji6gad/fjpgCD/QExTn7JzDt1u+9cmzjq3T/iw/uNwnzXpwxpSJ4ZP1nxD7Qqyjsmcpsp3ramV+yrop3P3t3a78muVqun7f+tWtpGalUjm2MvUracvUsgP6gV/QvlnYrUtOIebm7Z4HaKvbz7t/dqXvOLGDrzd/DcDifYs9vu3YR07m75lfoDxzd831SVt9eLXHf+ta33R8k09Zu0W8VnlPJW5/6n76TulLvyn9XGlL9y/lqXlPAdrFxG4V9Gbh3oUAHE13tuB6+5FXidVr7GXlZfHrvl9dvy0+XvcxVctWdawr0DVttWMpxJZbzaNdHnWVsfcr3lguKXareZlSbu/XKeumMGPLDNp/0J7dp9zXqHXt/bz7Z8pH61gM1iTeQFgvA1l5WVzXTPdxKw6uAPR1dO3Ua13K/tmcs6TnpDvWY7XfrGqzAvcRYPya8X7ndvx34X/ZeGwj0aWjffIsi3tmbiYnM93HaMzKMS6Lq93yah2DF5e8yMG0gx51eGOd//ScdNfL5tojayn1fCm+3fKt67xbI0rvrXyPTzd8CuCy0FvKrTfWHIOHZj7kUvLtbX6+8XOXsWLz8c2u/G0p20g+m+zaZuWhlaw76u7XUjJSXCNI1eOq06thL1pUa+HT/prDa9h50lPVsu6BO6bfwbsr3M+Q91a953ppss63NWL26OxHuenLmwDYmrLVdf38uv9X3l3+Ln0m9XHVs/LQSramOI8eABxOO8z2E+5RtLRsbZm/vN7lALSr1c4jPdIIt5JeJBiGsQpoB3wIHABygBTge6C3YRgTwyieIPhwJlvPYz6QeoC2Ndu6olRYyl1cVJzfbb3Jy88jLz+P6nH6Pbd97fauvEDRL0Lhrd/e4tHZjxZcsAAUivvaBx9SLliubnJ1wPwz2Wc8fGGtB2xBPsPWebIryRata+oQYh3qdPDJ616/u0+aZRHNzc9l3q55ZOVlebisWLy69FUAPrhWhxC7a8ZdTFo7yVG+aX9Mc7kVWK4WVzcOfCy8sSspTpMCrYdYbJlYnuzmvB7c3W19j4+F3aWiS90ufstZCpeT0uSNpWRaip4du9/wD9t+8MhrWKmhzwTVl5e87PptYHhEVfGOsHJJda1UlIsqhz/s7VuKAMBfO/0VwCOayCcbPqFOBU8rOEDjKo2Dcg3yVuQrxThP1gyVjcc38sHqD/j9yO9cVuMyl4zDWunwjo2qNHIp6dZEUQsndwtLITyTfcZVl3XtL9q7iJnbZzJuzThA92cDmwx07LsGNh3I6gdWc3/74MIm7k/d73iPATy34DnAWeG96VKtIMZFxXm8FOcZeTSvqkftGlRyW16tFxy74l4tznk01HLZsPdHu0/txsBg5aGVPuV3ntzJ3tN7AVzRigp6MUo+m8znGz93/bdfx5Yibr/XLXdJi+71u1OvotsybX8eRZeOprRyntQcKOb8bwd+80mzDATWs6tp1aaAHl2yXhLqVKjjOgdd63Vl4d6FLqW+YkzFAp9x3i/43vdI6VKlSaicwA0tbghYT7gIq5JuGEaCYRiqgM8Cs2ySLW2kQ127DcO4zzCM+oZhRBuGUc0wjOsNw/A17QhCmLH85xbtXcTG4xt9rJ7d6gcf33nh3oUsO7DMNZnK/nBvWaMlgMt6daFhWZsD+SMCvL/a09vNsg7bra5OOEVdsbCiVzhZ8zrX7UyFaM/IGT9u/xHQD6ZgJhbalQM7TlbczzZ+5rIWWXK92/9d/tWz4HjUVqQPcPt627ki4QrKR5enfHR5ypQqE5Tl1I7dQtu2Vlu/5SwFZ9SAUR7pL/V7iVm3e/r4BrIyN6zsdvGxXrIs7C8MAOOuHefxP+VsCpdUv8TlBuH9UtEvsR/V46q7ost4E1UqikuquV0qnF5I7ZMej5w54mgF9PYB98ZSqubtnseYgWOYNlS7Q9Qs775mZu3w9Yu+ttm1fuscvXK06/eCPQtcoxX2/qRJvHZXGd55uMti6q0EW/tXtkxZV5p9FMtSsBbt0y9LltJq7fOqQ6s4mek8d6NsVFkuqXaJ6+WsID/iU5mnXCNLRYW1L/bj4q3kglvx9MYavZiwZoLrhXHBngWAPgZWP2FFsEnNSnWNcCVUTgD8h+e0W8ftlC1Tll4NeznmgedoGujoSXZXIOuFDOBg2kGmbpzqEWGnUZVG+GP8mvF+8yysFyXvOTm1ytfi+mbXu9zaGlVp5HJ1sfDn+lMQ3239DtAv8rFlYoMyDoSDC8KSLgglDWsCFGhfTG/ubO1/kRZ/WB3t9C3uVeEsa0inOp1Crq84MDA8fHDPleX36ZEHuxtGceAUTaJW+Vrc3fZuH5cKgDta38GkGyYVWIc3tcu7/cwLGye9Rrka9GrYi2ubXcv/9f2/AstbLhP+ZExqm8TBvx0kvmw8WblZHgqXRaCJu2XLlOXnO3/mh9t+YOilQwuUx1uZeLrH0/Rv0t8jrVy0tmRfWv1Sn+0tSye4Lb9WWfsLQ81yNbm/w/0e+5xn5BFfNp4ND21g7DVjuaXlLR51D2s1jENPHPI4T978ut+90LXdnWlLinZ/8J78ZndLsMjIzfB5obBjKSZ7T+3l4U4Pc3PLmwHttmDhpDgObjHYb53+2JK8xaVAj1s9jkHNB/HY5Y+5FELvCaLW8X93gLN7nJNF1Ztpf0xzTJ+9YzZxL8bx5aYvg5bfYlHSIlbe72uptiugrnZ26qgvGbkZPi9MlkJtt9DaXQwt/Pm7WyilXO4d1su6QrmUf6eJvx3qdODhjg/7VdL9ubhFlY5iYdLCgPJY/Ln1n6lboS49GvTg9NOnSX4y2WdUwFsx/uzGz1zyFwbruv1p108e6eseXMfLV75MzfI1+XTIp3Rv4DlCmZqVGlTIUTveIyfrjq4jKzer0Mp+caOKY1ngkkzHjh2NVatWFVxQuCA5euYo9d6qx+K7F3sMVYO2UrR8ryXbh2+nStkqVH21Kp/f+Dm3XKYf5Dl5OdR+ozajBozitlb+F1EZMWuEa4KMP/om9uXnO39m76m9JLyTQJuabVj7oLZcJM1IIiM3g2lDp1HvzXouH8itj2yl+ejmfDrkU4a1GsbgaYM5mHrQYwi1edXmVClbhWX3uq1L6nndsU6+YTJ3ttEvB1dMvoIFexZw+unTdBjXweWP2LZWW37/y+8ALtkSKmv55u6aS7mocvyj+z/4YPUHLstuxZiKLLt3GZe9d5lLIbqj9R3Eloll/Jrx9Enow/y7fH2VZ22fxdAvhzJz2EyumHwFb1/9No9erl1unpr7FL8d+I3dp3a7lISvb/6aG7+40aOOhzo+xM6TO/lp508+9QN8fuPnbDq+if8u+i8Aux/dTe3ytYkpE+NxbACM5wxmbpvJoM8HAVqhs+dZ9P+kP3N2znH9f6D9A66hfG9qlKsRtO+4P/54+A9avqdHTA48foAvN33pWu2xToU67Byxk7L/KxuoCg8SKid4RJCoElvF5ZfbsU5H1h5ZS9o/01x1RpeOZsglQzyG12fdPosBnw7wqfuetvcwca32Ptzz6B4W7l3IXTPuKlCmsdeM5cGOD3LzlzdTLroc/+71bxq/67mI9DM9nmHcmnGOyqk/hnceXuC9CHBnmzuZsm6K3/ynuj3lclMqCipEVyDlqRSiX3Bb96qWrUpKRgpxUXFBhzGsWa6mX//5gqgcW9nDUt2zQU/XnIPnej9HnQp1XC+Sf7v8b6w+vNrls28x/675XDH5ikK1D5D3nzyPBa6ub369ywJq0axqM7Y+stXjXg1ExZiKPopdKVWK1/70Gk/89IRHekzpGEZ0GcFrS19zpT3S6RGPUQfQir73qA3AC1e8wIytM4gvG8+8XfPY/ehu18Rr+30VDDddepPPy0nbWm3ZeWJnQH/qL4Z+wc1f3RxUG5dUu4TNyZvp36R/wPCU1eKqFXif/bPHP3lpyUsByzjRsU5HH2t5KNzc8mbXpN2C7ls7Qy8dypc3hf7yVxQopVYbhuEYk1KUdC9ESb8wOJZ+jFWHVtGtfrcCJxfZmbZxGrd+fSs3t7zZNYRs8Y+5/+DVpa/ycr+XaV2zNQOnDgTcClry2WSqv1adqmWrcuTvR8jOyya2TKyP1SPYh4nxnEG/Kf1cE6qsdqztl9y9hB4f9QC0D+GYgWNo+0FbJg2axF1t7wrYjl2ptJcb3GIwN116E8O+GebThve27/z2Do/NeYxgcFJU7Z2x8ZzB60tfJys3i2GthpFYJZEO4zqw5vAaD0XDeM5g1PJRjJg9wqeNxMqJHpPcANrUbOMxAcqbjwZ95ONv/liXxxhyyRB6NuzpcWxubnkzh9MOO06OXJS0iO4NuvPkT0/y5m9vFnA0iha7kr70nqU8OPNBjwmgkwZN8onqUBIxnjNc52PH8B00GdWkgC1KNqEoGJFK3Qp1XUaEwpD1ryxiXih4RdBAL8KRxMxhM7lm6jUADGo+qEhHFUsSVyRcEdQk8vNJjwY9WHx3eLyjAynp4u4iXJCMWz2Oa6Ze4zNxrCCsxRza12ofsJyTT6rl0zaw6UCGTBtCuRfLMW/XvJDa98ZpWfF+if3oXr+7jz/z4Gl6KPsf8woOc+eP6VumuxR0C3/LZtsnTNr9mp3wXn2wb2JfnzJPzn2Sf83/lyt8nmVAsPzqLZwUdMDHJQIKXlrdaULo28vfptckX//NL/74wlFBB+g1qReZuZl+h+mdSKycSGLlxKDL+8PyUwU9ic/bL/VCUNC9cVqNM5C/dUmkpCvowDkp6ODrPmGfrGnnRGZwiziFG0tBh/MXTSSUIATni2Dj1Z9PnNxOIwFR0oULEmuRi0AL0ThhRWZoHN/YJ89SLC+vd7krrJQdy9/1shqXufzenCbi2f1CvSfSBRO1wHIZWbzXrTBuPOaefFrY4W0napWv5REyzj4Zyu7f6y9kmoW3T/yJjBMuK3rT+KYeedZEtBsv0a4r/RJ1uDxruXR/2H2RLX4/8nvAbYoSwzCCVkoqxVRi6yNbKRsVvBuKE1GlojwewsfSj7lCMV6IVIurRqMqjRwnr17Iq0GWVAJF3vAX/cSO90u2v8W5/IVZjGRCWbDuXHisS3CjnUVNoChTh8/4D3NaJbYKLau39JtvuWReLIiSLlzQhLp8dClVytFFBaBnw57sGL7DZ6U+V1um5Tc7L9tjYqg3z/Z81hXnN6lNkitiArgnJQXil92/eExOA2hTq43Luu/P2gRaWba7uthxsrh4v6zYQ+3ZH5iWZcRf3e1rtyf1abcfqD0ywMRBzhFSn+31LMZzBl3qdiEuKo7n+zzvWM4ilCXYi4Ngr7Va5Wvx7oB3iX4h2jHOdyjk5Od4rBR6+MzhYlNWA0VwOF8cf/I4O0fsdPTlnbFlxnmXxx6mTvDl3nb3+s3zjuITKYTil2yfmBwq/kJDFjW3t769wL6zOCjsKOFzvZ/jj+N/+M0PNMKU9x9fo1hJR3zSvbiYfdJz83N5Y+kbXNX4KjYnbz6nDqgglu1fxsnMkwxsOtAjfd6ueUSViqJ3Qm+fbZbuX8rpzNMMaOqekDZmxRiGXDKEuKg4xq8ZT/W46nSr341RK0YxasUo6lWsxx8P/+EK4fT1pq9pHN+YtrXa8uisRzmYdpDGVRqz7MAyBjYdSK+Gveg+sTuPdnmUd5a/w/e3fc+RM0e4r/19bEnewgerPmBElxEYGK6Ja/n/yafHRz24p+093Pf9fbx65auuxVEAPrz+Q95Z/g59E/pSt2JdnpzrjjXtz2ezelx1ltyzxGMlzf5N+jPjlhmu1UMVyq9i+MNtP3DtZ/6H/7+66Su+3/Y9O07s8FH4C+L006e1BfdFX6V+cIvBHtFlLN6++u2g/dfB/+ShLnW7OC4UFAkEM5lKKDzP9HiGF5e8COiIM/5ixwuCIBQGf0am4kYmjobAxaykT1gzgfu/d7tbFOcFa00A827DX7pT3s4TO2kyqgld63WladWmft+wk9om8dGgj1x1dKzTkQV3LaD8S76ht1pWb+n4Fp/+TDofr/uYB2c+yBdDv6BDnQ4uJd07qshDHR/yWRI7VDrU7uCzCANo5XrolwWHsStOrmx0Je1qtfOIeCAIgiAIJZn9j+8Py+iYTBwVgsIe1suyPBcXg1sMplWNVudUhxWv+UTGCZpU8R/pYcm+JajnFWNXjqVexXq0rtHarwXa3zCbYRiuMIR7Tu3xiH/8zvJ3PMp+tPajkPbDCcsf25twK+igI8k4xfAVBEEQhJJKTp7vGhDhRpR0wYU1ua9RlUaOSyWfDzrX7UyfhD5BlbWWJq4aV9W1nLCdTQ9rf19rQuHSA0s5kHqAY2ePeUx6DAZvH3P7anPey4wXxaIIry97/ZzrKC7qVKgT9DkSBEEQhJKAtT5GJBGapiJc0MRFxdGsajOPVeuKCye/ZdAvCLFlYh3z1j+43kMBbhzfmNSnU4kpE8OHaz4EPBfc8A6HZ7l2/bDth6CV9E8Gf0JilUQfmeyW+JjSMR4RUKJLR5/zS86JjMgNKbbp+KaAKy4KgiAIQkmjalnf0MrhRizpFwknM056rB7nRKXYSlyZeKXrf3p2uiusn2EYHqsQgl6Bc0vyFsd6z2SfIflsMntO7XF9LOyz2tOydKzYvaf2cuTMERIrJ1K7fG2fUFuH0g6RlZflER/5bM5Z9qfuJzsvm7m75gJ6YqHF8wv1jHYnhTnYFc3qV6pP9bjqzNw2kwV7FwB6pUn7YjF2BR1wXCb+QmLxvsXnHDpQEARBECIJp6ht4UYmjnpxoU4cDTQh02LFwRV0meBWcod3Hs6sHbPYPnw7Y1eO5eEfH2b5fcvpXLcz6dnpHhMvveut80Ydn1io8++aT5+EPj4rYVrL2Xtz+InD1CpfC8MwKPVf9/vkx4M/5mzOWdeS1MEi0TcEQRAEQXDi2N+PUb1c9YILFjEycVQIij+OuSdNVoypSHp2usu9xArTZ7nCWJM27STNSOL5Bdp67bRYgdMCJOB/gQrL79t7kufcXXNDVtCh4NUnBUEQBEG4ODmddbrgQucZ0VouElpUa1Hg0u2pWakevzNyM1zuKFY880urXwpov2uL+hX1CoeT101m5MKRfuu3tvFe6rlSTCWn4n45mFq4paY/uPaDQm0nCIIgCMKFTSR6loiSfpEQXzaeKmV9l7IPxGcbP3O9WVoTJ60Jl/aL2Vo9086zPZ8loXKCR9qB1AMedVlYK2V6cyjtEOCr1Ce1TQpyDzyZvWN2obYTBEEQBOHCJtQVys8HoqRfJESVinKFLPSHPcxg5djKnnkookpFuRRme0z1NjXb+NT1Qt8X2DF8B7/e417N0lpZ03v59oaVGnLyH77LfPtzd7mk2iUB98MfH6wWS7ogCIIgCL5EoiVdQjBe4JzIOEGzUc1IzUqlV8NeHnm/H/6dKz++ks1/3UyNcjVccdIBj4gt6nlFXFQcOfk5TPx9Ip//8TnP93nelf/mb2/6xBd98qcnmbJ+iis6DMDwWcMZPmu4j4w/7/6Zqz+52ie9+8Tuun0vS3rH8Y7zKwRBEARBEAqFv/DP4UQs6Rc4x9KPkZKRQk5+jst9xOL1Za9zIuMEc3fq8IXlost55NvdSizL+dvL3+bImSM+EzdfWvKS6/fS/Ut5fdnrHgp6IB74/oGA+ZE4BCUIgiAIwoVDzfI1wy2CD6KkX0RsTt7s8d+asGm9PZaPLu+RXyrA5dE03nOFz6GXuperX3tkbUhypeekh1ReEARBEAShKPEXgS6ciJJ+gXM4zTcUosWVjfTCRU2raoXbO4Z4anaqzzYVoisA0Lpma7/1Lty7MCQZ61aoG1J5QRAEQRCEouT42ePhFsEHUdIvcE5m+k7ItGhTsw2jBoyiToU6gKcFvFJMJcfQiGnZaTSr2oy07DSP9K82fVVoGS+rcVmhtxUEQRAEQThXUs6mhFsEH2Ti6AWGYRiMWjGKMqXKsDV5K4v3LfbITzmbQnzZeLp+2JXEKol8vvFzBjQZQLW4aqRnu91OTmed5kTGCcc2tqVscy1q5MQXf3wRksyfbvg0pPKCIAiCIAhFiT3CXaQgSvoFxsK9C3l09qN+8++ccScjOo9g+cHlLD+4HIDfDvxG4/jGPmWnb5kedLv9Evvx8+6fQxdYEARBEAQhzFSPqx5uEXwQd5cLjGpx1QLm1yxX0yeKy487fgTcCxUVBlHQBUEQBEEoqZQuFXgtmXAgSvpFRtd6XYkvG++RtvLgSgDa124fDpEEQRAEQRDCisRJF4qdLclbAubn5ud6rBYK7hnNFWMqFptcgiAIgiAIkUp06ehwi+CD+KRfYOTk5QTMX314NSkZnjOYT2WeYlvKNvad3lecogmCIAiCIEQkufm55+T2WxxEljTCOdOoSqOA+R/+/qFjevPRzYtDHEEQBEEQhIjnTPaZiHN5EXeXC4Tn5j9H+w/aUzaqbLhFEQRBEARBKFGUUpGnEosl/QLh5V9fJjsv229sc0EQBEEQBMGZ0kqiuwjFRHZeNqCHawRBEARBEITgMTDCLYIPoqRfYDSs1DDcIgiCIAiCIJQoLGNnJCFKegll6f6lNBvVjPTsdP4686+u9PSc9DBKJQiCIAiCUPIoWyby5vSJkl5CeeXXV9h+Yjsfr/+Y91a950qfsm5KGKUSBEEQBEEoeURa+EUQJb3E0rVeVwC+3vy1R3pefl44xBEEQRAEQSix5ObnhlsEH0RJL6Fk5mYCMG/XPFda13pd6d6ge7hEEgRBEARBKJFEYuANUdJLKJ9v/NwnbdmBZbSq0SoM0giCIAiCIJRcokpHhVsEH0RJL6HUr1TfMb39uPbnWRJBEARBEISSTVQpUdKFQmAYBtl52R7+5pfXvTyMEgmCIAiCIFw45Bv54RbBB1HSSwBbkrcQ80IMY1eNdaWVjYq8UEGCIAiCIAglkbM5Z8Mtgg+ipJcA9p3eB8B3W79zpX216atwiSMIgiAIgnBBEYnGT1HSSwBKKZ+0ehXrhUESQRAEQRCECw+Jky6cEwaG63f9is4TRwVBEARBEITQyMnLCbcIPoiSXgJQaEv60TNHyc3PZc3hNR6rjAqCIBSKtJqQ7ztSd85klYOs8v7z80tBbhQ2u4MvZ6tAbnTBbRno/QiGtFqebWbHQWYF9/+80s6/d/eB5GaedeWXgmOXepYLRF4ZONBJb+eYH2Q9BZFZQR//wmBQPNdDJJNZMbjrLC/yrKxC0eLktRBu5KorAVSKrQTAhmMb2JK8hQOpB8IskSCUQCzlo5SDZvjNFDhbDW4fCApIbgprk6DWOmg0T+dVOATJzeHHMdD8O+j4PsSe1PWl1oEVf4XOo6F0DhzqAE3mQEY8zHkTOoyDBkud5do0GCochh394WAnXe/0KTBwONRdARUPwLZr4VRDXV/5I7BpKFQ8CN1fgdhTEL9by7D1OjjdEJb8E8omw9BbYcsNsPIRaDsRrnkYSuXCsVaw6kFY/RctQ5tJsC4JGs+Gm26BnLKw/s/w+z0QfQYOdXbLe3cPKH8U9nWHmhvg97vh6idA5UF2eZgxCbbe4LufXV+HynvhcDtYe49Oq7MSjFJwuAO0ngJH2sKx1s7H6fH68NZ+z7QBw2HWKM+0S7+ETTc51+GPWmvgSAjha6NTIbuib3rpLGgyy73/1f+ADh/Asif0ebEodxQufwuqboNF/4asCnCyic67sx8kt4A19+rjYbelDRiuj/Oif8GZOr7t390TPlrsmdbhA9g8BM5W90zvNBpSmkPtNbCrHxzu6M67ZbBOT60HE391p1fdAikt4NZBUGMjxB2H5SNg/gtQKltfi81mwsHOkNQHyiXD8r9CVAZc9pmWu3S23r+4FNh4Kwy5HZrOgWWPwe4r4NqHYH9XfW22mgobboNjl8Gdf9LX/nsbYMif9b3wxy36muvyrr4/TzXU92e1zfDnAYCh78/T9WDeS7rN9BpwojHkR+n8I+2h6Q9w/f3w8/90m7XWwvz/6nv5yqdhTx/4fDoMvkvf8/V+A6M0HG4Pib9AfmnIjdVyVjwAlQ4GvHx8MAC7fpiv9D1acz3kRUN+GYgOYlLjvm6QEweN5xVcVvAhvmx8uEXwQRlGIFPGxUfHjh2NVatWhVsMD05nnqbDuA7sPLmThUkLqRRTibYftA23WIIQmOSm+gFT8w/fvLzSkNIMquyGqEz4PUk/IJt/D71e0A/Rmush7qQun1VeK80NFsORdrD6fjjeEhrPgRuHQek8WPSMVjzv7wxvHNZKwxN1YOsg+O5Dd9sJv+i2+z0Lbaboh+NIsx+840/ww/twsrGvzGUyINdhYlGN9f4VSzvXPAgz3y+4nCAIgnDemTMHrrrq/LerlFptGEZHxzxR0j2JRCU9Lz+PAZ8OYO6uuUwdMpVLq18qSrpQONKrQZlMiPFa/vhgR6i0F3b3hYQFUOGo77an62krVK3ffa3RBto6Nv+/2ip4sAtkVtF57SZoC/MHa6HJjzBgBIza4d528B0w/WNneZ+sDtsHwIwphd1jQRAEQSiQjz6CpKTz364o6SEQiUr6tpRtNB/dHICpQ6aSk5/DXTPuCrNUwnllfxc9ZN3331Amu+Dy64dpi3D/R6H9RzotOw5eTAfyYWRprVjv6wHffOI5FF/hIDzQAd44ov/HnoCWX8DqB/X/ivth2DXaJeOovCwKgiAIRYjK0+5E55mMDIiNPe/NBlTSxSe9BODtg77p+KYwSSKETG60dq2osdHT59CbfAUoKOWw4ll+KfjwN/273DG4ZDp8N0G7g1z5tPbx3NMbpvziu+13E6HhYkhpCqcbmIml3O4dTqTVdSvoAJnxbgUdILU+vL8+wM4IwkWO5btdEAm/wJ6+nmm1V+nRrGV/D63NuOOefucNF8DePvqlOrWAaGBOrlylsyAvBuothZONIL0WVNyn/b/nvu5bR4f34Uxt2D7QUblnlQAAIABJREFU9Pf2Q/1f4UgbPd9jf3eddncP2DEAFj+r5yZc8g1Mm+HepuXnUOMP2DxYzw8pkwV1VkFcsp7jsL+HLhedBpX2QdkUaDpL97u7+2ojQ3Jz+OV/UHkPnKnldmmrcACuv0/L3XQWfD9O1534s+5bL/1azzHZ2wt6/59OS62njSYqHzqOhf3d4I+bodNY7ZO+/nbtm1/xAKRXh9/v1f11gyV6xLHmek8f84Mdtb99dBqUPa3TssoDBsSk6xHQPX30cbGeEYfaQfXN2l3QIqscLPwPtP8Q4nfg8sl3wvKDzysDpXP9ny+L9Gp6zkEwZYsKAz1nxem5WAzExkae0Vos6V5EoiV9/u759J2iO/Jf7vzF9VsII1nlPV1G8kvpiUPek3um/AS7/gRDb9ETymLT4EQj3am3/sTsJEvD/9k6vqTeekLZNx/D7isDy9HtNdh1pfbTFoTzRYtvYMsQ//l2pa/KDqi2RY8C1V4Lrx6FszXgjiv1BL1p30DUWa1M5UdD+/Ha9WrD7XBDEhhKj/Rkl4fWH0OZHLeC4T3h7kx1GLseBjyqFZ7Vf9GjSRtu1wpb+SOw4hHtQvVYonbNSq+p5drXVU+4veQbreTVWaWVkugMONlQK1yl83Q7BpBdTk/srL9UK0IGesJtdIbnsciJ1S5mJxP1ZF+V567Hjve+2NMyK2iFuVyyc7lgsWQJtH0w9Rtoxb3KLufzYJEbrScqB6tkncu+CcI5YjwXHn1Y3F1CINKV9A0PbaDV2FZhlugC5FB7HaXiiv9AdgWovM//A23RP+GXF+GahyDmNDT7AV5O1XlPxevJjmerAApeTfHctuFC2Nvb/f/mIdpCs/yx4tw7IdJo/q2e0Boq1zwENTbAR0sCl7vjSm153N8NMitDXpTvpNXaq7VVdMZkqLwLHjMti8cuhWMt9fyDWaMhfhvUWQ0tpkP1TZ4TgTMqwbbrdCSYAcO1ZbTpTCh/XCuWsWm+suVGa8U7Kiv0/RcEQSgmREkvAUSikr5gzwKumHwFAKefPk2llyuFWaILEG/3DyskXexJeLyBtppvuBWOX6pDpgkXJw+2hiVPw8ZhwZUvlQOdR2lludY6PYG25kad98Z+SDNXDq69CtpPcCvSzWfArYM9I88k9YaERfr3yYaw8yooewK+/Epbi2e/o/Mufwv6/81XlrPxuvwLGZAXC20mw6C7YefVOhRiuRTnbeJOBLevgiAIJZhIVNLFJ70EUDHGHY9398ndYZTkAuJAJ1j8jI63G5Pum78uSX9nVoGX0nSEkt/vO68iCoXkin9pF4YVw0PftvUU7b/bYRx8/ZlnXstpUGsDDL1dfyx++a9+cWs3Qcc67/tvHSoyOl37zPrz4bynJ7xj3s9/6aS/D3TVcxhuudF3BCfmtPt3lb3Qcbz+3SJKt9F+gna3cFK2wa1sP9gO1twHPf+n3TSazvZ/PERBFwRBCBuipEcwhmFwy1e3UK9iPZrEN2HHiR0SetGbA5203+mfnvJUKOy+jccuhWlfQ4tvtd/4mvsgx1yR76Uz3jU6Iwp6yaDlNOj9Pz2BquJ+PWFsxXC9mIyd6hvh+GWeaW0mw+Ak9/+MeL0wisXlbzm3ecV/oN2HeqGeUPxpq+yBwX/Wk4Et7O17E5fsnG69BESfDW7Bk+pb4OoQJyUKgiAI5x1xd/Eiktxdpm+ezpAvAkzOulixJmhlVoE3vVZ2a/6tXgjHWwETzp2YU5BVuXDbVt2qVzcE6P4ytJsIo7d5lmn+LfR7Bt6z+TzHnIYRjeFoa/j5RT3T3776ZekseKiVnsB2KhGq7sCH7DhY9rhW2DcN1atR3tkP1tyvJyse6AI336SjOXhHQsgqb04urAG11xVu38+Vgx30yECzH8PTviAIwkVAJLq7iJLuhSjp55GUxrDuTuj5kmcYqYIIFD5QKD5GKr30vPeLkTdRZ/SS4OPN+6jFN3DrjZBaW08ybDNFn++MyjB9Mmy7Xke++fNAXX5vdz3a0f8xdzgyDznM819/Cdzbs8h2TxAEQbh4iUQlXdxdIpjEKonhFqF4sVadXPQf6Pq6jnJiWUlrrdHWy4cv02HOVj+g43zX/j188pZU4rfDiaaF337Q3dDE9FuueChw2ZjT8FRV5xBzFQ9Dx3Hu/2VPwbBBOmye3ZWj4a/64492H+q4w13eDX4fBEEQBKGEIUq6ULRkVtCT3xrNc8fGPZEIE5fA4Lug8Tzn7bwX7jjSXn+/csoz3Vq04mIj2MVRnHj4MjjYyTls30gFI/OAUs7b1lwH7SZ5ppXK8VyspNur0GUUbLxFx5i2FPRrHoL5z8OV/wwsX/njwe6J5rr7oc9IqHSgwKKCIAiCUFLx82QWIoHUrNRwixA6n8zRn3kv6//zXoR3d8GZOvDxXNh4k/YRzi4buB7Bk790CJxfxcEX26JMtrZM33o9NP0BOo3xX7brG3DrIHj4Uuj7DNzXxbdMUm+IMyc7Vt+oVz2tdAC6vwHxtuhDnd6HJ2tCtW2+dZwLpQxR0AVBEIQLHrGkRzD20IsRT24UlM7RVnSApU9CvWV6BT87X31x/mWLdK76G8x9FQzb7Tj0Fvhqmvt/VAFRO4wg3rdbfK8/u/rCyr+60+uuhINdoNbvnlE/amx2rqfBMniqprlks/K/7DTI6oGCIAiCUEjEkh7BnMkOMjxguDkbDy9k6+W97XzxjXN5wZNOY33D6132hbZoWzgpu2Vsinujn4NvL/ak5/9bhkC31+C264Ovw5IpkIIuCIIgCCWAqxpfFW4RHBElPYLZllLEbgLnwuKnYdmjkK9gXzfIK63TDWDJP/TvLYPDJl6JRuWDsim795sL23gr0+3HQ9uJMKKRdl15vIE7r+f/4Kn44Nqr8ztc8W+42YwcVPEQXPWUuJAIgiAIFyWta7QOtwiOiLuLUDA5sfDzS/r3nLf1d6W98EBHeC3ESX8XG/d2hU03uifGVtkBD7bV7iv/NSfWYkDjOXqV0/q/Ql0/IUCvf8D9O343ZNrcoaLOQpxNqa+yE0429i9X7xcKszeCIAiCcMHx+rLXee2q18Ithg+ipEcw8WWDtIwWBwawYCRsvR6iHdxuTje8uBX0rm/oCZPv7ITUBv7L1f9Nfywlve1kiEnXv1t+DnnRUCYHBj4CDX6FFtPd2xbkh17Kttx8Ka+Qh43nwKqHg98fQRAEQRAiClHSI5ThPw7HIIz+vptuhIXPha/9SKbOCudl1QfdDetvh5ob4LfHtVXcm6o2F6abbnP/jkmHDhO82lkNXd7W9TlhV9KVqaRfdx9svBWu+rteYbPS/uD2SRAEQRCEiEKU9Ahl9MrR4RXgy6/C236kEXccur0Oh9vDgBG2DNuMznaT9Cc3Wke2aTzXnfdga71Y06VfBt+mAgY87j+/VI6trOk60+FD/QFIXBh8W4IgCBFEpzqdmP3n2VR9tWq4RREuAu5pe0+4RXBEJo4KFwfljpzb9k1mQY9X4aZbofyxwGXLZMNlX+oVNS1qbYAuY4o2Goq9LiVRVgQBoHfD3uEWgb90+Eu4RQiaqmWdleD0Z9IL3HbSoElFJof3kuyzbp9FfNl4KsdWLrI2nJg2dBrV4qqx7ZEICtRQBJQp5d8GO7L3SDrX7XzeZHn88scxnjNoXTPw5Ezva8CbI0/o5/h7A98rtCxlyziv0TKiywjH9HAjSroAGZVgyVPw6xPw4zs65vmFRpVdvmmlM4PbNjoNrvmrc16iGfqwtp/JnucLlVdwGUG4COjfpD9PdXsqrDL8vDuEkKhh4MEOD7p+Jz+V7FhmyropBdYzfcv0Asv4o3x0eZ+0Hg3cK0pXe60apzJP0a1+t0K3EQxtarbh8nqXc+MXN/ot81K/l4pVBid6Nuh5Ttvn5uc6pl/Z6Eq2pGxh7ZG151R/KLz121v8bc7fWH90/TnV88zPzwDw8I+e8602Pbwp6Dr+3s3BVZXALzXhJDKlEs4PeWXgcDuYuATyo93pKyLzjdIvd1ypVzg93NF/mc6j4YBXZ191GxzzerO/4S6YMdkzrf0EiPETs37gcL0YUChuLEXJkGGQEwfRGeFpXxAijMzcTF5d+mpYZdhxIsAKwBHA+6vfd/2es2OOY5mP139cYD29G/bm263fhtx+g0oNqBRTiQ3H/My3MTmdeZpeDXrx4/YfQ24jWL7a9BU/bPshYJkJayYEzC8OFu9bXCz1zts1j4oxFcnOyy6W+v3x1m9vnXMdE9dOdEzvMK6AFblNykWVI6qUsxHS6aUxEhBL+sXMj6NhwgpPBb0k0vhn/+4eLb7Rq3e2cHiQKAPu6wwDbW/l5RxcWdqP9992zBno/B6UD1Okm9afuX3QBaEEM6zVsCKp57Wl5xZGbd9j+4pEjpJC/0/7O6YbRsEudOWiywXVRlLbJI//+07v81DQ46L+n737Do+i7NoAfk96D4EESKNDKKGFEAi9NymC9I5UxYqoKPqiYlf85MWCiGJBrCCi8iIgKL333nsLnYSSkMz3x2RmZ3ZmtiSb7ELu33VxsTs7O/ts2p555jznBAEAVp1YpXvuzSw7Va7y6ZNNltQJs1n7w1cOu/x1Q/xCcHrcaZcf15aaJWsCAK7fuW5zvxF1RxTGcAw9WPVBp59z667jk1R/HvzTcHtBp1XlFYP0oupmcWDzvZM3aSpurfS/vHDSWt+HpO6dfjelrprNX7V6/kYg5VPggTHAsKZApUVA43ekbp8v+QMTA4GSewv2PRDdQya3nFwgx60RVSPfl/gB4IUmLzi0X8ngkrpt7Su2R3x4fL7HAABrHl6juT+o1iCXHNfMXwP/ckmecYPYBrjy/BWMSx1nc7+KERUdTpn4attXmvvfPPiN5ns9s4vxTHVUcBQigyIdeo28igmNUW4PrjW4QF9LLT0zHTGhMXZTs7oldLP5OGA5ybFHTulQpxUBQFxYnOb+2JSxqFO6jkPHdLVmZZqZPvZosmVCLSooCtUiqzl17Hox9TAmeYzhY45+DQsbg/SiZlcvYPpm4LPN7h5J/g1qAwxsL902C9LV2j0HtHzF+LH6nwFlV0kVVdpOAKoukBaA+jqYt073DEf+GB978phyO+c/Ocj+j+2cf/ViJPVMbMO4hs4P0En1om1f6rW3WAuQFmyZXQa23u+lZi85NK61w9fC18sXD9d5WLMgTJwkGi4Qm7t3LlYMWwFxkoiOlTrqHvcS9B9Xwb7amdxyxcrhsZTHNNvaVmhr+JqhfqGY23uucn943eFYNHCR7jU6V+msGfu/Q/81zFF+rcVrmvup8anK7U86fYJPHvhEkzv7Xtu8z/hvGbVFt23l8ZVYP2K9EnRWj6qu26dHtR6GxytXrBwAoEJEBawbsQ7FAoqhZ/WeeKrBU3i9pXHjswE1B2DzWcvnyCedPsHjKY/bHbs4ScSg2oOwYtgK9KreCwCQHKNPVTz8xGEE+Qbh8QaP675/LzR5Ab1r9Nbsv/rh1YZjLR5YXHm++l9UUBQeSX5E+XrN7zMfY/6UArgVQ1eYXtnpVLmTsoARAH7q+RPESSIiAiLsvncASCiRoLn/Ttt3TPddO3wt5vedb3NRa8O4hnipqeV3Un5/Rj8jsqkdpuKXXpYKbiefPqk5+Q72DcbW0VshThJxfYLtWXcAeLLBk5p1Dmbqlq6L9hWlz2wBAsRJIub1nqfZ56c9Pxk+V/23Z/oD03Hh2QvYM9Y8Fz2xZKLmfv2Y+vh36L8YWmeo4d+Dwk7/cRSD9KLml5+Ac0nAtXLuHkneRKhyPSv+DQTcyL2j+oWL2ejgwVgRpShy5PJ5uanl0Lp8a1SMqIg5O+eg0Re2F6+pL7cevHxQub393Pa8D9RB6kDJiCOLtV5e9jKG1RmGDSM24J025kGDM45eOYqsnCx8ue1LzNk5x+7+csB2/Opx/O/Q/3SP54iWE/GKEVI33YwsbQWS23f1J9VLjizRbZP3VS8WNJuF/uPAH0pjua7fd0Xzr5rjhb/1s/X/+ec/mvvn088rtx9d+CgOXT6k6X0xMmmk4evlVVJ0Ep756xmcuXHGdB9BXTJW5djVYwCAvjX6arbP3jkbq07qU1AAYM6uOSgdUlq5/+jCR/HlVuOcYTM/9ZIC3MolKgMAWpdv7dDz3lr1Fq7dvqbZdur6KcO0lMu3LuPd1dIahYzMDExaPgkbTm/A3Zy7mp8ptbs5d01/ZoN9g+HrbTmhlb+njjYf3H9pv27b+23ft/kcW2lF606tw4UMfZqm9Umt9cm8dR+WUfVG4YHKDwDQ/o1Uv1ej4wLA1PVTHVpE3LN6T2XW3qwPTI+qxieS6udki8aTJuoyitY/SxvPOBoXeBYG6UVBlj+wvzPwr2MzYC4Vu872449Xtn+Msv9I/5dbbr6POic9/Lj9YwKAd5b9fajIunX3Fvx9/PHppk+x/vR6u/vHhsYCAK7cuqI5RkGLD7OfnlG+WHmbj7++8nXM2DIDXoKXyzodqwOgAfMG6B6vVLyS4fMe+fMRu8c2SlUBgOiQaHgL3g6NzzqgNwtgAcuist8P/O7QsQHgk43aMnEfbfhI87UVBO3rtSjXwvRYdUvXVW63Lt8aYf5hun2C/YLxwboPlPv1Y+rr9rF+TWvWV5ku3ryoBGzW+dqHLh9ClypdbB7PWWfTzyq37c1M/3VYv+DVKIAEgOeXPg8AOHn9JF5b8RrGLx6PK7ev4LPNnynfd0ebB3oJXrorOIDzJ13qhYpm45bduXtHuW1UQtDo5LRUSCndNjlwLRZQTFd600vwUr4G3l7emu1qZr9fZl8/Xy9f/NzrZ8zvMx85Yg6+2CqtoZJn1K3Ty2xVWZFf26xyjfrnu0qJKqbHuZcwSL+fXagG7OsKvHEb+P53YHnB5JLa1N52XiOKHbXcHtYUqPuFtNizoeXDBr16A60nSP+bUae7tB8HRG8GevU03jcgN4iKcXPZRPJoa06uwZ60Pdhy1vyysdrpG9IisK3nthbIeMzSdLpX7W7zeTWiaiDAJ8Ch10j+PBkjf3fNDK+cQmHGesbMbEGXkbWn1hpu33puq27mz8zEphN1zzVz4ppzi0mfb/y8Lmi5ePOiJsD5fLN2Qfrp69pFhOrFltZBidHMqjwbLrO+MlCzZE2MqDsCU9pN0aSXyIsJAWD5Mf1EyIrjKwAAdUrpc5StF5dWi3IuR7jf3H4QXhVw4JKUzrEnzfFSetbKhJcxPfGTyV/Hy7cuK9tG1RuFapHVNGlh1oGi+muZnpkOP28/ZYGjIwtsjahTg8Ytlj4nW5ZrCUA6QUksmaj8Dqlnyo1O/Kdvno6qkVV1KUBqm89uVr4+gT6BmqsgAPDFli+UKjrqk5DsHO2stZ+3vtDEuIbjTP8OZeVk4dLNS+hWtRt+2WNJsZEnF6xPeIx+BmVyEK7+/sg5+5NbTlZOAML9w3VXlJKik0yP68kYpN9Ptg0GtjwMnE8ENjwCfLIH+MH58liFSsiRZtMfqSnlhHcbIS32VM+Mh6QBTd8Bgo3r+QIAqs6X/o/ZCBQ7CYxOBmrMNd53ZH2gxX+Atu6tpXy/Keh6xu7SuExjp/a3FyjY0iC2geH2lNgU3HjhhuFjaTelykIVIirg5NMnDfeRTyBcRU5N+W+H/wKwBBdqZpekZY/U186Yh/qFApDqVsseqmZeu1pNnU/v5+2nyf1uU6GN4XMerqvtMOjK6g5vt3lb93OQHJOsCS62n9emQlmXbVQvtlRXQtmdthth/mF2Z5rlmUpACmB3PLID7Su1x7jUcdg4ciMax0s/1zsesaRDGc20y0GU9eJCQF/1pHd1GxMpAGZ0nqG5Lwflt7KkwFPdiMrerL+1zOxMJdg3E+4fDkCbA9+pcifsGbsHFSIqKNvKhJfRPG9e73nK1+HPg3/iRuYNzN8nfebIqTpHrx7VPKdfYj+sG258JTk5JtlwAfarLaTCBuNSx2HnIzuVQDo6NNrm+6oQUQG/9/sdU9pN0T1WPLA4YkJjUKd0HdSPrY83Wr2BUP9Q3Qy2+sRXnUZm/X1oW7Gt4RisU8+ebmjplj3mzzEYMn+I5me+admmyvjU/H38AQBdE7oq2+RZf7kqjboK0PTO03Hy6ZOatTIZWRk4e8NyVQaA7j6gDdwdPbkvbAzS7xdX46X63gu+AD7dCSzMe0cul7LXCVMQgRKHgFK7rB5wcnaiwVSgfydgkPEfEI0Sh4EWk81rn5ND1DORo+uNxtcPfm1jb32XuGkdp+X5tZ9v/DwG1hqIhf0XKjPM41PHY2x9k6ZTkBaG/dzrZ6x+eLWy7c1WbwIASgVLl4Ynt5yMt1q/hXXD1+HMuDM4+PhBzOs9D1tGbVH2faz+YxhWZxgAYPHAxUgsmQgvwQtpz6ZhRucZGFJ7CPrU6IOPO32sef2TT5/E4oGLTceXHJOMdhXboWx4Wd1jbcq3gZfghVFJo5RtExpPwMphK5UPtQsZFxAXFod/h/6Lzzp/hiNPSA28dqftxuZRm5WTqPfavoeF/S11pysXr4xDj+vressBg0wdVESHSEGDr7cvooKicCNTfwKx4fQG5faH7T8EoF2EWa5YORx47AB+7vUzNo/ajJ96SQvGJreajAE1pfSYV1q8gv9rb1xfuUW5FkowtWjgIvy3w39xfcJ1eAlemhld64VpAHD2mbMIDwjH5lGWfH71h7w6mFanF+wYswOrhq1Cv8R+yrbPu3yOy89dxsphK/Frn1+x+9HdAKRqLiuHrcTtibfxftv3MbHpREQEWgJr9Uz7jM4zcOOFG5jWcRquTdDmWgPAmXFnsHnUZux+dDfWj1iPIN8g5fkty7XEH/3+wMikkdg3dh/+GfIP0p5NQ8XiFZXnG10J+KP/H9gxRgrQjz55FL/1/U33tdr96G4cf+o4No7ciKcaPqVsXzpoKfY/th+vtngV60dIqWBmKUgAcHrcaZx8+qSuFKOR+jH1sXHkRpsnTd6CN65NuIbBtQdjcG2pIsvmM5sxd680MbOw/0K83fptZX954WV8eDw2jNiA6Z2no3hgcTxW/zH9wQHNlacBNQcgNiwWR560NMSTTyoASyrZ++0seeUCBJQJL6MsRpQr+zSIbYD32r6Hvwf/rUkpkTUt2xSrH17tUIWiaxOu4dwz5zCv9zxsHLkRlYpX0pxIlQ4pjfUj1uP0uNPKSUlSdBJebPoiQvxCbNYFP5duWRgb4BOg+Zv0TOozuv0/WPcBZu+YDQDoU6MPNo7ciDdavYEnGzyp7KNujrVp5Cbl+2adlvP1g19j08hN6FG1Bx5Jlk7kL926BMDydVeftJYOKa07gVw0YBE+fuBjrB+xXvl7r06lksmTC2PqjbGbcuQunjkqct5tx1aWFzrRzmyI2cM+TlZV8c4GqvwPCNR/wHmycQ3HGV4+zItJzSe55DjW1JfD1dQzdUevHrWZ0wsAo5O1JT8TSyYa5nbK/L39ddtKBpfEh+0/RNXIquhYqSMaxTfCDw/9AADok9gHlYubr3FoGNcQPav31Mz4rzstzXRdvCldpYkKisKEJhPQIK4BokOjUal4JYT6h6JudF3Uj5VyfJuXa67M9ibHJCM1LhU5Yg4igyIxst5ICIKAyKBI3eXkjMwMNIgznikvFlAMdUvXxe27t3X5pRUiKiiX6dXNPJqVbYYmZZrAR5ACyqzsLGX7qHqjUD7CkodeqXgldK4sBchlwsugY+WOSgm8QbUGaQI6WY2oGpqgul3Fdsrt/26QZtDXnlqLjKwMbDpjnjpWpUQVzSylWuUSldGzek8kRScpQYOPl49S3cXf219ZJGpdiq9vjb7K91uAgMcbPI5Q/1Ddaxhtk783ZpfA5fUFANAloQualW2GFuVaoGapmmhcprGmbF/NkjURERiBJmWa4MGqDypVVQRBQIPYBsgRc/BMo2d0M5KiKMLf2x/PN34eI+uNRLBfMB5LeQxh/mG6VKHo0GgkRSehelR15cRE/ruRdjMNqfGpEAQBCZEJaF6uuUNlC4sFFEPNUtLvdrli5dA1oavua1U9qjpKBJVAckwyAn0tJyvhAeGoUqIK/H38kRKbgsktJ+PnXj+bNie6lXULGZkZpgs1Zf8e/xcbz2w0rPailhKbgjD/MHz94NcoHqBfQ9EovpHm91ye7QaA+rH1TdO/5Iou6nGuOyX9jVCfhKgfl68YqYNeESI2nN6gnOwJgoDxqeNRtlhZVI+qbrimQD126wBenZ4jC/MPQ6mQUuherbvpOpKU2BQE+ARgwf4Fdstl2rpyUTdaWhPhJXjh0s1LusfVJ7WlgkspPy9mJ1r1YswrUgX5BqFeTD1kZmfqZudl9lKMqkZWRYBPAFJiU2xe4T2fIS3uNnsdT8Ag/X6Rrl8kck9rNEVadNrFfU0VCkPZYmVdVvrJ3gdgXll/YLYo1wLNyjZTZk4AqSKHs3VmX/j7BZt/HO9k39Ftm9F5Bn7b/xuG/TYMA+YNwL6L+5QAVhRFTWBqrfSU0rptC/YvACAFQQNqDjANJgHLZf9Qv1A89Zc0q3jtzjV8vV1/BeHjjR8rubyAFGBW/bgq3l9jXMXh6u2rOHb1GLKys3TVZ67evqpsU+cmy5d+m5drrrwHaw9WfVDJtZX/lwPAZmWlesRmH849f+6JPw78gU6VOwGA4Qz/9TvXTavlNIpvhGDfYHSu3FkJkuylQ8jkCjmXb11WgouooCjdfvIMpb38d0eoTzKPX7MsPn+52cu6fdUf/A2/MC+zWXt6bQS9WTD1l0fXk056d13YhRt3jFOhCkr9z7WLUl9q9hKalW1mmupVaVolVP24qssqbKhTmOTbyTHJSnUaX29fXY6+tcu3LuvWNrSrIJ2Iqk+U1bPKMvVVkMOXtSk/ctrW8mPLlWB7/r75eH/t+/hp9084eOkgnGX0Oya8KqDqR1WdPpY1+aqY+uffeoJBliPmYPIKfZrO4ymPKw2QWpbXp77lxbZz27Dw4EL4ps8uAAAgAElEQVQ8VO0h1IiqAcCyFsNefrn1mo0mZZooJ/tq8mfYtzu+LbDPz/xikH6/mG1+Gd2t7KW7mAm8CoxMve+7aWbnZLssF9YoWHSFEL8QTeOR41eP4/jV45ixxZJfuv/SftMV92bkGSpHyH+UX1z2omZh0YlrJ5SW3jvO78hzsHLq+il8t/M7w/JoMvnDWh3AZedkm14Nmbp+qhLMyTP1uy5Yp3VZLDmyBF9t/0qTOhLmH4bLty4rXRHVVzXkWU/5w9VeUGIdjL+x8g3NfXkmUJ0TDkgLaAEYlvZTn4joXg8CMrIycCf7DqpGVlXqYztCXqx7PuO8kqqx96K2qZg8C+wqZicrE5ZOwIrjK/DPsX+UbXIgZo/1mNWSopNwJ/sOZmyeYbqPo69lq+xiYYieEo13V79rmPerZj0LK5cFdLRzqUwdtMqzuD5ePsrJmrfgje92fgfAvDRi8cDiSI1L1Wwb+ttQANLflT41+gCwnMwCUiqOdQ3/I1csaTBJ0UnKSfPY+mNx/Kr0t0Ld4VO+CqXm5+1ns9mVuhKLOrXD1t8rR5QrVk7JMZfXKAD6RerqAN6s5Kt8QrL6hCWd0PrEvk7pOppcc9n41PG61/xk0yfK302Z/DfKbO2OMharCjRh/mGaVDOZOh3H2c+vwsIgnVyn2WuAv1W6SbClVjACbSz8LKIyszM1LZjzk/ribAUKR60/vV4JLkP8QnD06lEcv3Zcl8Nn/QfVmr10GLmxjLqrnEzuQmdd/UEQBCX/ODYsFv8e/9fma8jkBVZyLrrMVk3xk9elRZlGi9PMLr8mldbO+NgqLwZIs+Zq8oe7PMujbpktp2QsOLDA9Hjz981X3tOSw1K98I2ntbOZ8qX3DSM2YNcju7B08FLDMalPTmTyZfiqkfoZPXkh3ccbP8adu3ew8/xO3fszI5+AmDVY+rLrl6hSoopyomfvZ88RZt/DPw/+iepR1TUl3fITFL/Y5EUAUlURAOhWVd9RUh2EGOX7A1AqWQDOL7LMq4G1BhpuP5d+Ds8vfR7f7vjW5vOtx/llty8hThKVGfjW5VtrgkUzH6y1VP+SF9yevH5SWciaI+YoCwHNykRevX3VtERqjpijBMbq7pylQ0ojMihS83da/Z72pu3FocuH4O/tj2DfYGVRt5rR34D/dvgvhtQeYvxmoQ2aP+/yOWZ3n226r5HOVTobzj6fvHYSaRnSGB+u+7CyINx65l6dHmT0GfXhug+VOulLj1r+fgyrO0zTNGhQrUGGHVSTopM06zzU5u6di91p0joP+TPELKCW1wpl5WjLKy88uNBmSp4ns/2JQfeGTaPs71MYqs0D1j9hud/hSaD4UaDrw0BadaDMauBH+w0PbMrrzLyLda7SWZnBzY+e1Xti+qbpyv38pL4E+ATg9t3b8PHysTkrMK3jNOy6sAufbf7MoeN+/9D32Hh6IyavmIy327yNW1m3IEJEQokEdPyuI86mn8WMzjNQu3Rtm8dRXyLuX7M/Xm3xKipPs+SKyo1ljD44e9fojQG1Bmgusz+R8gS6JXRTSveZXaI1Iv+xl/+Yv9DkBby16i2sPrna9Dkjk0bi4KWDeLnZy1h7ai3WnFyDUP9QzNo2S/eh8E6bd9CyXEvUj62PejH1EBEQgWVHl2Fis4n4ec/PumPHh8VjaoepiAmNwcoTK9G8bHPsurALV25fgZfgZbPd+9QOU7Hi+Aql0oraH/3+UALYV1q8grs5dzE8abhmHznwVy+2/LLrl0jPTMe59HP4evvXOH3jtOGM7iedPsGRK0eQdjMN+y7uU7Y/mvwoulftjnfbvIvm5Zpj8eHF6PpDV3Su0hm/97Nfa7xLlS6Ys3MOykeUR8fKHTFz60zM6DwDv+77FZ0qd8KwutKH8UvNXkKQb5BhAFK7VG1NTr09tgLdUsGlNL9T3at1V2Zdv3nwG5NnSZ1BFx3SdjGVg5xg32D4evkq6QZqPav3RNrNNDSJb2L6Hhb2X4j+8/rjyJUjpj8fUUFRhoFiXvl5SWO3XhRt7edeP2Pn+Z3om9gXvx/4HYklE7H48GIldcrMkStHbDYb61m9J37Z8wu2jLaURR1YayD2XdyHLlW6oEZUDcSFxSHQNxAzOs/Au6vf1SzoVJNrds/sOlP3WLOyzdAovhE+2fSJZhZ8QT/phPhW1i3Uj6mPMzfOaCoQfdb5MxQLKIauP3TF4iOLsXb4WuxN24v2ldrjyJUjOJd+TldVCNCv17GmTr+JCY1BREAE3mz1JjpW1nfmNZKVnaWsWVH7qNNHyslnRGAEPuzwIeLD4lEtUltK863Wb+H7Xd8DAJYMWoLmXzWHt+CN8Y3GIzIoEk3LNEVmdiaafdXMMPf9z/5/wtfL17QyTL+a/dCvpjZIXzlsJbac3YLKxSsriz4jgyIxofEEdEkwPvF6vvHzytdH7be+v2m+j7JPN30KAHil+StOp2sWFgbp94M/HAu2CswDY4CEBUDYWSDpC2Bt7urv+rnVPJJmSf8fcqDyyj3ix54/YvHhxRj1+yjDD8G/B/+N1t/Y754X6BuIaRtsVzl5rcVrum6GANChUgdNACDPBO4YswPVP9G2BN86eis+3fgpZmyZAV8vX0zvPF0XpL/Y5EW8uepN3euUDC6JPol90Cexj+6xNhXa4Nsd36JkcEmbq+PVl4wFCPiux3em+6pr6QJSAOvn7afLGZzacSoAaUEmIDX8SI5JxudbtPWnbZFngq1n1I0E+gZiWifpe9WnRh+sObkGPl4+yuIjtecaW8p7yhUtjGZMZe0qtkP3alKdYXlxqbxQVU2uXgFIAU3t0rVROqQ0zj5jnGbwQJUHlNvhAeHK+NWsy80BUIJgQFocevrGacMgtmFcQzxS/xGkfiGlDVSLrIa9F/diasep8PHywbONnwUA/L7f8SZA1uSZx3ox9TCynraGe2xYLKa015edA4BtY/TBQq1StXTrDl5u9jImr5ismSGsW7ou4sLilOZF1rWbw/zDEOYfhut3rttM4akbXVdZdCdTN8bKyskyrPnesVJHNI5vjFIhpUx/r2qXrq1UkjEjnxgb5ePmRcXiFdGjWg88Wl9/tUse05qTa9ChUgf0rC71qZBP/uwF6IC+jKG1n3vpT3DVP9c1StZAjZJS/nJ0aDT+r4NxZSB7vL28lcB4wf4FeLvN25rHA3wCsGjgIgT6BGpK96l/Frad24YAnwDD3zlnydVNAGD4guHYdGaTrq29LUaNnwBgTPIYzf2Y0BjD36eyxSxpK7VK1UKXKl0wJnmMQ99TwLHvvbUmZZpormIA0on0W23eMn1OQmQCXm/1um67UYoNYJkUm9SiYIouuALTXe5lnjGpDNT+RgrQAaDNBGBwK2BiAODtmTlerpCdk43uP3Y3naVqP7u94XZrp6+f1nWqkytHJJZMBCDlMBrVoR5ae6jmvrzQcthvw3T7NpzZUMkhl2ecrWdL1Cvu5UvygCX4L/V+KbywVFsaTJ7lkxcGquscq8mpLADwn+b6Ew41dZ4gIF3GtnWFQb68uidtj/K1s1erXA7U5A58SqtqBxuTqNNQOlTq4NBzrPWoZmnmY11CrM70OhgyX3/5W90AKCEyIU+va03OOXeGfGlergoj/yzZysF2xrKjywBIKVz7L0p5t67Iu57Xe57uioOcv6pOx+qb2NfuLPzoeqMNuz/aI896yyc96lx32cwtMxHzQQyu3c5ftSr5KoqrFsUduHQAm88Y5yQDlsYyeS1n16p8K11gVpBspRfKwbdRis+NzBso8W4JZSbWiJx64QrqakJ5SdvoUqWLpmuts9QTJ3fu3sHvB35X8u0d0fLrlujyvWu707rCvdDbg0H6vepaHDDlDLBUP/NZ4Mr+Y7ndYwDgp0pP8L4LVFgO+Oorc5jXW7z32Ftk4ugilPCAcN02ORjZfUGaJetaxTILoC6t9tX2rzTPk7vNyTN1cvUHQFspRQ4IrdNK1Itt1DPq8mzchYwLeHu1dkbJmllr6LdXva0EQepgSG4uYm3Z4GWa+9Y5xzO7WC5RJ5SQgtVSIaWUznzWjWHESaJm5qlJmSaY23uukq9aIkjKPx1eV5sKYkZO/fAWvFEvup7DregBKb2kZsmaSiUKwLKATrb9/HZNXeGC9O8x23n8cgCuLhsn35Z/zo0qYOSHnJNdOqS0ku9ulBbirJeXv6y7ciXPMqp/Z+fsnKN0X7Run55f8gytfEJolEY0a5t09dHoEn1e2JuhdtS/x/81XJsAaAPJe0FEQITmb6Qz5AXqZot+/bz9HLo6d6+QF+EClnx1s/U7Ricn/xz7xyXpoa7WMLahzXrxnoBB+r1q5YtAejSwyn7TA5cT+WNjFow6K8AnAB9t/Mjma1QsXlG53J6RmYEJjScAgC7XNTEqUXPfrFayHIxYVwP5++jfNvc3Ilf3kI9lVu1jytopyvv546Dlj7XRgsNZ22bpGlwcuHQAVSOrYkq7KZjWcZom/1wdxFnnsz7d8GnD+vH+Pv6ID4vHosPS11Auo6auBe2oE9dO2O2uaU0QhDxV9Xl9peVSrnXpN2cZLeAy0jexLwbVGqSp1CAHuvKs3v8O/U/znPw2BpGrfbiqh4Ds+13f48N1H2q2pWdKTc3U38MzN87gfMZ5dKrcCV0SuqB52ea6q0TvrXnPdOGhLe+teQ+ANLvt5+1n2IDr2p1ryj75kZ/ZUyNmFYSmdpiKX3r9oiwwd+akVW3Z0WWabpKF7ceeP+Lb7tLiV/nqnXrNkEz+Wyb/7FjLzM60ub7FWfLkQ179fuB3bD23Nd/j6F61u7KQ2ej9hfqFurRzb0E7cf2E6ffQUzDauldlu/bDyympH9jfx57nI4AXHCtjplEpNxiIXW97vwKWnWM7KHN0EYqtRVLygqS9F/cqC+Nu3b2l5LhafxA+Wv9RZL6UqSwEfHf1u8pj6sZA8h9R6yBenom3pp5psE5FkcuRyQ1crMv3qckz6HJTHQBYN2Kd0rlULpU2tM5QJH4inXDItbHbVmyLUP9QjEsdh+92fqcs2gMsM5N3c+7q8qvfb/c+XmnxCoRXBQivWmbwV51YhZSZKcosZusKrXHiqRM2y6CpyeUur925hviweDt7a5UNL4uy4WWVhViA/uSmWEAxwzxKdZfX/DbgkLsa2kvXaVq2Kb7p/g2igvV1yuXqK3KHUDPVoqqheGBxmxUs1PamSWkzaRlpyomAutRdflgv8pVL7alPLNJupmHdqXX4s/+fmNVtluFxxjUcl6fXl08abS1UHZ86HoDtLp6OsLeY21n9EvsZVtx5osETSI1PVZpL2atiZKZ+TH2lJnZByxazdc3UetforaS3yBVN5K6XavLVlWcbPWt6fFemUhj1KHCGujtvXsiz4x92+FA5zhMNntDtd+ypY5rOxLIqJaqgYZx5PwF3iQnx/Ks/XDh6L7oWB2x17LK8y7R/Cvgrdwaq4pL8Hy/QsVJsOm2fB2I3ApUX2t/XSlxYHE5dP6XZdmviLQS+4djsaemQ0sjMzsTlW5fhJXhBnCRi1tZZeHiBfrV+xosZmqDQWmpcKtaeWovLty4jMijSsITc4NqDMXfvXMzdMxc3J97EU4uewlfbvlIusUaHRmveT3pmOkoElcCvfX6F8KqArJwsfNb5M4z+YzSOPHlEdzk67dk0HLx0EFU+qoLZ3WejSZkm2DRyE5I/l5oXta3QFosHWervGy1UeqDKA5rt28ZsU963vF2+LwiC4TFeb/W6brGPXMP7wrOWGaS0jDT0/Lmnrr66PItrXZ7R1sIquRJJTGiMUwuwZHIpwRwxB2+0fgNvtH7DzjMsPuwg/R6pfz4al9GWnbvy/BXD577e6nWIoog3V72Z7xmrBnEN8vTeAWnB6LpT65RAZnaP2ZjdYzYazGyADac3IEfM0QS9lYpXwqXn9J0KzexKk2ZkL2RcUNK/HC3fmFe2SoQalfac0n6K6aJVW/7T/D/KuozM7Ex8tPEj3WK4sSljMTZFP8PurABv6ftj1L03L+Y8NAdzHppj+rg8054tZsMbzs+mbxi5Ia9Dc9q1Cbbz/X28fEx/PwJ9A23+7uT198pMVHAUfL18kZWThYoRFZVSk45aPmS5/Z1s6JrQVfOezN6fWU36/Y/lr557QZnacapSgMBTcSb9XrT+8cJ/zdSpwGNVgNF1AT/z2d8C53sbqD0bCNK3SbbHOkAHjNstW5ODDXud55whL1wM8g3SzN6qZ8fletzyLLVMTkuxfj9Gs1fyh+adu0ZrBCy5qj/s/gEATOv/5sXK4ysd3lcURSw+vNhmCseRK0cM02lqlZQ6acaFxbn1UnlhkRf4ujMH2GyB7QOVpWoy1ukuGZkZWHx4sd1mNzK5hJq/j2uCS1veXyuV6LOVWlIvup7LU0dk9q5C5Ie8dqCw6qj/uPtHAPkrJUvG5Co5n3f5HL/2yWcpY7pnMEi/J+XjD251fQkrpBrUkQ3VB7SIPAhE55Y1K57b2riM44HY/UBe7CcHD7/t/83uc4xm6FqUawFAvyhNnRcrB0Jy+sqnmz5VclWNyB3f1OTFb2bPkwMTuYauOiiy13zIjFy1pNlXzezsaXE35y7az26Pqh873+ZaPuExy8EvCHJg5WjnSXvsLd5Uk38uHK1EU5AcDf6WH1uO9rPbY+TvI+3vDKkkJSB9b+WayOrqQ64kp0vZei+h/qFKgyVX8vXyNZ19dAX599m6Sybde+RGQ8F+wZpF3HR/Y5DugQq0Pa1cu1zNy+r1avwAtLXUecZYbak+AMCjNYDxJYFiJ107Pg8k5yRWj6qu5OPJM95GC3qsy/8ZtbxWB8D/Xa9vQgNYSjDKrGen5AWkMrPZclvksfav2R+AtmlGXisfWOfrD6g5AN2rds/TsdTMrlrIJyC37t4qtLxH+YQgr7m31mydfFlbekQqOemqih158VTDp5TcejW566CrSv65Wq1StTRdWwFLbr76e2ndvvyfY/+YLorOj6ycLMMSjK4iVwiyLvFZUIxq+5Nrjfp9lEM9OOj+wCDdw9y5ewe+k41bYQOQaqOvMV+sYlf5f/TbrLt4NvwQSPwBaPQe0L8TELVP/xyfLCDEdZ3s3MWR2WI5SN+TtgeJJRMxpPYQZdbaqDrJwccPau6nZ6ajc5XOeLiOJXd9+/ntAKTFf9YLk1YNk1I25EBHzoe1bsLSvpK2Fru60oSjpQStyyLKi/PGp45Xmus4y/rqwuweszGvzzybz5HTI9SdE21V9lDn2J6+fhqA9HWWF9qp67IbkU9ObC10tUX+/ucnSO+baCnBWK5YOYef16p8KwBQFum5Q9/Evjj21DFEh2rLIsoLpq1/r+Tvi3Xqlhn5e3oz6yZuZUk/166YvFg0YBFmdDYum6f2XKPndP0LCkpBVpeQG/sY9VkoCB0qSouQ81rdhczJqXzyZwcVDVw46mFsVfuACOBVF1ziDjsJXFdXpLA6ZvhJwEsE2j2He4F1583xqeOVPFN7rC9xz+o2S9MMaGH/hcoHHSAtlFR3cWwU1whfbfsKk5pPwqv/vmr4GltGbUF8eDwiAiLQIK4B2lZoi5jQGPSq3ksJ8iMCInDl9hX0r9lfaS6z6sQqrB+xXqkD3qlSJ8zZNUfJo994eqP2vagCo64JXREbGotQ/1BsX7LdtGavfCVg/en1GFR7kFKX2aiLprP2PLrH/k65vL28Ne91/Yj1NgNgdcWNCU0mIDU+Fa3Kt1IW1X6w9gNNsyDrhU5NyjTB4oGLdd0gHTWszjAkRSflKwWiX2I//LBLWgtQp3SdPB/HkyzsvxAHLh3QpV6lxKZg+ZDlaBzf2OSZWo83eBx1StdBckwykqKTEBUcpTmByyvrkwrAclKZnZMNL2/pZNG6RfnhJw4XSHrRppGbUD6ivMuPK/up50/YcX4HUuNTC+w11OR0PVeVqCWLH3v+iP2X9qPBzAbuHgoVIgbpHsYor1jxt+MVJGwanQScrg/Mya2QYj2THpb/zn6FKS1DO6P/Xrv3TIP0LlW6KK2+w/3DlQVqMuvL9Op29kbk/UckjTAty6gOBEfVG6Xcblq2KQBo6qTP2TlHmd0VIWoCk+uZ13H51mVsG70N+y7uw4lrJzSvoy7X5iV4wdvLW5nZNPu5kj9MXRmA9E3siy1ntygLnRylfq9egpdu8WHNkjUxp4d0kqJ+P95e3srssvz9sFejWIAAby/vPOfdB/sF57vEmvr9OfP1l7txHrx8UHMC6QnCA8JNUx7kdRiO8PHyQcvy0uyvl+ClfH8LgiNfe+urWK5SUHn2smC/4EIL0AHpZxLgTHpBCA8Id8mJKt1bmO7iYWw271j1ovljZupNB3r10m4LvghU+Z/x/sGu7RyoITjX8MVRm89q21TbKn2oruF67c41XfUI65bX9tIZ/jz4JwDpRGFyq8mY3EpfI9ZZcvDXpkIbzXY5v7R26drok9jHZv7ngv0LMOmfSYgJjUGj+EamH5pyYC9/kMsLIfNTn/n23dvIyMxfDe/6n9dHw5na/PJgv2D0q9kPX2z9AkPnDzV8nlnFE6M66a2/aa2csLnDnJ2WUnYrTzi+APvKbak8o3r9AOWdnKOe3wZMBFSMqAjAzmQTETmMf5U8jKs77MErG6iUmwoStdt4H19VQCUU4IKvcv8A5ZcCzY3TQgrau23eRatvtDNyNtOLYEmHaVmuJZqWaap7XF5cFhUchW4/dEPH75yvoqDunmnLkw2eRLh/ON5b/R4SP0nU/ayoFx9+vuVzAMD59PNYc3KNwyXR5BlDR9MSjMzfNx+nb5zO8/Nld7K1C2HP3jiLytMqY+u5raadHh3tzCfP+G07ty1/g8wHdTMjZ77eckqQq9vVF3WFVabwfiaXjbXX7I3yrqBKgZJnYpDuYVw2OyaXWqwzC/BPB14MBh6ppd1Hntku6/qqBYa8s4EhbYGWrxTKy1lfGnxu6XO6uuhyDrbai01eVLrqyR82h68cxqHLh3T7qi+VL9i/QJMb76je1XujWEAxlA0viyG1h+DgJSmAtG7Oo34fu9N24+Q1bWUdo4Yl8pjNglp5oeh3O7+TjuHjj4QSCZouo57i6NWjht8DtfWnHOtEa71g9l5Sq5T0e+zoyR3Z9s7qdwB4bkWae8m8vdICceuuruQ6n3f5HEsGuaChIN0TmJPuYeRKBjrpTqYf9OoN3IoAgnI7Fxo1IHqqLHApASiryt+1zk/3EDM6z8CoP0bZ3xHSjPPU9VNRuXhljGs4Dn3n9rX/pFwRgRFIiU3BrG6zsDttt9JR8e3Wb2tqmMt+3iOdDBnN4FaIqOBQK/NsMVupUw4AvWr0wtpTa3Ut1Keu13ZG61yls+a++hLzymErserEKqW5yLXbxiX+WpVvhUeTH8ULTaUydFUjq2LfYwbVfJzQq3ov7LywM1/H+PrBr212STSr0PFWm7cwc+tM3fbPOn+mWTw7st5I7E7brXR+dIdlg5fhuaXPYdOZTVhyZInD1XTkAMjo55Gct2rYKszdO9dl5TSJClJBr2Mgz8KZdA/z/NLnjR8452T1BwGWAN1M+GmgwjKrjZ4XpIuTRIysZ14OTW7yAABD6wzFhx0+xMwuMzGg5gD0SexjuujMqK164/jG6PZDN6w5uQZvtn5TuQTer2Y/DKw1ULe/nB9sdHm3fcX2um1GPtv8GTKyMnD82nF8v+t7hPiFYEaXGQgPCLf5POv66+oTvCZlmmBCkwnWT9Hx8fLBxw987NI6yhlZGflu4T649mD0Seyj2SZftUiNSzX9eYgMikTXhK66aimj6o1Ct6rdlPtBvkGY0WUGIgIjrA9RaFqWb4nH6j8GQPqaOWr1Cemk2nrhMOVN3ei6eL3V6+4exn1BXk9zL16hIvJEDNI9jJwrq5Pt4lz1+8i41HHKbTlV5I2Vb+D7Xd9j/r75SjUMa1dvX9V8mCwdtFRZjPnJJoOmTwbkSixG6SElAks4tBhtTL0xKB5YHOWKldPUzrZm3dzImqcsJFx4cCHOpRfcAmR7ucOzus3CXwP/KrDXd6Wo4CgAQKWISnb2tGherjmAgqs4QpRX8oQIF44SuQaD9HvF9+6rQuFu9pqYfLXtK+W2/OFw9OpR/Lj7R3y9/Wubz1UHfE8uetI0D9yMnPduNOt9Lv2cU3mu9krBtSrXSjf7Xy/acunT6IRAbqBUmPnL7k4bmLllJt5Z9Y5bx+AouRpNzVI13TwSovyTF+IXRE15oqKISXgexu31ZT00J10txC9E06VP3Va7YaylbJ9RRZPxqeNx6dYlzNo2C1FBUZo66bvTdivVCRwlz17niDl4q/VbmhMKo9xoI3Kd9Mu3LuOb7d/g6weNTyzSbqbh6u2rOPzEYZy6fkr3eIkgfbUPX29pAWxhVq7oVb0XNp3Z5PLj1ouph3+G/IMqJarY3E9OGZvSforLx+BqcpqUek2CPX8dkq4S7L+43+7VFaLCJK/NYTlLItfgb5KHWX5suZtH4J4g/fz48w6XlHuhyQua+/sv7VduqxumdK7SGQNrWvLIfbx88HTq0+hYSSqTmHYzTQliZc6W5JMXlgb4BGBCkwl4qdlLymPPNnrWoWMMrTNUuS2XdDQiNyaqEFFBabJkFJir9ajWAxtGbEDxwOIOjcUVrty+4nApRGcE+ASgebnmhl0j71VympT8vXVEl4QuAKR6+USeJD5M6mTNIJ3INTiT7mECfQJNy+Xdb6xbtber2E5TOzqhRIImAJf3P3z5MHLEHLy8/GXlsfiweJy8fhIL9i9Ax8odNce2fh1HAsjeNXo79B5G1Rul6SKq9m7bd/Fu23ftHmNWt1maNvdmZnadiZldtbPzfw38C+2+bYclR5bg+p3rumC8ZHDJfDUmyou8lKEsqhIiE3Q/n/Z0Tejq9HOICsOutF0ApMpDvvC1szcR2cPTXQ/zdpu39Rs3OVZ6MF98c9NHIuyXDFeYYR8AACAASURBVCwo6gB9RucZmgAdAHac34E3V76Jvw7/pQnQAUvjm+3ntxsee+vZrXh71dtIz0y3293xrdZvYVRSIXzNXUTuTBrsG2xnz8Jhq3QiEd2/ft8vrZ2yt46IiBzDIN3DNIzTtkJHekngj8+cO8hD/Zx/4RENgdpfAz30ZQYLgpxyYmbevnnKbbkCy5sr38TEZROx4ri++VKD2AYAzNMGJi6biBf+fgHHrx6HaJXS83GnjzGo1iAAUgm/+LB4w/KMnirQJxCA51xifrDqg3bzxomIiMg2z/hUJ8W0DdO0G66Wc+4AXUYCNX9w/oVL7Qa6D5VqpxeCY1eP2XxcnTIhV2yRy/ot2L9As2/P6j3RLUGqgR0fHm94vDM3zgCQGsEkRycDkAJyAHi0/qN4LEWqV50Sm4KBvw7E9E3TnXk7brXm1BoAlsoK7nbx5kWcvl44P0dGulftjpolWS2FqLDJa4I8ZcKA6F7H3yQPM3vHbO2G//3XPQMpYHsv7nX6OVUjqwLQX22Y2HSi0tberG18jZI1AABh/mEoV6wcACmYBICHfnoICw8uxPoR65VmHI5WZvEEd+5KqT6e8sH499G/nWrO42rf9fgOa4evddvrExVVjeMbA/CAKmVE9wnP+FQnc6cbOPmEgltQ1rN6T8PtkUGRmNJuCraM2uLQcRYNWIRDjxsH0wAwvO5wzX05v/HDDh9i8cDFWDRwEf4a+Be8BC/UKV0HdUrXweazmwEAvl7Gi5U+7/I5/h78NypEVFCC8596/gQAmLd3Hl7991WkxKbcU2kusm+6f4PlQ5YjNizW3UMBABx/6ji2jzFeG1AYAn0Ddd1YiajgyZ2GrVMKiShvWN3lflOAdc7bVmiLX/b8otv+cJ2HNV0/zZQMLonrd66jfaX2NvcrE15Gc1/OSQ/wCUDbim0BSJVgUuNSkZUj1ZcO9w/H9TvXkRqXanjMIN8gpRueXD/d+nXuVSF+IZrSk+5WJrzMffO1JSLHFZXKZESFhTPp5LDRf4w23P7Tnp8cen6tUrWQFJ1k+vhzjZ5zajyrT67GhtMbAADVoqoBgFI/3Ba5znaof6hTr0dERObkzsaeknpHdK/jb9L9Juiiu0dgKCooCqtPrMaak2tM9wn0laqUyJVB7FUIaVW+FZqUaQLA0oRo/r75dsciL2q8fue66T4DaxVOlRsiovvFlrNSyqPcSZeI8odBuic7X8P8sTImtb6r/F4wY7FBFPUpNt2rdtfcv5tz1+6l0KVHlgKQyjOuHb4W5YuVd/h15ZQYszrpaqtPrgYAnLp+yvDxjzp+hNH1jK8aEBGRMbkqV7bIIJ3IFZiT7smOtjJ/zMukWYRXweWkNy3T1LARUFxYnG7b510+x6/7flXubxm9BeWn2g66P2j/AX7a/RPC/MPQMK6h0hlUEATD/ZcfW67cblymMebtnefQgsGpHaYi2C8YXapI7dXn9p6reQ9jU8baPQYREWmtGLYC327/1mMaqxHd6ziT7slEG98esyC9AP3Z/0/D7b1r9FZuf9DuAwDA1nNblW0/PPSDUvbQlpTYFLzf7n0lKJeDaEfyG9tVaAcAiA21X+EkNiwWXz/4Nfx9pM6YPar1QEpsit3nERGRuaToJPxfh/8znVghIucwSPdktoL0RFXDotT3C34sMA+WO1TqoNyWZ7/VzYoqFq8IAGhfsb3SGdTVftgtfT2OXj3q9HObzmqKl5a95OohEREREeUZg3SPZjIbkTgHqDPLcj8htwNn7DqXvnqfGn2U2/P7zEeAT4DhfqtOrFJulwopBUDbzELOHc8Ws53KVXx95evS80wWIbUo10Kp5rLx9EYAgL+3v8PHl606sQpvrHzD6ecRERERFRQG6Z7qbG1gzTPGj/UcoM09D74AvBACDG/k0iGo01hS41PhJXhhWsdpSI5JRreEbspjuy7ssnmcK7evAAD2pO3B7gu7HX59owWpaldvX1WaZ0QFR0njNKmTTkRERHQvYZDuQS7fumy589k2ID3GsScKOYB/hssXjYb7hyu3S71fCney7+CxlMewceRGTerLvL3zlNs/7/kZAHAj84ayLcQvBACQWDIRtUvXdvj1R9UbZfPxtIw0pGWkAQBeaf4KAGBE0giHj6/WuUrnPD2PiIiIqCCwuosHUaeIOEXIce1AcqnzygGpjOL8ffOx8vhKTeUWNbkGeZh/mLJNDqSXHlmKHNHxsdpbBHpqnKWE4pA6QzCkzhCHj60mTmILayIiIvIsnEn3IA51aWvyln6bUHhBZt9f+uKDdR8olVcAQIT+9VNiU5SKKfJiTmcCdADYcm6L6fGJiIiI7mcM0j3I7bu37e8UcUS/rQDLMVrXu72TfQcA4Oftp2yrEFFBud2/Zn8AQGRQJBKjEvP12ufTzwOwNCoiIiIiKio8IkgXBKGrIAhLBEG4LAjCbUEQDgqCMEUQhBJOHMNXEITHBUFYlXucu4IgpAuCsEsQhPcFQShVkO/BFTKzMx3YSzWr3GwyUPsrIPxEQQ0JGVkZhtvn7p2r3FYvIi0ZXBIAsPnMZny57UsAQEyog7n1VtpXbA/AwSsMRERERPcRt+ekC4LwKoD/WG2uBGAcgB6CIDQTRfGkA4f6AUAPq23BAGrk/ustCEJdURQv5XfMBeXQ5UP2d1LXR29l/WVzLTnglpnlzKuD9Ls50qz+2fSzyray4WUBSPXUNYtj7cjKyQIgpbtwNp2IiIiKErcG6YIgNIUlQM8B8BKAvQCeB9AQQDkAMwG0t3OcStAG6NMBzAVQC8C7ALwBxAPoDeBTl70BF1O6dGabLCB9qJ9UxaWQtCrfCiuHrYSPlw9uZt1UOnQCwKHHD6HStEoAgPIR5ZXto+qNQq1StXD9znVlmxy4p2em48YdS9UXe95aJeXfi6JoWjKeiIiI6H7k7jyCp1S3vxRF8S1RFOdDCqblvI52giDUsHOcYlb3nxVFcakoih8A2Kfa7gcPpuR2bxtqskfhLaCsWbIm/H380aRMEySUSEDZ8LIQIGBml5loW6Gt0kXUWph/GNpVbKfZduvuLQBSN9Jz6ecKfOxERERE9zp3B+ktVbeVtpW56S3qROtWdo6zC8BZ1f33BEFoLQjCOABVc7elA5ifj7EWOGXh6BXjALgw7bywE38f+RsAMHbhWFSaVgmZ2ZkYnjQciwctdupYQb5BAKSTkMolKjv8vEeTH3XqdYiIiIjuF24L0gVBiAAQodpkPcWqvm8zahVF8TaATgC25G4aA2ApgCmQUl2WAkgVRfG4yVhGCYKwSRCETWlpaY6/CRc7eOmg217byMnr0lKA+fukc5tsMRs/7f4JIxeMBABUjayKUL9Qw+dGBUUpt8/ekM6fFh1ahA2nNzj8+nIXUUFgrgsREREVLe6cSQ+2um9d2kR9P8SB410DcABSbru1RgB6CSbRniiKM0RRTBZFMTkqKspolwJ36vopTN883S2v7Yz+c/tj5taZAIBJzSfh/9r/n+F+DeIaKGkv1k2RHLXu1DoAuTnpREREREWIOxeOWq+A9LdxP93WgQRBKAZgLQC5zOJwAD9CWng6F0ACpAWqVwB8mLfhFqzBvw7Gqeu5HTS3Div013+o2kOasooA0DexLwBg6eCl+GLLFwj0CUS2mK173EiATwC+7f4tRiwYgeFJwwEAMzrP0CwotefFpi8iNjSWJRiJiIioyHFb9COK4hVIQbOstNUu0arbh+0c7iFYAvTtoih+KYpihiiKu6Gt5tInT4MtBE3KNJFuXIsFMqy/FI7rXrW7w/u+3vJ15fYvvX9Bu4rtkByTrGwL8ZMuYDSKb4Qvun3hdNpJyeCSWNBvAYoFSOt6R9YbiWcaPePw85uUaYLPu37OdBciIiIqctw9RblcdbupfEMQhPKQSibKltk5jjpHxTpJOtzktkcpXyy3jOGdwhvilLVTAAA9qknVK70EL85aExEREXkAd0dk/1XdHioIwouCIDwIKVVFtjR3RhyCIHwlCIKY++8V1T7bVbcrCIIwQxCEdoIgjALwtOqxja5+A66ibv5TWK7cli5klAySmhYdunwIBy4dKPRxEBEREZGWW5sZiaL4ryAIbwCYCOmE4Q2rXU4AGOHAoRYB+B+Ajrn3R+b+U0sD8FreR1uwlMWRd4yrpTgqLizO6ecsPiKVVJQ7nr7W4jVsO7/NcN9vu3+LX/f9mvcBEhEREZFd7p5JhyiKLwHoDiml5Sqkqi6HAfwfgGSzsolWxxABdAMwFsAKAJcBZAO4BWAPpMWidURRtJfb7jaxYbHSjU2P5Os4jeMb23y8YoS+mmW1yGoAgPiwePh7++Pl5i9jbu+5uv0AYGCtgaaPEREREZFruHUmXZbbZdRuoyFRFIcCGGryWBaAT3L/3XOUMoV3A/J1nKToJNPHUmJTsG74OlSeVhlJ0UmY89Ac5Ig58PGSfgyOPnnU7vFn75iNPw78gR96/pCvcRIRERGRObfPpJNk2dHctbE53vk6zs4LO9G2Qlvd9ukPTMcH7T6AIAg4fOUwft7zM3y8fODn7acsFvX28oa3l+3XH/TrIPy4+0eb+xARERFR/njETDqpiPk7b7qQcQGN4xtjyZElAID3276PEL8QNIxriMigSFeMkIiIiIgKGGfSPUTr8q1zb+WvJviiQ4vw/a7vlft70vZgdPJo1PmsDp5f+jwAILFkolJ2kYiIiIg8D4N0DxEfnlsWPp/pLvFh8dh/ab9y/8ttX2LKGqke+nc7v8vXsYmIiIiocDBI9xA7zu+Qboj5C9Lfa/eebtv4JeM193dd2IV5e+fl6fjeQv7GR0RERET2MSfdQ2w4vUG6cfAB8518b9k8xktNX0KAT/6qw9jz/UPf48+DfxboaxAREREVdZxJ9xAhfiG2d6g2F6hsOzh+feXrDr1Wjagaec5J71WjF7568Ks8PZeIiIiIHMMg3UMcvHzQ/MGQs0CfnoB3tkPHSolNMdzeqnwrAIAg5H1x6pdbv0Sn7zrl+flEREREZB/TXTyE0szIiJNlGR9MeNCSPpNrdvfZSIhMACDlpO+6sMvZIQIAhi8YnqfnEREREZHjGKTfCxwM0nvX6A0AeKHpC0iOScYXW79As7LNEOoXigG1Bij7TWw6EXFhcQUyVCIiIiLKPwbpHqJxfGOsPrna+EEHgvQfe/6oBOkA0LZiW7StqO88CgCvt3Isd52IiIiI3IM56R6iYVxD8wcdCNJLBJYw3P7IH49g4t8T8zosIiIiInIDBukewjqHXMOBIP1c+jnD7dM3T8ebq97M67B0gn2DXXYsIiIiIjLGdBcPoTQzMuJAkH4h44ILR2NuzkNzsPjw4kJ5LSIiIqKiikG6hwj0DcS1O9eMH4zZaPf5NUvVNH3MlYtEuyZ0RdeEri47HhERERHpMUj3EEq6SuIcYFd/ywOBF4Ge/XT7f/rAp2hapilOXT+FDt91MD3u7Ym34SUwq4mIiIjoXsLozdOoU1vK/w08HwWE6FNZPt74MdJupmHbuW0AgJXHVxoezt/HH77evgUyVCIiIiIqGAzSPY6qG6hg3mF014VdOHHtBO7m3AUA5X8iIiIiuvcxSPcQrcq3km6I6iA9x+Zz1pxcgzql6wAAkmOSC2poRERERFTIGKR7iKTSSbm3HA/SSwWXQqh/KAAgPCC8gEZGRERERIWNQbqHWHkiN6dcnZPul266/9cPfo2Xmr2kLDg9c+NMQQ6PiIiIiAoRq7t4iAOXDkg3Tqk6jwqi6f6Daw8GAFy7fU3zPxERERHd+ziT7iH8ffylGzdinXpeQmQCAKBaVDVXD4mIiIiI3IRBuodQ6qRrmM+kExEREdH9i0H6PU6uj7786HI3j4SIiIiIXIVBuie5Udrppzxc92HULV0XY5LHFMCAiIiIiMgdGKR7iAcqPwCcaKzdWOs7u8+LDo3GltFbEBvmXC47EREREXkuBukeonpUdeBuoGVDrW+BhD/cNyAiIiIichsG6R5iyZElQGawZUOJA+4bDBERERG5FYN0D3Hi2glouo2ysgsRERFRkcUg3UP4ePlou40SERERUZHFqNBDXMi4ANwNcPcwiIiIiMgDMEj3JGufVt0RTHcjIiIiovsbg3RPciPO3SMgIiIiIg/AIN1D9KjWw91DICIiIiIPwSDdQ1SMqOjwvvvG7ivAkRARERGRuzFI9xC/H/jd3UMgIiIiIg/BIN1DXMi4YLXFvE56iF9IwQ6GiIiIiNyKQbqH8BKsvxXm1V3SM9MLdjBERERE5FYM0j3ExZsXtRuC0kz3PZt+toBHQ0RERETuxCDdUwnm6S7X71wvxIEQERERUWFjkO6x9EF6+4rtUb5YeTQv29wN4yEiIiKiwsIg3UP0S+xnd5+U2BQcefIIwgPCC2FEREREROQuDNI9RGxorN19yoaXLYSREBEREZG7MUj3EL/s/UW7QZWTXr5YeUxsOhEty7cs5FERERERkTv4uHsAJLl2+5rpY3FhcXi64dMI8w8rxBERERERkbtwJt1TlftHuRngE4DI9yLx99G/3TceIiIiIio0DNI9xJXbV7QbShwCANQpXQdNyzQFAKw+sbqwh0VEREREbsAg3cNdvX0VaTfNGxsRERER0f2HQbqHO3b1GKZtmObuYRARERFRIWKQ7iGG1hnq7iEQERERkYdgkO4hooKi7O5TIaJCIYyEiIiIiNyNJRg9xJydcwC8q9v+Tpt30KlyJ/x54E+0Kt+q8AdGRERERIWOQbqHSM9MN9z+XOPncPvubfRJ7IOSwSULeVRERERE5A5Md7kHrDi+AuWnlsfSI0vdPRQiIiIiKgQM0j3EtTvmHUc3nt4IAFh3al1hDYeIiIiI3IhBuqc40cjdIyAiIiIiD8Eg3VOcrWv60MBaAxEVFIWH6z5ciAMiIiIiInfhwlFP4XvTcrvUdgDA+hHrAQBli5XFhWcvuGNUREREROQGnEn3RP07AwAECG4eCBERERG5A4N0TxR6BgAgCAzSiYiIiIoiBukeQx2Qi7lbGKQTERERFUUM0j2FqArIBSlILx1S2k2DISIiIiJ3YpDuKUTVtyI3Xo8KjnLPWIiIiIjIrRikewpR/63Iys5yw0CIiIiIyN0YpHuKHF/dprPpZ90wECIiIiJyNwbpnuKfSbpNGZkZbhgIEREREbkbg3RPcStSt4klGImIiIiKJgbpHowlGImIiIiKJgbpHowz6URERERFE4N0DxYVxBKMREREREURg3QPVjywuLuHQERERERuwCDdg2XlsE46ERERUVHkcJAuCMJSQRD6CILgV5ADIouLNy+6ewhERERE5AbOzKTXBTAHwBlBED4UBKFmAY2JcrFOOhEREVHR5EyQHg1gAICtAB4HsE0QhPWCIIwUBCGkQEZXxLG6CxEREVHR5HCQLopipiiKP4ii2BZABQCvAygF4DMAZwVB+EIQhMYFNM4iiXXSiYiIiIqmPC0cFUXxuCiKkwCUB9ABwHIAQwGsEARhjyAIT3F2Pf84k05ERERUNOW3uksdAF0BNAUgADgMIAfABwAOCoLQKJ/HLzpSpkn/l9ypbIoIiHDTYIiIiIjInZwO0gVBKCYIwlhBELYA2ARgBIC/ALQRRbGKKIqJANoAuAngY5eO9n4WfF76P2GBsikikEE6ERERUVHk4+iOgiC0BvAwgO4AAgAcAPAcgK9EUbyk3lcUxWWCILwNBulOkFNbRGXL3Zy78PFy+FtERERERPcJZyLAJQDuAJgHYIYoiv/a2f8QgNV5HViRJViC9Cu3riA6NNqNgyEiIiIid3AmSB8H4BtRFC87srMoisshLSglR4j6RaK37952w0CIiIiIyN2cKcH4oaMBOuXBmvHS/0dbKZuY6kJERERUNDkcpOcuFl1q4/HFgiCMds2wiqDMMOn/k1Kp+f41+yM+PN6NAyIiIiIid3GmustQAAdtPH4A0sJScoFvu3/r7iEQERERkZs4E6RXBrDTxuO7c/chFyj+TnF3D4GIiIiI3MSZIN0XUulFMwF2HicnXLtzzd1DICIiIiI3cSZIPwCgrY3H20HqOEpERERERPngTJD+PYB2giBMFgTBT94oCIKvIAivQgrS57h6gEVVmwpt3D0EIiIiInITZ2r8/R+AjgAmAnhEEIR9udurAigOYCWAKa4dXtG1ZNASdw+BiIiIiNzEmTrpWZBmyycAOAWgbu6/kwCeA9BGFMXMghgkEREREVFR4lS3nNxA/d3cf0REREREVACcyUknIiIiIqJC4HTfeUEQSgFIBhABgyBfFMVvXDAuIiIiIqIiy+EgXRAELwAfAxgB2zPwDNKJiIiIiPLBmXSX8QBGQyrFOASAAGkR6VgABwFsgu066kRERERE5ABngvQhABaJojgYwP9yt20WRXE6gHoAInP/JyIiIiKifHAmSK8AYFHu7Zzc/30BQBTFDACzIKXCEBERERFRPjgTpN8CkJV7Ox2ACKCk6vFzAOJdNC4iIiIioiLLmSD9OICKgFIv/RCADqrH2wA477qhFVF1v0CFiAruHgURERERuZEzQfoyAN1V978F0E8QhOWCIPwDoBeAn1w4tqKlwmLp/xo/4WbWTfeOhYiIiIjcypk66e8DWCwIgr8oincAvAUp3WUggGwAMwBMcv0Qiwoh978cjK432r1DISIiIiK3cjhIF0XxLICzqvvZAJ7I/Uf5JeZe1BBycPHmRfeOhYiIiIjcyqF0F0EQQgRBWCYIwvCCHlCRpQrS5++b796xEBEREZFbORSki6KYDqB+AY+laFMF6VduX3HvWIiIiIjIrZxZOLoNQLWCGkiRpwrSm5dt7t6xEBEREZFbOROkTwIwUhCElgU1mCJNFaSXCS/j3rEQERERkVs5U91lIIATAJYKgrAdwAEA1rUCRVEUmbeeF3KQDpHpLkRERERFnDNB+lDV7Tq5/6yJABik54VoKcG48fRG946FiIiIiNzKmRKMzqTGkLNU6S5Dag9x71iIiIiIyK0YeHsKVZB+6dYl946FiIiIiNyKQbqnUIJ0Eb/u+9W9YyEiIiIit3I43UUQhGUO7CaKotg6H+MpwnJz0iEiIzPDrSMhIiIiIvdyZuFoBUgLQ62fHw1pRv4iAEaX+SWIaFuxrbtHQURERERu5MzC0XJG2wVB8AcwDsAwAOzCk1dydRcA0SHRbhwIEREREblbvnPSRVG8I4riWwDWA/gg/0MqqizpLsG+wW4dCRERERG5lysXjq4C0N6FxyuaBBGrTq5y9yiIiIiIyI1cGaSXB+DnwuMVLap0l97Ve7txIERERETkbs5Udylj8lBxAG0APAHgHxeMqYizXptLREREREWNM9VdjsE8ghQA7IcUqFOeWGbSf9z9I8amjHXjWIiIiIjInZwJ0l+DPkgXAVwGcADAUlEUc1w1sCJHTncRRIxIGuHesRARERGRWzlTgvGVghqEIAhdATwOoB6AIAAnASwA8KYoipecPFYLAI8BaASgBIDrAI4DWA1gvCiKWa4beUEQMbj2YHcPgoiIiIjcyJULR/NEEIRXAfwGKa89AoA/gEqQaq9vEgQh3oljvQtgOYCHIDVZ8gMQCSn4fyL32B5KsL8LERERERUJDgfpgiC8KgjCLhuP7xAE4SVnXlwQhKb4f/buPLyq6t7/+OcbMkAYkgCJzJNMziIp6C0qDi1qFRScrsqFKgoWp+s8tIq2Xtsi5Wqt+lML3notbUUtXmstDqA4gIZqFQeoyCAgBRGQMCQkWb8/9slxJ2Q6yQl7wXm/nmc/7ums891nm5xPNmuvLd0eW6yQdKuksyQtjK3rJemxBrY1QdINscVtku6RNErSKZImSnpSUnki9e1Voe4uAAAASG2J9Ek/S9JLdWx/SdLZkn6WQJvXhOZnxB6KJDNbrKCLikn6vpkd4pz7qLZGzCxD0p2hVSOdc/Or7fZIAnVFiJAOAACQ6hLp7tJb0qd1bF8a2ycRJ4Tm40/wcc59IWl1aNuJ9bRztKQusfk1kk40s0/NbJeZrTKz+8wsL8Ha9jK6uwAAACCQaJ/03Dq25Ulq0dCGYqE5HJzXV9slvHxgPc0dHprvJuknkgYo6IPeQ0F/9LfMrK76oxXr7jKo86CICwEAAEDUEgnpHyno470HMzNJI1X3lfbqWldbLq1juU09bVUP3x9LOlPSpQpGd5GkgZJurunFZnaZmRWZWdHGjRvreavm9fWuhAazAQAAwH4okZD+W0lHm9njZpZfuTI2P0NBl5PfJtDe9mrL1UdeCS8X19PWrmrL1znn5jjnHpP0UGj9aTW92Dn3iHOu0DlXmJ+fX9Mue0FwJX3VlpURvT8AAAB8kcg46Y+a2fGS/kPSWDP7Mraps4KE+Ufn3EO1NrBne5vNbLO+7fLSqdounUPzy+tpblW15RW1zOc0tL7IMLoLAABAykuoT7pz7iJJ50t6XtLW2PScpHOdc//eiPefF5o/tnLGzHpLCo+P/mo97byhqsMr9qplvnqY94fjxlEAAAAEEhmCUZLknPuTpD8l6f3vlzQ6Nj/ezJYr6E9+a2iflyuHXzSzxyWNi62/s/IpqM659WY2W9J5sW33mllLBU8cvTzU1u+TVHczCEL64QccXs9+AAAA2N81OKSbWbqkbOfcN7Vsbydph3OurKFtOudeM7O7Jd2m4Kr+3dV2WS1pQgObu0rSkQpGdTlU0p+rbZ8j6dGG1haVHw4aH3UJAAAAiFgi3V2mSSqqY/u7kn6RaAHOuR8reFDSq5K2KBjVZbmk6ZIKnXMN6qLinNsgaaik/1IwZnuJgptTFym4mj7aOef/E0cBAACQ8hLp7jJC0tN1bH9awbCH1yVahHPuz9rzyndN+42XNL6O7VsVXJW/LdEaoheEdCOrAwAApLxErqR3V92jrHyuqjd7ohEI6QAAAEgkpJeq6rCI1XWSVNG0clJYrLtLh+wOERcC9a99rAAAIABJREFUAACAqCUS0t+XdK6ZZVbfYGYZCkZW+SBZhaWeIKQf0/3oiOsAAABA1BIJ6Q9IOkTSX8ys0MwyzSzDzAol/UXSwbF90AS7K0qjLgEAAAARa3BId849LekeSScpGDFlR2xaJOlkSb90zv2xOYpMCbHuLu+sfSfiQgAAABC1hB5m5Jy7zcz+LOkiSX1jq5dJ+r1z7t1kF5eKvinZGnUJAAAAiFhjnjj6roIx0fdgZkc75xY2uaqUxLAuAAAACCTSJ71GZpZvZteZ2UeS3kxCTanJMU46AAAAAglfSZckM0uTdJqkiyX9QFKGpA2SHk1eaamJkA4AAICEQrqZ9VMQzP9DwbjokvSspPslLXDOueSWl0qCdH5AmwMirgMAAABRq7e7i5m1MrNxZva6pE8lXatgRJerFSTL3zvnXiegN1Gsu8t3uhZGXAgAAACiVueVdDN7VNK5ktpKek/SNQpC+SYzO3Av1JdySsp3SWoZdRkAAACIUH1X0i+R9C9JRzvnBjvnfu2c27QX6kpBsXHS1zBOOgAAQKqrL6S/q2A89Llm9qiZDdsLNaWmWHeX7WXFERcCAACAqNUZ0p1zQyUdJmmmpFGSXjOz5WZ2u6Tee6E+AAAAIOXUe+Ooc+4j59x/Suoq6XwFTxi9XdLfJDlJ/2ZmHZq1ypTAOOkAAAAINPhhRs653c65p5xzp0rqJWmKpJUKRnv50sxeMbMfNUeRqYSQDgAAgEY9cdQ5t8Y591Pn3IGSTpb0lKRjJP06mcWllFif9G5tu0ZcCAAAAKLWqCeOhjnnXpX0qpnlSLqw6SWlqiCkH9H5iIjrAAAAQNQadSW9Js65rc65B5PVXqraWbYj6hIAAAAQsaSFdDRRrLvLu2vfjbgQAAAARI2Q7o0gpO8o2x5xHQAAAIgaId0zjO4CAAAAQrovHOkcAAAAAUK6Z9K4lA4AAJDyCOneCMJ5j5weEdcBAACAqDV4nHQz+7yeXZyknZJWS5or6VHnHHdBNlSsu8uhBxwScSEAAACIWiJX0ldLKpPUS1KepC2xKS+2rkxBSD9a0q8kLTaz/CTWmhK27y6OugQAAABELJGQfo2k9pJ+JKnAOXeUc+4oSfmSrohtu0RSR0lXSuon6a7klrs/C66kv7P2nYjrAAAAQNQa3N1F0r2S/uicezi80jlXJulBMztU0jTn3Pck/cbMjpH0g+SVup+LdXcpLS+JuBAAAABELZEr6UMlfVDH9g8UdHWp9JakAxpTVEozF3UFAAAAiFgiIb1E0nfq2D4ktk+lLEl0sG4whl4EAABAIJGQ/pykH5rZzWaWXbnSzLLN7BZJ42L7VPo3ScuSU2YKiHV3YZh0AAAAJNIn/XpJgyT9l6S7zGxdbH2XWDsfSrpBksyspaRdkn6TvFJTQ9/2B0ZdAgAAACLW4JDunPvazIZKmiDpdEm9Y5tekfR/kh5zzpXG9t0laWySa93PBZfQB3QcEHEdAAAAiFoiV9IVC+EPxiY0g22l36id2kVdBgAAACKUSJ90NKdYn/R31jBOOgAAQKpL6Eq6mbWWdIGCBxV10J5Dkjjn3CVJqi3FBB9lmdsdcR0AAACIWoNDupkNkfS8gieK1sYpeOooGqlDdvuoSwAAAEDEEunu8itJmZLOldTROZdWw9SiecpMAbHuLkO7DY24EAAAAEQtke4ugyX9l3NudnMVk9oYJx0AAACBRK6kfyNpU3MVggAhHQAAAImE9GckjWiuQlKeI50DAAAgkEhIv0lSgZn92swONOOab3LR3QUAAACBRPqkb1EwessQST+SpBpyunPOJTSsI6oipAMAACCRQP07BSEdzYHuLgAAAIhpcEh3zo1vxjoQw5V0AAAAJNInHc2KdA4AAIAAId0XjhtHAQAAEKi1u4uZVUiqkJTtnCuNLdfXJ50bR5uIkA4AAIC6AnXljaLl1ZbRLEjnAAAACNQa0qvfKMqNo82M7i4AAACIoU+6ZwjpAAAAaFT/cTPLltRBNfTRcM6tbmpRqYm/lwAAABBocEg3szRJN0q6UlKnOnZt0dSiUhlX0gEAAJDIlfSfS7pe0keSnpa0qVkqSnGEdAAAACQS0i+S9KJz7rTmKiZVOcbMAQAAQEgiHaHzJM1prkIAAAAABBIJ6R9K6txchaQyrqQDAAAgLJGQfqekSWbWvbmKSVXfhvSKKMsAAACAJxLpkz5Y0ipJH5vZs5JW6NunkVZyzrmfJqu4lGNcUgcAAEBiIX1KaP6iWvZxkgjpCaK7CwAAAMISCem9m62KFPdtSCetAwAAIIGQ7pxb1ZyFQHR3AQAAgCSeRe8FursAAAAgrNYr6WZ2u4L+F3c75ypiy/XhxtFGoLsLAAAAwurq7jJFQWr8haRSVb1xtDbcONoUdHcBAACA6g7pvSXJOVcaXkby0d0FAAAAYbWG9Oo3inLj6N5AWgcAAAA3jnqBK+kAAAAIS2ScdEmSmRVKGiopT3uGfG4cbYR4SKdPOgAAAJRASDezVpKekfR9Saagb4bFNrvQOkJ6oxHSAQAAkFh3l9sVBPS7JZ2gIJSPk3SqpAWS3pV0cLILTAV0dwEAAEBYIiH9bElPOedul7Qktm6tc+5vkk6WlClpfHLLSw10dwEAAEBYIiG9u6TXYvPlsf9mSpJzrkzSLEnnJ6+0VERIBwAAQGIhfZu+7cO+TVKFpC6h7VsldUpSXSmF7i4AAAAISySkL5fUX5Kcc+WSPlLQBUZmZpJGS/oi2QWmArq7AAAAICyRkP6ypDFm1iK2/P8knWJmyyX9U0G/9N8mub4UQ0gHAABAYuOk/1zSE4oNu+ice9DMWkq6SEEf9Ucl/TLpFaYAursAAAAgrMEh3TlXLGlptXW/kvSrZBeVsujuAgAAADWwu4uZtTGz5WZ2TXMXlIq4kg4AAICwBoX02FX0DpKKm7ec1PRtSCetAwAAILEbRxdKKmyuQiC6uwAAAEBSYiH9ZknnmtkPY0MuIkno7gIAAICwOm8cNbMekjY653YquEF0s6THJP0yNvTijmovcc65k5ql0v0Y3V0AAAAQVt/oLisUDLE4S1IfBSlydWzbAc1YV2qiuwsAAABUf0g3fTsueq9mryZF0d0FAAAAYYn0SUczobsLAAAAwgjpAAAAgGca8sTRY80skSeT/q4J9aSk+JV0+qQDAABADQvpl8Wm+piC/hqE9EYjpAMAAKBhIf0RBQ8yQjPhxlEAAACENSSkL3DO/b7ZK0lhdHcBAABAGDeOeoWQDgAAAEK6F+juAgAAgDBCugfo7gIAAICwOvukO+cI8XsVIR0AAABcSfcC3V0AAAAQRkj3Cd1dAAAAIEK6F7iSDgAAgDBCuge+DemkdQAAABDS/UJ3FwAAAIiQ7gW6uwAAACCMkO4BursAAAAgjJDuE7q7AAAAQIR0L9DdBQAAAGGEdA/Q3QUAAABhhHSf7M6OugIAAAB4gJDugSVLYjM7CiKtAwAAAH4gpHvg1VejrgAAAAA+IaR7wCzqCgAAAOATQjoAAADgGUI6AAAA4BlCOgAAAOAZQjoAAADgGUI6AAAA4BlCOgAAAOAZQjoAAADgGUI6AAAA4BlCugd4mBEAAADCCOkAAACAZwjpHvj+96OuAAAAAD4hpHugbdvYTLe3Iq0DAAAAfvAipJvZSDN7ycy+NrNdZvZPM5tmZh0a2V4bM/vMzFxoGp7kspPGuagrAAAAgE8iD+lmdqekOZJOlpQnKUtSX0nXSioys+6NaHa6pAOTVuTeYqR1AAAARBzSzexYSbfHFisk3SrpLEkLY+t6SXoswTbPkDRB0q7kVNn8KlxF1CUAAADAI1FfSb8mND/DOXePc+7Pks6VVHlZ+ftmdkhDGjOzfEmPxhZvSl6ZAAAAwN4TdUg/ITT/RuWMc+4LSatD205sYHuPSDpA0suSft3k6vYS+qQDAAAgLLKQbmZ5CvqgV1pfbZfwcr39y83sYklnStosabxzDY++ZnaZmRWZWdHGjRsb+rJmQFoHAABAtFfSW1dbLq1juU1dDZlZb0n/HVu83Dm3NpFCnHOPOOcKnXOF+fn5ibwUAAAASLooQ/r2astZdSwX19PWryW1lfR759wfm1oYAAAAEKXIQrpzbrOCrimVOlXbpXNofnk9zXWL/feC8Njo1faZF1uf24hym5VFfmsAAAAAfBJ1OpwXmj+2cibWfSU8Pvqre62iKDFOOgAAACSlR/z+90saHZsfb2bLJX2sYLz0Si875z6SJDN7XNK42Po7nXNTYvNTJdXUmXx6aP43kj6TtDMplSdReUW5pBZRlwEAAABPRBrSnXOvmdndkm5TcFX/7mq7rFbwYKL62nmypvVmFg7ps51z8xtZKgAAALDXRN3dRc65Hyt4yuirkrYoGNVluYKr4IXOuVURlrdXME46AAAAwqLu7iJJij1l9M8N2G+8pPEJtGuNryoKpHUAAAB4cCUdAAAAQFWEdA/Q3QUAAABhhHQPtEiLjezCEIwAAAAQId0LjkvpAAAACCGkeyAYJx0AAAAIENI9UMGVdAAAAIQQ0r1CWAcAAAAhHQAAAPAOIR0AAADwDCHdA8ZpAAAAQAjp0AOMkw4AAIAwQroHKlxF1CUAAADAI4R0D5RVlEVdAgAAADxCSPcAw6QDAAAgjJDuFdI6AAAACOkAAACAdwjpHqC7CwAAAMII6R5oYcEQjK2zsiOuBAAAAD4gpHugcpz0/Oz8iCsBAACADwjpHqgcJz07gyvpAAAAIKR7oaSsVJK09KulEVcCAAAAHxDSPVLueKgRAAAACOkAAACAdwjpAAAAgGcI6R5gnHQAAACEEdI9kG7pkqR2LdtGXAkAAAB8QEj3QHqLIKQzTjoAAAAkQroXyivKJUmZ6VkRVwIAAAAfENI9sGt3iSRp2SbGSQcAAAAh3SvlFYyTDgAAAEI6AAAA4B1COgAAAOAZQroHGCYdAAAAYYR0D2SmZUqSclvlRlwJAAAAfEBI90DlOOkFrQsirgQAAAA+IKR7YHdZMKqLmUVcCQAAAHxASPdASXkwTvpnX/8z4koAAADgA0K6RxgnHQAAABIhHQAAAPAOId0DjjEYAQAAEEJI90D8hlEjrQMAAICQ7oXKcdI7ZHeMuBIAAAD4gJDugcpx0g9gnHQAAACIkO6F0rLdkqTdjO4CAAAAEdK9UFK2S5K0YsvnEVcCAAAAHxDSPVJWvjvqEgAAAOABQjoAAADgGUK6BxgnHQAAAGGEdB8wTjoAAABCCOkeaNmipSSpgCEYAQAAIEK6F1qkVY6T3iniSgAAAOADQroHSspKJEk7y7ZHXAkAAAB8QEj3QEl5ME76qq2rI64EAAAAPiCke6BydJfd5aXRFgIAAAAvENI9wBCMAAAACCOk+4QhGAEAACBCuhescpx0AAAAQIR0L7RKbyVJ6tymc8SVAAAAwAeEdA+0sNg46W0YJx0AAACEdC9UjpP+TcmWiCsBAACADwjpHtgVGyd9zTdrIq4EAAAAPiCke6DCVUiSShknHQAAACKke4Fx0gEAABBGSPcJ46QDAABAhHQvpBmnAQAAAN8iHXqgVXq2JKlbu24RVwIAAAAfENI9kG4tJEmd2hwQcSUAAADwASHdA7vKg3HSv9q5KeJKAAAA4ANCugcqH2b05bZ1EVcCAAAAHxDSPVBWUS7p27AOAACA1EZI9wlDMAIAAECEdAAAAMA7hHQPtIiN7gIAAABIhHQvVI6T3jOnZ8SVAAAAwAeEdA+kp6VLkgraFERcCQAAAHxASPfArrJdkqSN2zdEXAkAAAB8QEj3QGVIX799fcSVAAAAwAeEdA/sLi+TJO3avSviSgAAAOADQrpPGCcdAAAAIqQDAAAA3iGke6BydBcAAABAIqR7oXVGG0lSn7wDI64EAAAAPiCke6BFWvDE0fzWHSOuBAAAAD4gpHtgV9lOSdL64i8jrgQAAAA+IKR7YGds6MX1xf+KuBIAAAD4gJDugbKKYJz0kvKdEVcCAAAAHxDSAQAAAM8Q0gEAAADPENI90MIyoi4BAAAAHiGke6BNZjBOev8O/SKuBAAAAD4gpHug8omjHbI7RFwJAAAAfEBI98CO0h2SpHXb1kVcCQAAAHxASPfAt+Okr4+4EgAAAPiAkO6B3RWlkqSS8l0RVwIAAAAfENIBAAAAzxDSAQAAAM8Q0j3AOOkAAAAII6R7IKdljiTpoPyDIq4EAAAAPiCkeyDNgtOQ1yov4koAAADgA0K6B3aU7pQkrdm6JuJKAAAA4ANCugd2lgUhff12HmYEAAAAQroXdpfvliSVlpdGXAkAAAB8QEgHAAAAPENI94BzUVcAAAAAnxDSPZDRonKcdNI6AAAACOleyG2ZK0k6tOCwiCsBAACADwjpHkiLnYbcVrkRVwIAAAAfENI9UFy6XZK0asuqiCsBAACADwjpHtgVGyf9y2LGSQcAAAAh3QulsXHSyyp2R1wJAAAAfEBIBwAAADxDSPcA46QDAAAgjJDugcwWmbE50joAAAAI6V6oHCf9yE5HRlwJAAAAfEBI94DFTkO7rHYRVwIAAAAfENI9UFxaLElasXVFxJUAAADAB4R0D+wq2yVJWreNcdIBAABASPdCaXmJJKm8oiziSgAAAOADQroHOhTslg78m1SwJOpSAAAA4IH0qAuANPS4bdLYU2JLv4i0FgAAAESPK+keaJPZpsp/AQAAkNoI6R7o2rarJGnK8VOiLQQAAABeIKR7IM3SdHr/03Vg+wOjLgUAAAAe8CKkm9lIM3vJzL42s11m9k8zm2ZmHRr4+hwzu8rMnjGzZWa2xcxKzewLM3vSzLx+lOeG7Rv0/LLn9fj7j0ddCgAAADwQeUg3szslzZF0sqQ8SVmS+kq6VlKRmXVvQDMHSbpP0lmS+knKkZQhqZukCyS9Y2anJr/65NhWuk2SNHf53IgrAQAAgA8iDelmdqyk22OLFZJuVRC0F8bW9ZL0WAObq5D0vKSLJX1P0s2Stse2ZUh6oOkVN48du3dIknaW7Yy4EgAAAPgg6iEYrwnNz3DO3SNJZrZY0ipJJun7ZnaIc+6jOtpZI2mQc+6D0LqXzWy3pGmx5T5mVuCc25DE+gEAAICki7q7ywmh+TcqZ5xzX0haHdp2Yl2NOOfWVAvolZZWW95ewz6Rc85FXQIAAAA8EllIN7M8BX3QK62vtkt4ubHDnpwXmn/FOVdjSDezy8ysyMyKNm7c2Mi3arycljmSpC5tu+z19wYAAIB/oryS3rracmkdywk/5cfMrpc0Nra4VdKVte3rnHvEOVfonCvMz89P9K2arHKc9OuPuX6vvzcAAAD8E2Wf9OpXtbPqWC5uaKNmZpLuVTA6jCRtkXSac+6ThCvcS1qktWCcdAAAAMRFFtKdc5vNbLO+7fLSqdounUPzyxvSppllSfqdpHNjq9YoCOgfNqXW5ra+eL2eX/a8JGnkgJERVwMAAICoRX3j6LzQ/LGVM2bWW1J4fPRX62vIzHIlzdW3Af0DSUf7HtAl6ZuSbyQxTjoAAAACUYf0+0Pz483sVjM7U9IfQ+tfrhx+0cweNzMXm6ZU7mBmBZLelHRcbNUXkm6R1NvMhoWmnGY9mkaqHCe9tLx6t3wAAACkokjHSXfOvWZmd0u6TcEfDHdX22W1pAkNaOrg2FSpu6S/1LDfCZLmJ14pAAAAsPdE/TAjOed+bGZFCkZfOUpStoIr4c9Jusc5t/fHRAQAYB+1a9curV+/Xlu3blVZWVnU5QApJT09XTk5OerUqZNatmzZtLaSVFOTOOf+LOnPDdhvvKTxNayfr+DppPuk9q3aS5IGdBgQcSUAgH3Zrl27tHTpUhUUFGjgwIHKzMxUMOgZgObmnFNpaak2bdqkpUuXasCAAU0K6lH3SYe+fYjRpMJJEVcCANiXrV+/XgUFBercubOysrII6MBeZGbKyspSly5dlJ+fr48++kjl5eWNbo+Q7oEWFoyT3ievT9SlAAD2YVu3blX79u2jLgNIeR06dFBFRYXeeOONRrdBSPfAum3r9Pyy5/Vw0cNRlwIA2IeVlZUpMzMz6jKAlJeZmam0tDT94x//UElJSaPaIKR7YGvJVknSS5+/FHElAIB9HV1cgOiFfw6//vrrRrVBSPdA5TjpZRXchQ8AALA/aWy/dEI6AAAA4BlCOgAAQALmz58vM5OZqVevXlGXg5Dx48fHz82UKVOiLqdJCOkeKGhdIEn6TpfvRFwJAAD+69WrVzyINWSaP39+1CUDCfPiYUaprnObzpKkCw+7MOJKAABAfQYNGqQFCxZIUpOfKonkuu222zRhwgRJUo8ePSKupmkI6R5okdZCP+j3A8ZJBwCgAWbPnq1du3bFl2fMmKGZM2dKkjp16qSnnnqqyv6HHXZYje3s3r1bzrmEh63MycnRsGHDEqwajf28E9GvXz/169ev2drfm+ju4oE136zRX/75Fz3w7gNRlwIAgPcKCws1bNiw+BS+YpqVlVVlW7du3ZSbmxvv+vLll19q/PjxKigoUFZWlj7++GNt2rRJkyZN0tChQ9W5c2e1bNlSrVq1Ut++fXXppZfq888/r/L+tfVJX7lyZZVuNl9//bUmT54cfwLsUUcdpb/97W8NOsby8nJdddVVOvbYY9W1a1dlZ2crKytLPXv21IUXXqj333+/xte99tprOvfcc9W9e3dlZWUpLy9PhYWFmjp1apX9SkpKdP/992vYsGHKy8tTZmamunTpotNPP11vv/12fL/w8axcuTLhzyAZn3elOXPm6PTTT1enTp2UmZmpjh076rvf/a7+53/+J75PXX3Sd+zYoV/+8pcaMmSI2rVrp6ysLPXr10/XXnutNm7cWGXfiooK3X///fF9MzIylJ+fr8GDB2vixIn69NNP6zp9yeGcYwpNgwcPdnvbW6vfcpoiZ1Nsr783AGD/UVRUFHUJkbjjjjucJCfJ9ezZs8q2FStWxLdJcv369auy/N5777lPPvmkyrrqU15enlu+fHm8zXnz5tX4fvW9lySXmZnpVq5cWe8x7dy5s86aMjMz3cKFC6u85vbbb691/yOOOCK+36ZNm9ygQYNq3Xf69OnxfcPrV6xY0eTPoDGfd0VFhRs/fnyt+48aNSq+77hx4+Lr77jjjvj6jRs3ukMPPbTWNrp27eo+//zzBn2WktysWbPqPYdFRUXuvvvuc1988UWt+0gqcrVkUrq7eGD77u2SJCcXcSUAgP3V8MeH77Hu3EPO1Y++8yPt2L1Dpz152h7bxx85XuOPHK+vdnyls/909h7bLy+8XOcdep6+2PqFxj47do/t1x1znc4YcIaWfrVUE5+fWGXb/PHzG30sTbF69WrdddddGjp0qFatWqWOHTsqIyNDd911lwYMGKCcnBy1bNlS27Zt0x//+Ef97//+rzZv3qxp06bpN7/5TULvtXnzZj366KPKzc3VNddco7Vr16q0tFQPP/yw7rnnnjpfm56erp/85CcaOHCg2rdvr1atWmnHjh166aWXNH36dJWWluquu+7SX/7yF0nS3Llzddddd8Vff8IJJ2jixIlq166d3n//fS1cuDC+7YorrtB7770nKXgy5tVXX63hw4dr27Zteumll5SVlZXQcdYlGZ/3o48+qscffzze5tlnn63zzz9fmZmZeuedd7R+/fp665g8ebKWLFkiSTryyCN10003KTc3V4899piefvpprV27VuPGjdPrr78uSXr66aclBedh+vTpOuSQQ7Rp0yZ99tlnevHFF5WRkZG0z6g2hHQAAJAypk6dqiuvvHKP9UcddZQeeughLV68WF999ZXKyqo+YDAcchvqwQcf1DnnnCNJWr58uW6++WZJ0rJly+p9bXp6uk455RRNnz5dixYt0r/+9S+VlpbWWtOjjz4anx88eLBefvllpaUFvZpPPfXU+LatW7dW6bM/depUXXXVVfHl8847L5FDrFcyPu/wsZ111llV6j/jjDPqrWHLli3x0C1JN954o7p16yYp+IPlueee0+7du7VgwQItXbo0/seDJGVkZKh///466qij4usqz2NzI6QDAJAC6rpynZ2RXef2jtkd69zePad7ndsHdBwQ2ZXz6saMGbPHuhkzZuiSSy6p83WbN29O+L1OOumk+HyHDh3i8w15TPxLL72kU089tc6nVYZr+vjjj+PzZ555ZjygV7ds2bIqgXj06NH11tIUyfi8w8fWmHqXLVtW5XO84IILat13yZIlGjBggCZNmqS33npLO3fu1IgRIyRJBQUFOvLIIzVmzBhdfPHFSk9v3hjNjaMeqByC8cTeJ0ZcCQAA+7fOnTvvse7nP/95fP6UU07Rc889pwULFmj69Onx9RUVFQm/V/v27ePz4UAXdEWu29SpU+PBcsiQIZo9e7YWLFigWbNmJdROMoXDffUbLWuzNz/vZCguLpYkjR07Vq+99pomTpyoIUOGKDc3Vxs2bNDcuXM1ceJE3XDDDc1eCyHdA53adJIknTngzIgrAQBg/2Zme6xbvXp1fH7q1Kk644wzNGzYsHhgi0K4pp/85CcaM2aMhg0btke3kEoHH3xwfH7OnDl7hNzKQN+/f3+1aNEivv7ZZ5/do61w+M/Ly4vPr1mzJj7/f//3fw06jmR83uFjq6/emlQ/5qVLl9Z4o2ZxcbHGjRsXb/O4447Tww8/rEWLFmnz5s1atGhRvI3wH0vNhe4uHkhPS9cP+v1AvfN6R10KAAApp0+fPvrkk08kST/72c90ySWXaPHixbr77rsjrWnp0qWSpOnTpysjI0PLly/Xj3/84xr3nzBhgmbPni1JKioq0ogRI3TppZeqXbt2+vDDD/XGG29ozpw5ysnJ0TnnnKM//OEPkqQbbrhBa9eu1fHHH6/i4mK98sorOuKII3T55ZdLCgJuZTidPHmyJk+erMWLF+uJJ55o0rEl8nlPmDBBRUVFkqRnnnmisnkVAAAgAElEQVRG559/vs477zxlZGRo8eLFWrNmTZV+69Xl5uZq9OjR8b7sp512mm644Qb17dtXW7Zs0apVq/T666/r008/jQ+teM455yg9PV3Dhw9X165d1bp1a82dOzfeZnic/mZT27AvqTpFMQTjhuINTlPkHlj0wF5/bwDA/oMhGOsfgrEmDz/8cI3D7A0fPrzGdhs6/GDYzJkz4+uPP/74eo/pr3/9a701VX+PW2+9tUFDMH711Vfu8MMPb9AQjE8++WSN+4SHM2zoZ9DYz7u8vNyNHTu2SUMwbtiwoc4hGKu/54gRI+rc96qrrqr3HDIE435gx+4dkqSiL4sirgQAgNQzceJEOed03333aeXKlerevbsmT56sww8/XPPnz4+kplNOOUVPP/20fvrTn2rp0qXKz8/X+PHjddFFF6l///41vubuu+/WSSedpAcffFALFy7Uhg0blJ2drQMPPFDnn39+fL8OHTpo0aJFeuihh/TUU0/p448/1o4dO9SxY0cNGjRIQ4cOje97wQUXaN26dXrggQe0bt069erVS5dffrmOOOKIKjfGJiLRzzstLU2/+93vNGrUKM2YMUNFRUX6+uuv1a5dOw0cOFBnnll/d+H8/Hy98847evDBBzV79mx98skn2rFjh/Lz89WjRw+ddNJJOuuss+L7X3755SooKNC7776rf/3rX/rmm2/Upk0bHXTQQfr3f/93XXHFFY069kSY28s3HfiusLDQVf6Tyt7yycZPdPCDB2tgx4H6ZPIne/W9AQD7j8WLF2vw4MFRlwFAwc/jm2++qdGjR8eHfKzOzBY75wpr2saNowAAAIBnCOkeSTNOBwAAAAjpXshvnS8peLwyAAAAQEgHAAAAPENI90BGWoZO73+6eucyTjoAAAAI6V4oKS/R88ue16qtq6IuBQAAAB4gpHuguDR4DO7CNQsjrgQAAAA+IKR7oPJhRn//8u8RVwIAAAAfENIBAAAAzxDSPWAySVLL9JYRVwIAAAAfENI9UDlO+sWDLo64EgAAAPiAkA4AAAB4hpDugcwWmTq9/+nqldsr6lIAAAC8NH/+fJmZzEy9evWKupxmlx51AZBKyoJx0k/re1rUpQAA4L1evXpp1aqGP1tk3rx5Gj58eNLrWLlypR5//HFJUm5urq655pqkvwdSFyHdA9tKt0mSFqxeoMu/c3nE1QAAgIZYuXKl7rzzTklSz549CenNbNCgQVqwYIEkqWXL/X+wDUK6ByrHSV+yYUnElQAA4L/Zs2dr165d8eUZM2Zo5syZkqROnTrpqaeeqrL/YYcdtlfrS1XFxcVq06ZNs7Wfk5OjYcOGNVv7vqFPugf6te+nnjk9Ne3706IuBQAA7xUWFmrYsGHxqUePHvFtWVlZVbYNGzZMGRkZ+uUvf6khQ4aoXbt2ysrKUr9+/XTttddq48aNVdquqKjQ/fffH983IyND+fn5Gjx4sCZOnKhPP/1UUtDl5oQTToi/btWqVfH+0mamlStX1nkMs2bN0qhRo9S3b1/l5uYqIyNDHTp00PHHH68ZM2bIObfHa9avX6+bbrpJhx9+uNq0aaNWrVqpT58+Gjt2rDZs2FBl39dee03nnnuuunfvrqysLOXl5amwsFBTp06N7zN+/Ph4vVOmTKny+l69esW3zZ8/P75++PDh8fUzZ87Uf//3f+uggw5SZmamfvzjH0uSHnjgAZ166qnq3bt3/DMsKCjQiBEj9Oyzz9b4eSxfvlyTJ0/WwIEDlZ2drdatW2vgwIG67LLLVFJSIqn+Pulz587VqFGj1KlTJ2VmZio/P18jR46MX30PW7hwoUaNGqXOnTsrIyND7dq1U9++fTVmzBg9+eSTNda41znnmELT4MGDHQAA+6KioqKoS4jEHXfc4SQ5Sa5nz55Vtm3cuNEdeuih8e3Vp65du7rPP/88vv/tt99e676S3KxZs5xzzvXs2bPO/VasWFFnzeedd16dr7/66qur7P/uu++6Dh061Lr/e++916BjOOKII+L7jRs3Lr7+jjvuqPJ+4eObN29efP3xxx8fX9+vX78aax46dGidxzZ9+vQq7/X888+77OzsWvffvHmzc865efPm1Xqeb7rpplpfn5aW5h566KH4vp988onLysqqdf8RI0bUee4aqqioyN13333uiy++qHUfSUWulkxKdxcAAPZzZlFXULMaLhYn3eTJk7VkSdCd9Mgjj9RNN92k3NxcPfbYY3r66ae1du1ajRs3Tq+//rok6emnn5Ykpaena/r06TrkkEO0adMmffbZZ3rxxReVkZEhKehy8/bbb+uqq66StGc3m86dO9dZ18iRI3XCCSeoS5cuatu2rSoqKrRy5UrddNNN+uqrr/TAAw/o5ptvVqdOnVRSUqJzzjlHmzZtkiQVFBTolltu0cEHH6y1a9dq1qxZsthJnjt3ru666674+5xwwgmaOHGi2rVrp/fff18LFy5MxscqSfrnP/+pkSNH6oc//KHMTOnpQawcN26cLrvsMh1wwAFq06aNdu/eraVLl+q6665TSUmJpkyZoiuuuELp6enauHGjLrjgAu3YEXT97dOnj2666Sb16tVLn3/+efzG3Lr89a9/1S9+8QtJUqtWrXTnnXdq0KBB+uCDD3TrrbeqpKREV155pU488UT1799fzz//fPzq/DnnnKNLLrlEFRUV+uKLL+L/H/iAkA4AAPZLW7ZsiYduSbrxxhvVrVs3SdIVV1yh5557Trt379aCBQu0dOlSDRgwQDk5OZKkjIwM9e/fX0cddVR83c033xxvq7CwUMXFxfHlym42DTVixAhNnTpVv/nNb/T5559rx44dVbq4lJeX691339UZZ5yhl19+Od59Ji0tTS+++KIGDRoU3/eHP/xhfP7RRx+Nzw8ePFgvv/yy0tKC3s2nnnpqg+triMGDB2vOnDl7rD/99NP185//XK+88opWr16tnTt3Vtm+detWffLJJzrssMP0pz/9Sd98840kqU2bNnr99dfVtWvX+L6TJk2qt47f/va38fmzzz5bxxxzjCRpyJAhOumkk/TCCy+orKxMM2fO1D333BM/n5LUo0cPHXTQQerevbvMTJdddlliH0IzIqQDALCf2xtXrH20bNkylZeXx5cvuOCCWvddsmSJBgwYoEmTJumtt97Szp07NWLECEnBlesjjzxSY8aM0cUXXxy/YtxYO3fu1He/+10tXbq0zv02b94sSfr444/j63r37l0loFcX3vfMM8+MB/TmMHr06D3WrV+/XoWFhXv0ka+upmMbOnRolYDeUOE2nnjiCT3xxBM17lf5LyqjRo3S7bffrvXr12vatGmaNm2aWrVqpYEDB+rEE0/U1Vdfre7duydcR7Jx4ygAAEh5lVfFx44dq9dee00TJ07UkCFDlJubqw0bNmju3LmaOHGibrjhhia/17PPPhsP6K1bt9b999+vefPmacGCBVVGoqmoqGjye9XFQv2gysrKqmz76quv6n19TV16ZsyYEQ/oBxxwgH7729/qtdde04IFC9SxY8f4fs19bDWpPMcFBQX6+9//rrvuukvf+9731KNHD+3atUvvvfeepk2bpmOPPTZ+dT9KhHQAALBf6t+/v1q0aBFfXrp0aY036BUXF2vcuHGSggE1jjvuOD388MNatGiRNm/erEWLFsXbmDVrVnw+fJU6kdC5evXq+Pwpp5yiK6+8UsOHD9fhhx+uNWvW7LH/wQcfHJ9fsWKF/vGPf+yxT2VXmfC+c+bM2aOucJeavLy8+Hz4fV999VVt37693uOwGm52CB/bRRddpIsvvljHHXecevToEe9THxaud9GiRVq3bl2971vdQQcdFJ+/5ZZbajzH5eXl+utf/yop+Aw6d+6sn/zkJ5o7d65WrVqlr7/+Ot5NZtWqVXrrrbcSriPZ6O4CAAD2S7m5uRo9enT8hs7TTjtNN9xwg/r27astW7Zo1apVev311/Xpp5/Gh1Y855xzlJ6eruHDh6tr165q3bq15s6dG28zPD57hw4d4vPr1q3T7373O/Xp00etWrXS4MGDa62rT58+8flXXnlFTzzxhHJycnTvvffGu4GEnXzyyerZs6dWrVqliooKnXLKKbrlllt00EEH6csvv9Qf/vAH3XPPPTriiCM0YcIEzZ49W5JUVFSkESNG6NJLL1W7du304Ycf6o033oj3I+/fv3/8PWbNmqXevXurZcuWVYZpTFT42GbPnq1jjjlGFRUVuvPOO2scVvLcc8/VLbfcom3btqm4uFjHH3+8brzxRvXq1UsrV67UzJkz9cILLyg3N7fW97zkkkv0zDPPSJKmTp2qiooKHXfccUpLS9Pq1av1wQcfaM6cOXriiSc0fPhwPfXUU/rVr36lUaNGqU+fPiooKNC6deu0YsWKeJvh8xyZ2oZ9SdWJIRgBAPsqhmDcc2i+DRs21DkEY/XXjBgxos59r7rqqvi+ZWVlrlu3bnvsc+CBB9ZZ7/bt212fPn32eF2nTp3cwIED48szZ86Mv2bRokUuLy+vQUMw3nrrrQ0agnHr1q01DuvYrVs3l5ubW+8QjOH6Kn355Zc11nnwwQe7goKCGtucM2eOa9WqVZOGYLzxxhvrPG/h95w1a1ad+3Xr1s198803dZ7DhmjqEIx0dwEAAPut/Px8vfPOO7r33nt19NFHKycnRxkZGerSpYuOPvpo3XbbbVVGgLn88ss1duxYDRw4UHl5eWrRooVycnJ09NFH67777tP06dPj+7Zo0ULPPvusjjvuOGVnZze4puzsbL366qs666yz1L59e+Xk5GjkyJF64403dMABB9T4miFDhmjJkiW6/vrrdeihhyo7O1stW7ZU7969deGFF6pLly7xfe+++2698sorGjNmjLp27aqMjAzl5OToqKOO0oUXXhjfr127dnrhhRc0bNgwZWVlqX379ho7dqwWLVpUZQSURHTq1Enz58/XySefrHbt2qlDhw666KKLNG/ePLVq1arG14wcOVLvv/++Jk2apH79+qlly5bKzs5W//79NWHChFpfF/aLX/xCc+fO1VlnnRV/QFFeXp4OPvhg/cd//Idmz56to48+WlJwg+r111+vY445Jv7go6ysLPXt21eTJk3S22+/rbZt2zbq+JPJXKre8l2LwsJCV1RUFHUZAAAkbPHixXV2swCw9yxevFhvvvmmRo8eHR/6szozW+ycK6xpG1fSAQAAAM8Q0gEAAADPENIBAAAAzxDSAQAAAM8Q0gEAAADPENIBAAAAzxDSAQDYjzC0MhC9ZPwcEtIBANhPpKenq7S0NOoygJRXWlra5KBOSAcAYD+Rk5OjTZs2RV0GkPI2bdqk4uJiSVJaWuPiNiEdAID9RKdOnbRhwwatW7dOJSUldH0B9iLnnEpKSrRu3TqtW7dOGzZskHNO7dq1a1R76UmuDwAARKRly5YaMGCA3n33Xa1du7bRV/AANI5zTsXFxdqwYYO+/PJL9e7dW23atGlUW4R0AAD2I61atdLgwYP14osvas2aNUpLS+OKOrCXOefUu3dvjRgxotFtENIBANjPtG7dWmPGjNG2bdu0bds2VVRURF0SkDLS0tLUrl27Rl9Br0RIBwBgP9W2bVu1bds26jIANAKd1QAAAADPENIBAAAAzxDSAQAAAM8Q0gEAAADPENIBAAAAzxhjp1ZlZhslrYro7TtK+iqi98aeOB9+4Xz4hfPhF86HXzgffvH5fPR0zuXXtIGQ7hEzK3LOFUZdBwKcD79wPvzC+fAL58MvnA+/7Kvng+4uAAAAgGcI6QAAAIBnCOl+eSTqAlAF58MvnA+/cD78wvnwC+fDL/vk+aBPOgAAAOAZrqQDAAAAniGkAwAAAJ4hpAMAAACeIaQ3IzMbaWYvmdnXZrbLzP5pZtPMrEOC7fQ0s0fMbJWZlZjZBjObY2bfba7a90dNPR9mlmNmV5nZM2a2zMy2mFmpmX1hZk+a2ZHNfQz7k2T9fITaa2Nmn5mZC03Dk1z2fiuZ58PMhpvZbDNbF/udtdHMiszsPjPLaI769zfJOB9mlmFmV5rZG7F2ysys2MyWmNm9ZnZAcx7D/sDMrjGzp8xsRbXfLeMb0Rbf5U2UjPOxT32XO+eYmmGSdKckV8u0QlL3BrZzlKSva2mnXNK4qI91X5iScT4kHV1HG05SqaRToz7WfWFK1s9HtTYfraGt4VEf674wJfN8SPplPT8nbaI+Xt+nJH5/PF3PuVgtqUPUx+vzJGlLLZ/d+ATb4bvck/OxL32XcyW9GZjZsZJujy1WSLpV0lmSFsbW9ZL0WAPaSZf0e0l5sVUvSBopaVpsOU3SQ2bWJymF76eSdT5Cr39e0sWSvifpZknbY9syJD3Q9Ir3b0k+H5VtniFpgqRdyakydSTzfJjZBEk3xBa3SbpH0ihJp0iaKOlJBYEEtUji90dfSaNDqx5W8DvrOn17DrpLOrfJRe/fPpQ0Q9KPJG1oTAN8lydVk89HzL7xXR71Xwn746SqVy8eDa3vruB/jMpth9TTzhmhfbdKahXa9lJo29Soj9nnKYnno5ukw2tYf62q/hVeEPUx+zwl63yEXpcvaX3sNVdVOxfDoz5e36ck/nxkSFrLZ+/N+Sis9rPQJrRtSWj91VEf874ySVoZ+tzGJ/A6vsv9Oh/7zHc5V9Kbxwmh+TcqZ5xzXyj458VKJ9bTTnj7351zO0PLbybQTqpLyvlwzq1xzn1Qw6al1Za317APvpWsn49Kj0g6QNLLkn7d5OpST7LOx9GSusTm10g60cw+jfWnXhXrj55Xx+sRSNb5WCLpy9DyVDM7ycyulTQwtq5Y0p+bUCsahu9yj+xL3+WE9CSLfQmFv4jWV9slvHxgPc2F/+mrKe2krCSfj9qcF5p/xTkX+Q+2r5J9PszsYklnStqs4EqKa3KRKSTJ5+Pw0Hw3ST+RNEBSlqQeCv6V4y0zy21ctfu/ZJ4P59wuSadJ+nts1SQFf8hOk9QiNn+Mc25VU2pGg/Bdvm/w7ruckJ58rastl9ax3CaBtprSTipL5vnYg5ldL2lsbHGrpCsTbSPFJO18mFlvSf8dW7zcObe2ibWlomT+fFQP3x8r+APqUknfxNYNVND3EzVL9u+rrZKWKegmU92/STrHzKzh5aGR+C73nK/f5YT05Kv+l1dWHcvFCbTVlHZSWTLPR5wFpkmaGlu1RcHd4J8kXmJKSeb5+LWktpJ+75z7Y1MLS1HJPB/Vb9q9zjk3xzn3mKSHQutPS6C+VJO08xH7F4u3JZ2v4Lv+EgUh8FAF/6yfreAG1aubUC8ahu9yT/n+XU5ITzLn3GYF//ReqVO1XTqH5pfX09znSWonZSX5fEiSzCxL0h8U3GQiBf1vj3POvd3YOlNFks9Ht9h/LwiPl1ttn3mx9XSxqEGSz0f1bhMrapnPaVh1qSfJ52OMgns1JOkfzrkZzrntzrmPVPWPpvP2fCmSjO9yD+0L3+WE9OYxLzR/bOVM7J/nu4e2vVpPO+HtR5lZdmj5uATaSXXJOh+VV6fm6tthyz6QdLRz7sMk1JkqknY+kBTJOh9vqOrwir1qmacPdN2SdT7yQ/Ntq23LqWUezYPvcs/sM9/lUQ8vsz9Oko5X1YcU3Kqgb+Y7ofUvhfZ/PLR+Smh9uoL+hJXbXlAwlNP00Lqdkg6M+ph9npJ4PgokfRTatlrBP90PqzblRH3MPk9JPB8XSrqmhik8hNYDsXVZUR+3r1Oyzkds2x9C2z5UMEb6xar6AJJJUR+zz1MSfz5Orfaz8Iik70u6TMHV+sr1/xP1Mfs8xT6zM2PThtDndn9ofcd6zgXf5X6dj33muzzyD3x/nST9rNovyPC0SlLP0L51fekVqvYnbFVIujjqY90XpmScD0nD62gjPA2P+nh9n5L181FL25yLiM5H7Mvv0zra+rOkFlEfr+9Tkn5fmYIwWNfvqg0iGNZ3LlY29Hd+PT8bfJd7cj60D32X092lmTjnfqzgKXGvKvjBLFXQ32y6pELXwGGvnHNFkgZJ+q2C/lK7JW2S9H+SjnfOzUh+9fufZJ0PJAfnwy9J/H21QdJQSf+l4ObEEgU3zS2SdLmk0c45njhaj2ScDxekkVGSJkt6XcEj6csVXLH9WMHISEc65+gHvRfwXY7GsNhfFQAAAAA8wZV0AAAAwDOEdAAAAMAzhHQAAADAM4R0AAAAwDOEdAAAAMAzhHQAAADAM4R0AAAAwDOEdACImJn1MjNnZlOa0MbjZsaDL5Igdi4ej7oOAM3DzK4xs6fMbEXs571yGp+Etl0DppUNaYuQDgDVNPCXbOXUK+p6fRL7TJ6vtu6aZHz5JYuZ5ZrZFDMbHnUtACIxRdLZknpF9P67G7JTenNXAQD7oLHVlo+VdJmkRyQtqLZtYxLeb5WkVpLKmtDGpZImJaGW5nCNpJWSHo+2jLhcSXfE5ufXsL2VpPK9Vg2Ave1DScskFSkI7AVJbPvYWtb/QVLX2PyfGtIQIR0AqnHO/W942czSFYT0t6tvq87M2jrntiX4fk7SroQLrdrGbjXw6sz+pDGfd32cc006FwD85pyLB2kzu6m+/c1siKT/lDRM0gGStktaLOl+59xz1dp+o4bXn6hvA3qJpF83pE66uwBAI5nZSjObb2aDzOxvZrZV0gexbW3N7GdmtsjMvjKzEjP7zMx+bmbZ1drZo096eJ2ZnW5m75rZLjP70symxv5wCLexR5/0ynVmlmNmD5nZhlgbb5rZ0BqOp4OZzTCzTWZWbGavxo5tfkP7UNbQppPUU9LxtXUTMrNCM3s29DktNbPbajjG+bHPvI+ZzTazryV9E9uWFnvN62a23sxKzWx17Lg7hNoYLmlFbPGOmvqI1tYn3cwmmNnfzWynmW01s7lmNqymY4599seY2Wtmtj32mT5mZm0a8zkCiIaZ/UjS25LOl9RNUoaCf407SdIcM/uvBjRzXWj+Sefc+oa8N1fSAaBpekh6VdJTkp6WVBnCukqaEFv3ewVdWY6XdKOkQZJGNLD90yT9SNLDkmZIGiXpekmbJTXky0GS/qagW85dkjpIulbSX8ysd+VVaDPLkvSypCMVdEt5R9LhsXVfN/B9ajJW0nRJX0m6O7R+Y+x9fyDpGUmfSZoWe69jYrUeKemcau21kfSapDcl3aZv/5k6U9INCj7vOQqudH1H0iWShpnZYOdcqaRPFFwRmy7p2dh7S1JxXQdhZr9QcO7ekXSrpLYK/nVlnpmNcs69UO0lR0p6XtJMBed/eKyWitjrAHjOzA5RcNU7TcHP7j0Kfv/0lvRzSXmSbjGzl51zr9bSxkGSTo0tOgW/5xrGOcfExMTEVMckaXzsl+v4autXxtZPqOE1mZIyalj/09hrhoTW9Yqtm1LDuu2SeoXWm6Qlkr6s1u7jivWcqb5O0oPV1p8TWz8xtO5HsXW3Vdu3cv3KBn5WTtLzNXxO82vYt6Wk9ZJel5Rebdt/xtoaHlo3P7buZzW0ZZJa1bD+kthrzq3r867hGB4PLQ9Q8AX9hqTM0PoukrbEjq9FtddXSBpard2/KOiS1Cbq/6eZmJiCKfR7vKbf8feGtr30/9u7vxApqzCO498fFiFZaVnZVRoVpt4kdNFFF5FJXZhlIKVZlESWlRlIQnVVdJFRGHRT/iMCE6TEpT9WKFRUElEQFVJbSrBlYSZq4UI+XTzvu/s6+846w7TupL8PDDNz5syZs7PwzplnnvO8ZLpLeVlbeWzjMOO/Uun3Vjtzc7qLmVln/iCjpceIiP7IPHEknSZpgqSJZGQaYEi6SRNbImJ3ZdwAdgCT2kideKHhfhnxuazSNofcLLm6oe8a4ECLr9Ou68n8zvXAeEkTywtQRqZn1zzvucaGSH8DSBpTVHCZyODf2ur7XWcu+SXg2chofPmafcXcLyZ/Han6NCJ2NrRtJ3/BntzBXMzsxJlWuT2LLBxQXu6pPDaj7smSLuDYQgRDjl3DcbqLmVlneiOithJIkcu4BJjO0D1AE1oc/8eatn3F9XkcJ02jboyI2CepfH5pCtAXEYca+vZL+qmN+bbjiuJ63TB9Lmy4/3tE/FnXUdJ8MvfzSjJvtKqT+U8prr+peaxsu4SsFFE63v/NzE4ezQImDwJnFLe/iIgd7QzqRbqZWWf+qmuU9CiZe/ge8CLQB/STueobaH3j/nClANXKAM2+RLT6/BFUvv4K4Ksmffoa7jd7v+cBm8ic8WXAz2TFnDHAu5z4Qgkd/9/MbNR9x2A++caIWFDXSdKZNW1jgfsrTa3nohe8SDczGxmLyFzHGyPiaNko6YZRm9HwdgOzJI2rRtMlnU5Gkmuj1y1qdibU74vrwxHxQZM+rVpELsqvjYiBhbykqW3Mp5kyKj4d6G14bFpDHzPrcpJmA2WVrWq1rZmSymPdx2RA5RHyS/7tkg6SG8KPkJVepgE3kZv4NzS8zF3AxOL2HrK4QFuck25mNjL+IReDA1HToqTgylGb0fB6yKjzsob2e4FzOhz7EHBuTfs24DdgpaQhj0saK+msFl+jfL8HPteUOT1PNJkPTeZUZ2sx9oriS0s5/kXA3eQH8JctjmVmo+9lsrrTm8D5lfaHKu0zIuJr4GFyIzhkZaat5LFrLZleV93bAwwce5ZXmlZHRNsnq3Mk3cxsZGwmy3W9I+kN4GxgAd17wqE1wH3A05IuZbAE43yyPGInnxefAYslPUX+fHwU6ImIw5LuBLYAuyStK15rPDAVmAfcQv1ZQRttBm4Ftkt6lcxJv5ljo2TAQE7+D8BtknqBvWQ0v6du4IjYJWkVWYLxQ0mbGCzBOA5YOExKkZn9j0XES5LKNLprgEnkr3a/kOfF6CHLvlbNAS4vbh8gj69t8yLdzGxkrCKj6IvJiim/kjnT64FvR3FetSLiiKTryHnPJRfnO8kTdqyhZrHbhsfJqPVScgEuMoXmcERsk3QV+dlBxsoAAAD6SURBVAvDHWRUaz+ZVvI8xcmhWpj/60XUfTlZQWE/+eG5ksENm1ULyao3z5B/256if7PxHysW9g+Q9ZH7yfdnQUR81Moczaw7RMTkNvt/Th6fWu2/lf9g74mKGo5mZmZDSBpDnohoZ0R0az69mdlJxznpZmYGDFQjaLSEjH6/f4KnY2Z2SnMk3czMAJD0GnkW0E/I6gVXk3n0vcDMiDg4itMzMzuleJFuZmYAFJs4l5IbnsaRGyrfBp6MiL2jOTczs1ONF+lmZmZmZl3GOelmZmZmZl3Gi3QzMzMzsy7jRbqZmZmZWZfxIt3MzMzMrMt4kW5mZmZm1mX+BUaQH6e0MwhfAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 864x864 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Testing Accuracy: 91.78768396377563%\n","\n","Precision: 91.69975541198025%\n","Recall: 91.78768152228342%\n","f1_score: 91.66359722936697%\n","\n","Confusion Matrix:\n","Created using test set of 5991 datapoints, normalised to % of each class in the test dataset\n","Confusion matrix, without normalization\n","[[ 597    5    3    1    4    1    2]\n"," [  25  373   20    5   16   26   56]\n"," [   1   20  312    0    3    0    1]\n"," [   2    2    0 1019    0   18    3]\n"," [   1    3    0    0 1266    1    3]\n"," [   1    2    1   21    0 1135   81]\n"," [   7   24    2    9    3  119  797]]\n","Normalized confusion matrix\n","[[9.74e-01 8.16e-03 4.89e-03 1.63e-03 6.53e-03 1.63e-03 3.26e-03]\n"," [4.80e-02 7.16e-01 3.84e-02 9.60e-03 3.07e-02 4.99e-02 1.07e-01]\n"," [2.97e-03 5.93e-02 9.26e-01 0.00e+00 8.90e-03 0.00e+00 2.97e-03]\n"," [1.92e-03 1.92e-03 0.00e+00 9.76e-01 0.00e+00 1.72e-02 2.87e-03]\n"," [7.85e-04 2.35e-03 0.00e+00 0.00e+00 9.94e-01 7.85e-04 2.35e-03]\n"," [8.06e-04 1.61e-03 8.06e-04 1.69e-02 0.00e+00 9.15e-01 6.53e-02]\n"," [7.28e-03 2.50e-02 2.08e-03 9.37e-03 3.12e-03 1.24e-01 8.29e-01]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'confusion_matrix = metrics.confusion_matrix(y_test, predictions)\\n\\n\\nprint(confusion_matrix)\\nnormalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\\n\\n\\n# Plot Results: \\nwidth = 12\\nheight = 12\\nplt.figure(figsize=(width, height))\\nplt.imshow(\\n    normalised_confusion_matrix, \\n    interpolation=\\'nearest\\', \\n    cmap=plt.cm.Blues\\n)\\nplt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\\nplt.colorbar()\\ntick_marks = np.arange(n_classes)\\nplt.xticks(tick_marks, LABELS, rotation=90)\\nplt.yticks(tick_marks, LABELS)\\nplt.tight_layout()\\nplt.ylabel(\\'True label\\')\\nplt.xlabel(\\'Predicted label\\')\\nplt.show()\\n'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"fL1zQLbOQ5Z1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619792343672,"user_tz":-420,"elapsed":1295871,"user":{"displayName":"Ngọc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl_wmhUZ8GzhAu_mcEuq0Ll-UVjVvQElGvgCE0eA=s64","userId":"05727681158388349460"}},"outputId":"02ffaaea-8e85-4cf7-f238-dcc34c57c7cb"},"source":["sess.close()\n","# sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n","sess = tf.Session()\n","if True:\n","    saver.restore(sess, \"/content/drive/MyDrive/Train-Openpose-lstm/Saved model/model.ckpt\")\n","    print(\"Model restored.\")\n","X_val_path = DATASET_PATH + \"X_val.txt\"\n","X_val = load_X_csv(X_val_path)\n","# print (X_val)\n","#print (pred)\n","preds = sess.run(\n","   [pred],\n","   feed_dict={\n","       x: X_val\n","  }\n",")\n","\n","print (preds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Train-Openpose-lstm/Saved model/model.ckpt\n","Model restored.\n","(31, 36)\n","31\n","(31, 1, 36)\n","[array([[ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.01,  2.27,  1.69,  0.34, -5.1 , -0.49,  1.36],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [ 0.11,  1.  ,  0.87, -0.24, -5.  ,  0.31,  3.04],\n","       [-2.56,  0.54, -1.59, -0.09, -4.52,  5.26,  3.06],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28],\n","       [-1.38, -1.53, -1.94, -0.21, -3.78,  7.69,  1.28]], dtype=float32)]\n"],"name":"stdout"}]}]}